{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from queue import PriorityQueue\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from sacrebleu import raw_corpus_bleu\n",
    "import math, copy, time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants here\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 3\n",
    "words_to_load = 80000\n",
    "d_model = emb_size = 300\n",
    "wiki_size = 300\n",
    "CUDA = True\n",
    "MAX_LENGTH = 80\n",
    "hidden_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftdir = '/scratch/yz4499/fasttext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_word2id, emb_id2word, emb_ordered_words):\n",
    "        self.name = name\n",
    "        self.word2index = emb_word2id\n",
    "        self.word2count = {}\n",
    "        self.index2word = emb_id2word #Dict\n",
    "        self.n_words = 4  # Count SOS and EOS +(batch: pad and unk)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2count:\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #This line is commented out since it will not properly deal with Chinese Letters\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "#reference: LAB4 hw2\n",
    "def indexesFromSentences(lang1, lang2, pairs):\n",
    "    id_list1 = []\n",
    "    id_list2 = []\n",
    "    for i in range(len(pairs)):\n",
    "        sentence1 = pairs[i][0]\n",
    "        sentence2 = pairs[i][1]\n",
    "        \n",
    "        sentence1 = sentence1.replace('quot','')\n",
    "        sentence1 = sentence1.replace('apos', '')\n",
    "        sentence2 = sentence2.replace('quot','')\n",
    "        sentence2 = sentence2.replace('apos', '')\n",
    "        #If either sentence is empty, then remove the pair\n",
    "        if sentence1 == '' or sentence2 == '':\n",
    "            continue;\n",
    "        \n",
    "        id_sentence1 = [lang1.word2index[word] if word in lang1.word2index else UNK_TOKEN \n",
    "                        for word in sentence1.split()] + [EOS_TOKEN]\n",
    "        id_list1.append(id_sentence1)\n",
    "        id_sentence2 = [lang2.word2index[word] if word in lang2.word2index else UNK_TOKEN \n",
    "                        for word in sentence2.split()] + [EOS_TOKEN]\n",
    "        id_list2.append(id_sentence2)\n",
    "        \n",
    "   \n",
    "        \n",
    "    return id_list1,id_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "datadir = os.getcwd()\n",
    "words_to_load = 50000\n",
    "# with open(datadir + '/data/wiki-news-300d-1M.vec') as f:\n",
    "with open(ftdir + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_en_embeddings = np.zeros(((words_to_load+4), wiki_size))\n",
    "    en_word2id = {}\n",
    "    en_id2words = {}\n",
    "    \n",
    "    en_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    en_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    en_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    en_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    en_word2id['<PAD>'] = PAD_TOKEN\n",
    "    en_word2id['<SOS>'] = SOS_TOKEN\n",
    "    en_word2id['<EOS>'] = EOS_TOKEN\n",
    "    en_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    en_ordered_words= []\n",
    "    en_ordered_words.append('<PAD>')\n",
    "    en_ordered_words.append('<SOS>')\n",
    "    en_ordered_words.append('<EOS>')\n",
    "    en_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        if i ==0:#Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        #print(len(s))\n",
    "        loaded_en_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        en_word2id[s[0]] = i+4 #for extra pad and unk eos and unk\n",
    "        en_id2words[i+4] = s[0]\n",
    "        en_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 0 has wrong dimension, hence skipped\n"
     ]
    }
   ],
   "source": [
    "#Reference Lab4 HW2\n",
    "#Over 200000 loaded words, 58 has wrong dimensions\n",
    "words_to_load = 50000\n",
    "datadir = os.getcwd()\n",
    "# with open(datadir + '/data/wiki.vi.vec') as f:\n",
    "with open(ftdir + 'cc.vi.300.vec') as f:\n",
    "    loaded_vi_embeddings = np.zeros(((words_to_load+4),wiki_size))\n",
    "    vi_word2id = {}\n",
    "    vi_id2words = {}\n",
    "    \n",
    "    vi_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    vi_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    vi_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    vi_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    vi_word2id['<PAD>'] = PAD_TOKEN\n",
    "    vi_word2id['<SOS>'] = SOS_TOKEN\n",
    "    vi_word2id['<EOS>'] = EOS_TOKEN\n",
    "    vi_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    vi_ordered_words= []\n",
    "    vi_ordered_words.append('<PAD>')\n",
    "    vi_ordered_words.append('<SOS>')\n",
    "    vi_ordered_words.append('<EOS>')\n",
    "    vi_ordered_words.append('<UNK>')\n",
    "    wrong_dim = 0;\n",
    "    for i, line in enumerate(f):\n",
    "        #print(line)\n",
    "        if i >= words_to_load:\n",
    "            break;\n",
    "        if i == 0: #Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        if len(s) != 301:\n",
    "            wrong_dim += 1#Skip the wrong dimension one\n",
    "            continue;\n",
    "        loaded_vi_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        vi_word2id[s[0]] = i+4 #for extra pad and unk \n",
    "        vi_id2words[i+4] = s[0]\n",
    "        vi_ordered_words.append(s[0])\n",
    "    print('In total {} has wrong dimension, hence skipped'.format(wrong_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #loading data\n",
    "# zh_train_pairs_cleaned = pkl.load(open('./data/zh_train_pairs_cleaned.p', 'rb'))\n",
    "# zh_val_pairs_cleaned = pkl.load(open('./data/zh_val_pairs_cleaned.p', 'rb'))\n",
    "# zh_test_pairs_cleaned = pkl.load(open('./data/zh_test_pairs_cleaned.p', 'rb'))\n",
    "\n",
    "vi_train_pairs_cleaned = pkl.load(open('./data/vi_train_pairs_cleaned.p', 'rb'))\n",
    "vi_val_pairs_cleaned = pkl.load(open('./data/vi_val_pairs_cleaned.p', 'rb'))\n",
    "vi_test_pairs_cleaned = pkl.load(open('./data/vi_test_pairs_cleaned.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs):#Needs the index pairs\n",
    "        self.pairs = pairs\n",
    "#         self.input_lang = input_lang\n",
    "#         self.output_lang = output_lang\n",
    "        self.input_seqs = [pairs[i][0] for i in range(len(self.pairs))]\n",
    "        self.output_seqs = [pairs[i][1] for i in range(len(self.pairs))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)#Returning number of pairs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.input_seqs[index]\n",
    "        output_seq = self.output_seqs[index]\n",
    "        return [input_seq, len(input_seq), output_seq, len(output_seq)]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    #Reference: lab8_3_mri\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "#         padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        padded_seqs = torch.zeros(len(seqs), MAX_LENGTH).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    batch_input_seqs = [datum[0] for datum in batch]\n",
    "    batch_output_seqs = [datum[2] for datum in batch]\n",
    "    #batch_input_length = [datum[1] for datum in batch]\n",
    "    #batch_output_length = [datum[3] for datum in batch]\n",
    "\n",
    "    sorted_pairs = sorted(zip(batch_input_seqs, batch_output_seqs), key=lambda x: len(x[0]), reverse = True)\n",
    "    in_seq_sorted, out_seq_sorted = zip(*sorted_pairs)\n",
    "    \n",
    "    padded_input,input_lens = _pad_sequences(in_seq_sorted)\n",
    "    padded_output,output_lens = _pad_sequences(out_seq_sorted)\n",
    "    \n",
    "    input_list = torch.from_numpy(np.array(padded_input))\n",
    "    input_length = torch.LongTensor(input_lens)\n",
    "    output_list = torch.from_numpy(np.array(padded_output))\n",
    "    output_length = torch.LongTensor(output_lens)\n",
    "    \n",
    "    if CUDA:\n",
    "        input_list = input_list.cuda()\n",
    "        output_list = output_list.cuda()\n",
    "        input_length = input_length.cuda()\n",
    "        output_length = output_length.cuda()\n",
    "            \n",
    "    return [input_list, input_length, output_list, output_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "'''\n",
    "NMTDataset needs index pairs, need to call indexesFromPairs functions beforehand\n",
    "The dataLoader is sorted according to length of the input_length, and padded to\n",
    "max length of input and output list repectively\n",
    "TODO: output_list is not sorted, hence need to sort (maybe) in the rnn sequence.\n",
    "'''\n",
    "# train_zh_dataset = NMTDataset(zh_train_pairs_cleaned, input_zh, output_zh_en)\n",
    "# train_vi_dataset = NMTDataset(vi_train_pairs_cleaned, input_vi, output_vi_en)\n",
    "# val_zh_dataset = NMTDataset(zh_val_pairs_cleaned, input_zh, output_zh_en)\n",
    "# val_vi_dataset = NMTDataset(vi_val_pairs_cleaned, input_vi, output_vi_en)\n",
    "# test_zh_dataset = NMTDataset(zh_test_pairs_cleaned, input_zh, output_zh_en)\n",
    "# test_vi_dataset = NMTDataset(vi_test_pairs_cleaned, input_vi, output_vi_en)\n",
    "\n",
    "# train_zh_dataset = NMTDataset(zh_train_pairs_cleaned)\n",
    "train_vi_dataset = NMTDataset(vi_train_pairs_cleaned)\n",
    "# val_zh_dataset = NMTDataset(zh_val_pairs_cleaned)\n",
    "val_vi_dataset = NMTDataset(vi_val_pairs_cleaned)\n",
    "# test_zh_dataset = NMTDataset(zh_test_pairs_cleaned)\n",
    "test_vi_dataset = NMTDataset(vi_test_pairs_cleaned)\n",
    "\n",
    "\n",
    "# train_zh_loader = torch.utils.data.DataLoader(dataset = train_zh_dataset, \n",
    "#                                           batch_size = BATCH_SIZE,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = True)\n",
    "\n",
    "train_vi_loader = torch.utils.data.DataLoader(dataset = train_vi_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "\n",
    "#Will use batch size 1 for validation and test since the sentence will be translated one by one\n",
    "# val_zh_loader = torch.utils.data.DataLoader(dataset = val_zh_dataset, \n",
    "#                                           batch_size = 1,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = False)\n",
    "val_vi_loader = torch.utils.data.DataLoader(dataset = val_vi_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "# test_zh_loader = torch.utils.data.DataLoader(dataset = test_zh_dataset, \n",
    "#                                           batch_size = 1,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = False)\n",
    "test_vi_loader = torch.utils.data.DataLoader(dataset = test_vi_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "#Input_batch in size Batch x maxLen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_list, input_length, output_list, output_length) in enumerate(train_vi_loader):\n",
    "    if i== 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "# MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 300\n",
    "max_length = 10\n",
    "BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n",
    "# CLIP = 50\n",
    "TEACHER_RATIO = 0.5\n",
    "# EN_ORDERED_NUM = 200003\n",
    "# ZH_ORDERED_NUM = 199945\n",
    "# VI_ORDERED_NUM = 199993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, emb, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.emb = emb\n",
    "        self.lut = nn.Embedding.from_pretrained(self.emb, False, False)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://stackoverflow.com/questions/52922445/runtimeerror-exp-not-implemented-for-torch-longtensor\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "       \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A self attention based Encoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, src_embed):\n",
    "        super(SelfEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        #self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        #self.tgt_embed = tgt_embed\n",
    "        #self.generator = generator\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(emb, src_vocab, N=6, \n",
    "               d_model=300, d_ff=2048, h=6, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = SelfEncoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(emb, d_model, src_vocab), c(position)))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reference: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "# #Reference: lab8 1_nmt, lab8 3_mri\n",
    "# class PreAttnDecoderRNN(nn.Module):\n",
    "#     def __init__(self, emb, emb_size, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "#         super(PreAttnDecoderRNN, self).__init__()\n",
    "#         self.emb = emb\n",
    "#         self.emb_size = emb_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size#vocab size of the output lang\n",
    "#         self.n_layers = n_layers\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "#         self.embedding = nn.Embedding.from_pretrained(self.emb, False, False,)\n",
    "        \n",
    "#         #self.attn = nn.Linear(hidden_size*, hidden_size)\n",
    "#         #self.attn2 = nn.Linear(hidden_size, hidden_size)\n",
    "# #         self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "#         self.concat = nn.Linear(2*hidden_size, hidden_size)\n",
    "#         self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "#     def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "#         #Here encoder_outputs is in size batch x max_seq x d_model\n",
    "#         self.batch_size = encoder_outputs.size(0)\n",
    "#         max_len = encoder_outputs.size(1)\n",
    "#         attn_energies = Variable(torch.zeros(self.batch_size, max_len)).to(device)#B X max_len\n",
    "# #         attn_energies = attn_energies.cuda() if CUDA else attn_energies\n",
    "        \n",
    "        \n",
    "#         embedded = self.embedding(word_input)\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         embedded = embedded.view(1, self.batch_size, -1) # S=1 x B x N\n",
    "        \n",
    "#         # Get current hidden state from input word and last hidden state\n",
    "#         rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "#         #rnn layer x batch x h\n",
    "#         #encoder-outputs  max_len x batch x h\n",
    "        \n",
    "# #         for b in range(self.batch_size):\n",
    "# #             # Calculate energy for each encoder output\n",
    "# #             for i in range(max_len):\n",
    "# #                 attn_energies[b, i] = (rnn_output[:, b].squeeze()).dot(encoder_outputs[i, b])\n",
    "        \n",
    "#         # Calculate attention from current RNN state and all encoder outputs;\n",
    "#         #More efficient\n",
    "#         #attn_energies = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(0,1).transpose(1,2))).squeeze(1)\n",
    "#         attn_energies = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(1,2))).squeeze(1)\n",
    "\n",
    "#         attn_weights = F.softmax(attn_energies) # B x max_len\n",
    "#         attn_weights = attn_weights.unsqueeze(1)\n",
    "#         # apply to encoder outputs to get weighted average\n",
    "#         context = attn_weights.bmm(encoder_outputs) # B x S=1 x N\n",
    "        \n",
    "#         # Attentional vector using the RNN hidden state and context vector\n",
    "\n",
    "#         # concatenated together (Luong eq. 5)\n",
    "#         rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "#         context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "#         concat_input = torch.cat((rnn_output, context), 1)\n",
    "#         concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "#         # Finally predict next token (Luong eq. 6, without softmax)\n",
    "#         output = self.out(concat_output)\n",
    "#         output = F.log_softmax(output)\n",
    "\n",
    "\n",
    "#         # Return final output, hidden state, and attention weights (for visualization)\n",
    "#         return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH, embed_size=emb_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(loaded_vi_embeddings, freeze=False)\n",
    "        self.attn = nn.Linear(hidden_size + embed_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_list, hidden, encoder_outputs):\n",
    "        \n",
    "        embedded = self.embedding(input_list)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        concat_input = torch.cat((embedded, hidden), 2)\n",
    "        attn_weights = F.softmax(self.attn(concat_input), dim=2)\n",
    "        attn_energies = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_energies), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        rnn_output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(rnn_output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for decoder PLS ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_batch = input_list.size(0)\n",
    "decoder_input = torch.tensor([[SOS_TOKEN]]*curr_batch, device=device)\n",
    "    \n",
    "#decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "decoder_hidden = torch.zeros(attn_decoder.n_layers, curr_batch, attn_decoder.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = encoder_outputs.size(0)\n",
    "max_len = encoder_outputs.size(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_energies = Variable(torch.zeros(batch_size, max_len))#B X max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(attn_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_weights = attn_weights.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 56]), torch.Size([32, 56, 300]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.size(), encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = attn_weights.bmm(encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 300])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_output = rnn_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = context.squeeze(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 300])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded = loaded_en_embeddings[decoder_input]\n",
    "\n",
    "embedded = embedded.view(1, batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 300])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_output, hidden = attn_decoder.gru(embedded, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 56, 300])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "# Calculate energy for each encoder output\n",
    "    for i in range(max_len):\n",
    "        attn_energies[b, i] = (rnn_output[:, b].squeeze()).dot(encoder_outputs[b, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_test = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(1,2))).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(temp_test, attn_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_input = torch.cat((rnn_output, context), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_output = F.tanh(attn_decoder.concat(concat_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 300])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = attn_decoder.out(concat_output)\n",
    "output = F.log_softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_zh_embeddings = torch.from_numpy(loaded_zh_embeddings).float()\n",
    "loaded_vi_embeddings = torch.from_numpy(loaded_vi_embeddings).float()\n",
    "loaded_en_embeddings = torch.from_numpy(loaded_en_embeddings).float()\n",
    "\n",
    "if CUDA:\n",
    "#     loaded_zh_embeddings = loaded_zh_embeddings.cuda()\n",
    "    loaded_vi_embeddings = loaded_vi_embeddings.cuda()\n",
    "    loaded_en_embeddings = loaded_en_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Example of encoder and decoder\n",
    "# self_encoder = make_model(loaded_vi_embeddings, len(vi_ordered_words)).to(device)\n",
    "# attn_decoder = AttnDecoderRNN(hidden_size, len(vi_ordered_words)).to(device)\n",
    "\n",
    "# #SLOW\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.Adam(self_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_outputs = self_encoder(input_list)\n",
    "# encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = self_attn_train(input_list, output_list, output_length, self_encoder, attn_decoder, \n",
    "#                       encoder_optimizer, decoder_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced from lab8 1nmt and modified \n",
    "def self_attn_train(input_list, output_list,output_length, \n",
    "                self_encoder, attn_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    '''\n",
    "    param: @attention is a Boolean variable indicating whether using attention\n",
    "    '''\n",
    "    self_encoder.train()\n",
    "    attn_decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, max_input_length = input_list.size()\n",
    "    max_output_length = output_list.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs = self_encoder(input_list)\n",
    "\n",
    "    #Initialize for decoding process\n",
    "#     curr_batch = input_list.size(0)#Take the current batch size\n",
    "    decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    #decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "    decoder_hidden = torch.zeros(1, batch_size, attn_decoder.hidden_size).to(device)\n",
    "    decoder_outputs = torch.zeros(max_output_length, batch_size, attn_decoder.output_size).to(device)\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "#     if CUDA:\n",
    "#         decoder_input = decoder_input.cuda()\n",
    "#         decoder_outputs = decoder_outputs.cuda()\n",
    "    \n",
    "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = attn_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs[di] = decoder_output\n",
    "            decoder_input = output_list[:,di].unsqueeze(0) # Teacher forcing\n",
    "            loss += criterion(decoder_output, output_list[:,di].contiguous())\n",
    "\n",
    "    else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = attn_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_outputs[di] = decoder_input\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)# detach from history as input: size batch x 1 \n",
    "            loss += criterion(decoder_output, output_list[:,di].contiguous())\n",
    "            \n",
    "    #loss += rnn_mask_loss(decoder_outputs.transpose(0,1).contiguous(), output_list.contiguous(), output_length)\n",
    "            \n",
    "    loss.backward()\n",
    "    #ec = torch.nn.utils.clip_grad_norm(batch_encoder.parameters(), CLIP)\n",
    "    #dc = torch.nn.utils.clip_grad_norm(batch_decoder.parameters(), CLIP)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference LAB8 1-nmt\n",
    "model_path = './model/'\n",
    "def AttnTrainIters(train_loader, self_encoder, attn_decoder, n_iters, \n",
    "                   print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(self_encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    epoch = 0\n",
    "    epoch_total = n_iters*len(train_loader)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        #print(\"Epoch {}/{}\".format(i+1, n_epochs))\n",
    "        for i, (input_list,input_length,output_list, output_length) in enumerate(train_loader):\n",
    "            loss = self_attn_train(input_list, output_list, output_length, self_encoder, attn_decoder, \n",
    "                      encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i > 0 and i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epoch_total),\n",
    "                                             epoch, epoch / epoch_total * 100, print_loss_avg))\n",
    "\n",
    "            if i > 0 and i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            epoch += 1\n",
    "            \n",
    "        torch.save(self_encoder.state_dict(), model_path + \"vi-self_encoder.pth\")\n",
    "        torch.save(attn_decoder.state_dict(), model_path + \"vi-self_attn_decoder.pth\")\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 15s (- 518m 28s) (10 0%) 6.1801\n",
      "0m 28s (- 494m 42s) (20 0%) 2.1405\n",
      "0m 42s (- 486m 35s) (30 0%) 1.8164\n",
      "0m 56s (- 482m 13s) (40 0%) 1.9355\n",
      "1m 9s (- 479m 32s) (50 0%) 1.8489\n",
      "1m 23s (- 477m 56s) (60 0%) 1.8009\n",
      "1m 37s (- 476m 41s) (70 0%) 1.8111\n",
      "1m 50s (- 475m 41s) (80 0%) 1.7441\n",
      "2m 4s (- 474m 55s) (90 0%) 1.5172\n",
      "2m 18s (- 474m 26s) (100 0%) 1.4728\n",
      "2m 32s (- 473m 59s) (110 0%) 1.5191\n",
      "2m 45s (- 473m 26s) (120 0%) 1.5622\n",
      "2m 59s (- 472m 58s) (130 0%) 1.4536\n",
      "3m 13s (- 472m 30s) (140 0%) 1.6161\n",
      "3m 26s (- 472m 3s) (150 0%) 1.6165\n",
      "3m 40s (- 471m 37s) (160 0%) 1.5706\n",
      "3m 54s (- 471m 17s) (170 0%) 1.4338\n",
      "4m 8s (- 470m 57s) (180 0%) 1.4805\n",
      "4m 21s (- 470m 35s) (190 0%) 1.5053\n",
      "4m 35s (- 470m 13s) (200 0%) 1.5523\n",
      "4m 49s (- 469m 50s) (210 1%) 1.6252\n",
      "5m 3s (- 469m 32s) (220 1%) 1.4455\n",
      "5m 16s (- 469m 17s) (230 1%) 1.5545\n",
      "5m 30s (- 468m 58s) (240 1%) 1.5151\n",
      "5m 44s (- 468m 38s) (250 1%) 1.4691\n",
      "5m 57s (- 468m 18s) (260 1%) 1.4140\n",
      "6m 11s (- 468m 2s) (270 1%) 1.4773\n",
      "6m 25s (- 467m 44s) (280 1%) 1.5456\n",
      "6m 39s (- 467m 28s) (290 1%) 1.4620\n",
      "6m 52s (- 467m 10s) (300 1%) 1.5309\n",
      "7m 6s (- 466m 54s) (310 1%) 1.5335\n",
      "7m 20s (- 466m 39s) (320 1%) 1.5388\n",
      "7m 33s (- 466m 22s) (330 1%) 1.5326\n",
      "7m 47s (- 466m 4s) (340 1%) 1.4420\n",
      "8m 1s (- 465m 50s) (350 1%) 1.4180\n",
      "8m 15s (- 465m 36s) (360 1%) 1.5108\n",
      "8m 28s (- 465m 19s) (370 1%) 1.4542\n",
      "8m 42s (- 465m 2s) (380 1%) 1.4069\n",
      "8m 56s (- 464m 45s) (390 1%) 1.5066\n",
      "9m 9s (- 464m 29s) (400 1%) 1.4409\n",
      "9m 23s (- 464m 14s) (410 1%) 1.4907\n",
      "9m 37s (- 463m 57s) (420 2%) 1.5011\n",
      "9m 50s (- 463m 41s) (430 2%) 1.4920\n",
      "10m 4s (- 463m 28s) (440 2%) 1.4606\n",
      "10m 18s (- 463m 12s) (450 2%) 1.5419\n",
      "10m 32s (- 462m 56s) (460 2%) 1.3430\n",
      "10m 45s (- 462m 41s) (470 2%) 1.4590\n",
      "10m 59s (- 462m 28s) (480 2%) 1.4356\n",
      "11m 13s (- 462m 13s) (490 2%) 1.3978\n",
      "11m 26s (- 461m 58s) (500 2%) 1.5270\n",
      "11m 40s (- 461m 43s) (510 2%) 1.4737\n",
      "11m 54s (- 461m 28s) (520 2%) 1.4792\n",
      "12m 8s (- 461m 13s) (530 2%) 1.5066\n",
      "12m 21s (- 460m 57s) (540 2%) 1.3771\n",
      "12m 35s (- 460m 43s) (550 2%) 1.3632\n",
      "12m 49s (- 460m 28s) (560 2%) 1.4853\n",
      "13m 2s (- 460m 13s) (570 2%) 1.4641\n",
      "13m 16s (- 459m 57s) (580 2%) 1.5976\n",
      "13m 30s (- 459m 42s) (590 2%) 1.5320\n",
      "13m 44s (- 459m 30s) (600 2%) 1.4286\n",
      "13m 57s (- 459m 17s) (610 2%) 1.4337\n",
      "14m 11s (- 459m 5s) (620 2%) 1.4022\n",
      "14m 25s (- 458m 56s) (630 3%) 1.3742\n",
      "14m 39s (- 458m 48s) (640 3%) 1.3778\n",
      "14m 53s (- 458m 39s) (650 3%) 1.3645\n",
      "15m 7s (- 458m 30s) (660 3%) 1.4289\n",
      "15m 21s (- 458m 21s) (670 3%) 1.4152\n",
      "15m 34s (- 458m 11s) (680 3%) 1.4513\n",
      "15m 48s (- 458m 2s) (690 3%) 1.5019\n",
      "16m 2s (- 457m 53s) (700 3%) 1.4358\n",
      "16m 16s (- 457m 42s) (710 3%) 1.4430\n",
      "16m 30s (- 457m 32s) (720 3%) 1.4859\n",
      "16m 44s (- 457m 23s) (730 3%) 1.3761\n",
      "16m 58s (- 457m 12s) (740 3%) 1.3944\n",
      "17m 12s (- 457m 1s) (750 3%) 1.4290\n",
      "17m 26s (- 456m 50s) (760 3%) 1.4202\n",
      "17m 39s (- 456m 39s) (770 3%) 1.4617\n",
      "17m 53s (- 456m 27s) (780 3%) 1.4547\n",
      "18m 7s (- 456m 16s) (790 3%) 1.3884\n",
      "18m 21s (- 456m 5s) (800 3%) 1.3627\n",
      "18m 35s (- 455m 53s) (810 3%) 1.4271\n",
      "18m 49s (- 455m 42s) (820 3%) 1.4188\n",
      "19m 3s (- 455m 30s) (830 4%) 1.3570\n",
      "19m 16s (- 455m 19s) (840 4%) 1.4439\n",
      "19m 30s (- 455m 8s) (850 4%) 1.4645\n",
      "19m 44s (- 454m 57s) (860 4%) 1.4564\n",
      "19m 58s (- 454m 45s) (870 4%) 1.4657\n",
      "20m 12s (- 454m 33s) (880 4%) 1.3900\n",
      "20m 26s (- 454m 22s) (890 4%) 1.3140\n",
      "20m 40s (- 454m 11s) (900 4%) 1.4392\n",
      "20m 54s (- 453m 59s) (910 4%) 1.4100\n",
      "21m 8s (- 453m 47s) (920 4%) 1.3297\n",
      "21m 21s (- 453m 35s) (930 4%) 1.4544\n",
      "21m 35s (- 453m 23s) (940 4%) 1.4590\n",
      "21m 49s (- 453m 11s) (950 4%) 1.4398\n",
      "22m 3s (- 452m 59s) (960 4%) 1.3754\n",
      "22m 17s (- 452m 47s) (970 4%) 1.2822\n",
      "22m 31s (- 452m 35s) (980 4%) 1.4575\n",
      "22m 45s (- 452m 22s) (990 4%) 1.4213\n",
      "22m 58s (- 452m 10s) (1000 4%) 1.4077\n",
      "23m 12s (- 451m 57s) (1010 4%) 1.3127\n",
      "23m 26s (- 451m 44s) (1020 4%) 1.5143\n",
      "23m 40s (- 451m 31s) (1030 4%) 1.3920\n",
      "23m 54s (- 451m 19s) (1040 5%) 1.4549\n",
      "24m 8s (- 451m 6s) (1050 5%) 1.2742\n",
      "24m 21s (- 450m 53s) (1060 5%) 1.3626\n",
      "24m 35s (- 450m 41s) (1070 5%) 1.5011\n",
      "24m 49s (- 450m 29s) (1080 5%) 1.3945\n",
      "25m 3s (- 450m 16s) (1090 5%) 1.4570\n",
      "25m 17s (- 450m 3s) (1100 5%) 1.4525\n",
      "25m 31s (- 449m 51s) (1110 5%) 1.3841\n",
      "25m 45s (- 449m 37s) (1120 5%) 1.4767\n",
      "25m 58s (- 449m 24s) (1130 5%) 1.3674\n",
      "26m 12s (- 449m 11s) (1140 5%) 1.4303\n",
      "26m 26s (- 448m 58s) (1150 5%) 1.4069\n",
      "26m 40s (- 448m 45s) (1160 5%) 1.4189\n",
      "26m 54s (- 448m 32s) (1170 5%) 1.3934\n",
      "27m 8s (- 448m 19s) (1180 5%) 1.4133\n",
      "27m 22s (- 448m 7s) (1190 5%) 1.6080\n",
      "27m 35s (- 447m 54s) (1200 5%) 1.4487\n",
      "27m 49s (- 447m 41s) (1210 5%) 1.3667\n",
      "28m 3s (- 447m 27s) (1220 5%) 1.4575\n",
      "28m 17s (- 447m 15s) (1230 5%) 1.3678\n",
      "28m 31s (- 447m 2s) (1240 5%) 1.2987\n",
      "28m 45s (- 446m 49s) (1250 6%) 1.3731\n",
      "28m 59s (- 446m 37s) (1260 6%) 1.4165\n",
      "29m 12s (- 446m 24s) (1270 6%) 1.3751\n",
      "29m 26s (- 446m 11s) (1280 6%) 1.4857\n",
      "29m 40s (- 445m 58s) (1290 6%) 1.4004\n",
      "29m 54s (- 445m 45s) (1300 6%) 1.3787\n",
      "30m 8s (- 445m 32s) (1310 6%) 1.4484\n",
      "30m 22s (- 445m 19s) (1320 6%) 1.3867\n",
      "30m 36s (- 445m 6s) (1330 6%) 1.4155\n",
      "30m 50s (- 444m 54s) (1340 6%) 1.4518\n",
      "31m 3s (- 444m 41s) (1350 6%) 1.4170\n",
      "31m 17s (- 444m 29s) (1360 6%) 1.4173\n",
      "31m 31s (- 444m 15s) (1370 6%) 1.3596\n",
      "31m 45s (- 444m 2s) (1380 6%) 1.4443\n",
      "31m 59s (- 443m 49s) (1390 6%) 1.3770\n",
      "32m 13s (- 443m 36s) (1400 6%) 1.3775\n",
      "32m 27s (- 443m 23s) (1410 6%) 1.4238\n",
      "32m 40s (- 443m 9s) (1420 6%) 1.4559\n",
      "32m 54s (- 442m 56s) (1430 6%) 1.3934\n",
      "33m 8s (- 442m 43s) (1440 6%) 1.3814\n",
      "33m 22s (- 442m 30s) (1450 7%) 1.3967\n",
      "33m 36s (- 442m 17s) (1460 7%) 1.4192\n",
      "33m 50s (- 442m 4s) (1470 7%) 1.3848\n",
      "34m 4s (- 441m 51s) (1480 7%) 1.4046\n",
      "34m 17s (- 441m 38s) (1490 7%) 1.4614\n",
      "34m 31s (- 441m 25s) (1500 7%) 1.4191\n",
      "34m 45s (- 441m 12s) (1510 7%) 1.4051\n",
      "34m 59s (- 440m 59s) (1520 7%) 1.3301\n",
      "35m 13s (- 440m 46s) (1530 7%) 1.3481\n",
      "35m 27s (- 440m 33s) (1540 7%) 1.3294\n",
      "35m 41s (- 440m 20s) (1550 7%) 1.3197\n",
      "35m 55s (- 440m 7s) (1560 7%) 1.4184\n",
      "36m 9s (- 439m 54s) (1570 7%) 1.4547\n",
      "36m 22s (- 439m 41s) (1580 7%) 1.3741\n",
      "36m 36s (- 439m 27s) (1590 7%) 1.3081\n",
      "36m 50s (- 439m 13s) (1600 7%) 1.4124\n",
      "37m 4s (- 439m 0s) (1610 7%) 1.3664\n",
      "37m 18s (- 438m 47s) (1620 7%) 1.2896\n",
      "37m 32s (- 438m 34s) (1630 7%) 1.3117\n",
      "37m 46s (- 438m 21s) (1640 7%) 1.4209\n",
      "37m 59s (- 438m 8s) (1650 7%) 1.3175\n",
      "38m 13s (- 437m 55s) (1660 8%) 1.3732\n",
      "38m 27s (- 437m 42s) (1670 8%) 1.4754\n",
      "38m 41s (- 437m 29s) (1680 8%) 1.3797\n",
      "38m 55s (- 437m 16s) (1690 8%) 1.3938\n",
      "39m 9s (- 437m 2s) (1700 8%) 1.3478\n",
      "39m 23s (- 436m 48s) (1710 8%) 1.4009\n",
      "39m 36s (- 436m 35s) (1720 8%) 1.3631\n",
      "39m 50s (- 436m 22s) (1730 8%) 1.3924\n",
      "40m 4s (- 436m 9s) (1740 8%) 1.4440\n",
      "40m 18s (- 435m 55s) (1750 8%) 1.3857\n",
      "40m 32s (- 435m 42s) (1760 8%) 1.3046\n",
      "40m 46s (- 435m 29s) (1770 8%) 1.3692\n",
      "41m 0s (- 435m 15s) (1780 8%) 1.2752\n",
      "41m 14s (- 435m 2s) (1790 8%) 1.2713\n",
      "41m 27s (- 434m 49s) (1800 8%) 1.3572\n",
      "41m 41s (- 434m 36s) (1810 8%) 1.3977\n",
      "41m 55s (- 434m 23s) (1820 8%) 1.3465\n",
      "42m 9s (- 434m 9s) (1830 8%) 1.4174\n",
      "42m 23s (- 433m 56s) (1840 8%) 1.4158\n",
      "42m 37s (- 433m 42s) (1850 8%) 1.4379\n",
      "42m 51s (- 433m 29s) (1860 8%) 1.3405\n",
      "43m 5s (- 433m 16s) (1870 9%) 1.3803\n",
      "43m 19s (- 433m 3s) (1880 9%) 1.3562\n",
      "43m 32s (- 432m 49s) (1890 9%) 1.3596\n",
      "43m 46s (- 432m 36s) (1900 9%) 1.4870\n",
      "44m 0s (- 432m 23s) (1910 9%) 1.3967\n",
      "44m 14s (- 432m 10s) (1920 9%) 1.3279\n",
      "44m 28s (- 431m 56s) (1930 9%) 1.3417\n",
      "44m 42s (- 431m 43s) (1940 9%) 1.4803\n",
      "44m 56s (- 431m 29s) (1950 9%) 1.3671\n",
      "45m 9s (- 431m 16s) (1960 9%) 1.3163\n",
      "45m 23s (- 431m 1s) (1970 9%) 1.3814\n",
      "45m 37s (- 430m 47s) (1980 9%) 1.3432\n",
      "45m 51s (- 430m 32s) (1990 9%) 1.3365\n",
      "46m 4s (- 430m 17s) (2000 9%) 1.2383\n",
      "46m 18s (- 430m 2s) (2010 9%) 1.2982\n",
      "46m 32s (- 429m 47s) (2020 9%) 1.3890\n",
      "46m 46s (- 429m 32s) (2030 9%) 1.3263\n",
      "46m 59s (- 429m 17s) (2040 9%) 1.4151\n",
      "47m 13s (- 429m 1s) (2050 9%) 1.3261\n",
      "47m 26s (- 428m 45s) (2060 9%) 1.3449\n",
      "47m 40s (- 428m 30s) (2070 10%) 1.2982\n",
      "47m 54s (- 428m 14s) (2080 10%) 1.4642\n",
      "48m 7s (- 427m 58s) (2090 10%) 1.4615\n",
      "48m 21s (- 427m 43s) (2100 10%) 1.4143\n",
      "48m 34s (- 427m 27s) (2110 10%) 1.4075\n",
      "48m 48s (- 427m 12s) (2120 10%) 1.3178\n",
      "49m 2s (- 426m 56s) (2130 10%) 1.3642\n",
      "49m 15s (- 426m 41s) (2140 10%) 1.3907\n",
      "49m 29s (- 426m 25s) (2150 10%) 1.3948\n",
      "49m 43s (- 426m 10s) (2160 10%) 1.2882\n",
      "49m 56s (- 425m 55s) (2170 10%) 1.3370\n",
      "50m 10s (- 425m 40s) (2180 10%) 1.3824\n",
      "50m 24s (- 425m 25s) (2190 10%) 1.2942\n",
      "50m 37s (- 425m 10s) (2200 10%) 1.5135\n",
      "50m 51s (- 424m 55s) (2210 10%) 1.4840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51m 5s (- 424m 40s) (2220 10%) 1.3632\n",
      "51m 18s (- 424m 25s) (2230 10%) 1.4142\n",
      "51m 32s (- 424m 11s) (2240 10%) 1.3116\n",
      "51m 46s (- 423m 56s) (2250 10%) 1.3236\n",
      "51m 59s (- 423m 41s) (2260 10%) 1.3439\n",
      "52m 13s (- 423m 26s) (2270 10%) 1.2946\n",
      "52m 27s (- 423m 12s) (2280 11%) 1.3335\n",
      "52m 40s (- 422m 57s) (2290 11%) 1.4308\n",
      "52m 54s (- 422m 42s) (2300 11%) 1.3095\n",
      "53m 8s (- 422m 28s) (2310 11%) 1.3736\n",
      "53m 22s (- 422m 13s) (2320 11%) 1.2846\n",
      "53m 35s (- 421m 59s) (2330 11%) 1.2969\n",
      "53m 49s (- 421m 44s) (2340 11%) 1.4137\n",
      "54m 3s (- 421m 31s) (2350 11%) 1.2623\n",
      "54m 17s (- 421m 18s) (2360 11%) 1.2588\n",
      "54m 31s (- 421m 5s) (2370 11%) 1.3195\n",
      "54m 45s (- 420m 51s) (2380 11%) 1.3793\n",
      "54m 58s (- 420m 38s) (2390 11%) 1.3896\n",
      "55m 12s (- 420m 25s) (2400 11%) 1.5062\n",
      "55m 26s (- 420m 11s) (2410 11%) 1.3798\n",
      "55m 40s (- 419m 58s) (2420 11%) 1.4290\n",
      "55m 54s (- 419m 45s) (2430 11%) 1.3384\n",
      "56m 8s (- 419m 32s) (2440 11%) 1.4383\n",
      "56m 22s (- 419m 18s) (2450 11%) 1.2680\n",
      "56m 35s (- 419m 5s) (2460 11%) 1.3225\n",
      "56m 49s (- 418m 52s) (2470 11%) 1.4241\n",
      "57m 3s (- 418m 38s) (2480 11%) 1.3809\n",
      "57m 17s (- 418m 25s) (2490 12%) 1.3718\n",
      "57m 31s (- 418m 12s) (2500 12%) 1.3629\n",
      "57m 45s (- 417m 58s) (2510 12%) 1.3504\n",
      "57m 59s (- 417m 45s) (2520 12%) 1.4386\n",
      "58m 13s (- 417m 32s) (2530 12%) 1.3524\n",
      "58m 26s (- 417m 18s) (2540 12%) 1.4369\n",
      "58m 40s (- 417m 5s) (2550 12%) 1.3115\n",
      "58m 54s (- 416m 52s) (2560 12%) 1.3030\n",
      "59m 8s (- 416m 38s) (2570 12%) 1.3201\n",
      "59m 22s (- 416m 25s) (2580 12%) 1.4364\n",
      "59m 36s (- 416m 12s) (2590 12%) 1.4011\n",
      "59m 50s (- 415m 58s) (2600 12%) 1.2573\n",
      "60m 4s (- 415m 45s) (2610 12%) 1.3779\n",
      "60m 17s (- 415m 31s) (2620 12%) 1.4020\n",
      "60m 31s (- 415m 18s) (2630 12%) 1.3920\n",
      "60m 45s (- 415m 5s) (2640 12%) 1.3756\n",
      "60m 59s (- 414m 51s) (2650 12%) 1.3366\n",
      "61m 13s (- 414m 38s) (2660 12%) 1.4547\n",
      "61m 27s (- 414m 25s) (2670 12%) 1.3217\n",
      "61m 41s (- 414m 11s) (2680 12%) 1.3142\n",
      "61m 55s (- 413m 58s) (2690 13%) 1.4296\n",
      "62m 8s (- 413m 44s) (2700 13%) 1.3826\n",
      "62m 22s (- 413m 31s) (2710 13%) 1.3487\n",
      "62m 36s (- 413m 17s) (2720 13%) 1.3835\n",
      "62m 50s (- 413m 4s) (2730 13%) 1.4068\n",
      "63m 4s (- 412m 50s) (2740 13%) 1.3332\n",
      "63m 18s (- 412m 37s) (2750 13%) 1.3394\n",
      "63m 32s (- 412m 24s) (2760 13%) 1.2379\n",
      "63m 45s (- 412m 10s) (2770 13%) 1.3256\n",
      "63m 59s (- 411m 57s) (2780 13%) 1.3699\n",
      "64m 13s (- 411m 43s) (2790 13%) 1.3297\n",
      "64m 27s (- 411m 30s) (2800 13%) 1.3056\n",
      "64m 41s (- 411m 17s) (2810 13%) 1.3147\n",
      "64m 55s (- 411m 4s) (2820 13%) 1.3532\n",
      "65m 9s (- 410m 50s) (2830 13%) 1.2969\n",
      "65m 23s (- 410m 36s) (2840 13%) 1.2969\n",
      "65m 36s (- 410m 23s) (2850 13%) 1.3495\n",
      "65m 50s (- 410m 9s) (2860 13%) 1.3303\n",
      "66m 4s (- 409m 56s) (2870 13%) 1.2652\n",
      "66m 18s (- 409m 42s) (2880 13%) 1.3042\n",
      "66m 32s (- 409m 29s) (2890 13%) 1.3138\n",
      "66m 46s (- 409m 15s) (2900 14%) 1.4215\n",
      "67m 0s (- 409m 2s) (2910 14%) 1.3316\n",
      "67m 14s (- 408m 48s) (2920 14%) 1.3125\n",
      "67m 27s (- 408m 35s) (2930 14%) 1.4112\n",
      "67m 41s (- 408m 21s) (2940 14%) 1.2367\n",
      "67m 55s (- 408m 8s) (2950 14%) 1.2925\n",
      "68m 9s (- 407m 54s) (2960 14%) 1.2900\n",
      "68m 23s (- 407m 41s) (2970 14%) 1.2971\n",
      "68m 37s (- 407m 27s) (2980 14%) 1.2643\n",
      "68m 51s (- 407m 14s) (2990 14%) 1.4576\n",
      "69m 5s (- 407m 1s) (3000 14%) 1.2872\n",
      "69m 18s (- 406m 47s) (3010 14%) 1.2865\n",
      "69m 32s (- 406m 34s) (3020 14%) 1.2796\n",
      "69m 46s (- 406m 20s) (3030 14%) 1.2666\n",
      "70m 0s (- 406m 6s) (3040 14%) 1.2891\n",
      "70m 14s (- 405m 53s) (3050 14%) 1.3414\n",
      "70m 28s (- 405m 39s) (3060 14%) 1.3592\n",
      "70m 42s (- 405m 26s) (3070 14%) 1.2682\n",
      "70m 55s (- 405m 12s) (3080 14%) 1.2563\n",
      "71m 9s (- 404m 59s) (3090 14%) 1.3425\n",
      "71m 23s (- 404m 45s) (3100 14%) 1.2391\n",
      "71m 37s (- 404m 32s) (3110 15%) 1.3236\n",
      "71m 51s (- 404m 18s) (3120 15%) 1.3296\n",
      "72m 5s (- 404m 5s) (3130 15%) 1.3261\n",
      "72m 19s (- 403m 51s) (3140 15%) 1.3160\n",
      "72m 33s (- 403m 37s) (3150 15%) 1.2492\n",
      "72m 46s (- 403m 24s) (3160 15%) 1.2363\n",
      "73m 0s (- 403m 10s) (3170 15%) 1.2165\n",
      "73m 14s (- 402m 57s) (3180 15%) 1.2915\n",
      "73m 28s (- 402m 43s) (3190 15%) 1.3194\n",
      "73m 42s (- 402m 29s) (3200 15%) 1.3095\n",
      "73m 56s (- 402m 16s) (3210 15%) 1.3784\n",
      "74m 10s (- 402m 2s) (3220 15%) 1.4089\n",
      "74m 23s (- 401m 49s) (3230 15%) 1.2673\n",
      "74m 37s (- 401m 35s) (3240 15%) 1.2807\n",
      "74m 51s (- 401m 22s) (3250 15%) 1.2814\n",
      "75m 5s (- 401m 8s) (3260 15%) 1.3945\n",
      "75m 19s (- 400m 55s) (3270 15%) 1.2780\n",
      "75m 33s (- 400m 41s) (3280 15%) 1.3240\n",
      "75m 47s (- 400m 27s) (3290 15%) 1.2175\n",
      "76m 0s (- 400m 14s) (3300 15%) 1.2394\n",
      "76m 14s (- 400m 0s) (3310 16%) 1.2773\n",
      "76m 28s (- 399m 46s) (3320 16%) 1.2816\n",
      "76m 42s (- 399m 33s) (3330 16%) 1.2159\n",
      "76m 56s (- 399m 19s) (3340 16%) 1.1913\n",
      "77m 10s (- 399m 6s) (3350 16%) 1.2750\n",
      "77m 24s (- 398m 52s) (3360 16%) 1.3820\n",
      "77m 38s (- 398m 39s) (3370 16%) 1.2532\n",
      "77m 51s (- 398m 25s) (3380 16%) 1.2799\n",
      "78m 5s (- 398m 11s) (3390 16%) 1.2574\n",
      "78m 19s (- 397m 58s) (3400 16%) 1.3113\n",
      "78m 33s (- 397m 44s) (3410 16%) 1.2513\n",
      "78m 47s (- 397m 30s) (3420 16%) 1.2195\n",
      "79m 1s (- 397m 17s) (3430 16%) 1.2489\n",
      "79m 15s (- 397m 3s) (3440 16%) 1.2853\n",
      "79m 28s (- 396m 50s) (3450 16%) 1.3322\n",
      "79m 42s (- 396m 36s) (3460 16%) 1.2459\n",
      "79m 56s (- 396m 22s) (3470 16%) 1.1518\n",
      "80m 10s (- 396m 8s) (3480 16%) 1.2228\n",
      "80m 24s (- 395m 55s) (3490 16%) 1.3016\n",
      "80m 38s (- 395m 41s) (3500 16%) 1.2223\n",
      "80m 52s (- 395m 28s) (3510 16%) 1.3043\n",
      "81m 6s (- 395m 14s) (3520 17%) 1.2201\n",
      "81m 19s (- 395m 1s) (3530 17%) 1.2786\n",
      "81m 33s (- 394m 47s) (3540 17%) 1.2827\n",
      "81m 47s (- 394m 34s) (3550 17%) 1.3136\n",
      "82m 1s (- 394m 20s) (3560 17%) 1.1589\n",
      "82m 15s (- 394m 6s) (3570 17%) 1.1775\n",
      "82m 29s (- 393m 53s) (3580 17%) 1.1725\n",
      "82m 43s (- 393m 40s) (3590 17%) 1.2928\n",
      "82m 57s (- 393m 26s) (3600 17%) 1.2617\n",
      "83m 11s (- 393m 13s) (3610 17%) 1.2907\n",
      "83m 24s (- 392m 59s) (3620 17%) 1.3134\n",
      "83m 38s (- 392m 45s) (3630 17%) 1.2437\n",
      "83m 52s (- 392m 32s) (3640 17%) 1.1816\n",
      "84m 6s (- 392m 18s) (3650 17%) 1.2628\n",
      "84m 20s (- 392m 4s) (3660 17%) 1.3109\n",
      "84m 34s (- 391m 51s) (3670 17%) 1.2149\n",
      "84m 48s (- 391m 37s) (3680 17%) 1.2621\n",
      "85m 1s (- 391m 24s) (3690 17%) 1.2612\n",
      "85m 15s (- 391m 10s) (3700 17%) 1.2038\n",
      "85m 29s (- 390m 56s) (3710 17%) 1.2552\n",
      "85m 43s (- 390m 42s) (3720 17%) 1.2197\n",
      "85m 57s (- 390m 29s) (3730 18%) 1.3071\n",
      "86m 11s (- 390m 15s) (3740 18%) 1.2883\n",
      "86m 25s (- 390m 1s) (3750 18%) 1.2513\n",
      "86m 38s (- 389m 48s) (3760 18%) 1.2060\n",
      "86m 52s (- 389m 34s) (3770 18%) 1.2860\n",
      "87m 6s (- 389m 21s) (3780 18%) 1.2457\n",
      "87m 20s (- 389m 7s) (3790 18%) 1.1826\n",
      "87m 34s (- 388m 53s) (3800 18%) 1.2406\n",
      "87m 48s (- 388m 40s) (3810 18%) 1.2329\n",
      "88m 2s (- 388m 26s) (3820 18%) 1.2537\n",
      "88m 16s (- 388m 12s) (3830 18%) 1.2611\n",
      "88m 29s (- 387m 59s) (3840 18%) 1.1839\n",
      "88m 43s (- 387m 45s) (3850 18%) 1.2154\n",
      "88m 57s (- 387m 32s) (3860 18%) 1.2678\n",
      "89m 11s (- 387m 18s) (3870 18%) 1.2456\n",
      "89m 25s (- 387m 4s) (3880 18%) 1.2351\n",
      "89m 39s (- 386m 51s) (3890 18%) 1.2991\n",
      "89m 53s (- 386m 37s) (3900 18%) 1.2398\n",
      "90m 7s (- 386m 23s) (3910 18%) 1.1892\n",
      "90m 20s (- 386m 10s) (3920 18%) 1.2031\n",
      "90m 34s (- 385m 56s) (3930 19%) 1.1620\n",
      "90m 48s (- 385m 42s) (3940 19%) 1.2835\n",
      "91m 2s (- 385m 29s) (3950 19%) 1.2168\n",
      "91m 16s (- 385m 15s) (3960 19%) 1.1851\n",
      "91m 30s (- 385m 1s) (3970 19%) 1.2110\n",
      "91m 44s (- 384m 48s) (3980 19%) 1.2064\n",
      "91m 57s (- 384m 34s) (3990 19%) 1.1635\n",
      "92m 11s (- 384m 20s) (4000 19%) 1.2784\n",
      "92m 25s (- 384m 7s) (4010 19%) 1.2049\n",
      "92m 39s (- 383m 53s) (4020 19%) 1.2027\n",
      "92m 53s (- 383m 39s) (4030 19%) 1.1885\n",
      "93m 7s (- 383m 26s) (4040 19%) 1.1343\n",
      "93m 21s (- 383m 12s) (4050 19%) 1.3532\n",
      "93m 35s (- 382m 58s) (4060 19%) 1.1822\n",
      "93m 48s (- 382m 45s) (4070 19%) 1.3433\n",
      "94m 2s (- 382m 31s) (4080 19%) 1.2162\n",
      "94m 16s (- 382m 17s) (4090 19%) 1.2827\n",
      "94m 30s (- 382m 4s) (4100 19%) 1.2011\n",
      "94m 44s (- 381m 50s) (4110 19%) 1.2222\n",
      "94m 58s (- 381m 36s) (4120 19%) 1.2602\n",
      "95m 12s (- 381m 23s) (4130 19%) 1.2350\n",
      "95m 33s (- 381m 5s) (4145 20%) 1.7742\n",
      "95m 47s (- 380m 51s) (4155 20%) 1.1914\n",
      "96m 1s (- 380m 37s) (4165 20%) 1.2311\n",
      "96m 15s (- 380m 23s) (4175 20%) 1.2422\n",
      "96m 28s (- 380m 9s) (4185 20%) 1.1715\n",
      "96m 42s (- 379m 55s) (4195 20%) 1.2493\n",
      "96m 56s (- 379m 41s) (4205 20%) 1.2142\n",
      "97m 10s (- 379m 27s) (4215 20%) 1.2015\n",
      "97m 24s (- 379m 13s) (4225 20%) 1.2245\n",
      "97m 37s (- 378m 59s) (4235 20%) 1.2129\n",
      "97m 51s (- 378m 45s) (4245 20%) 1.1977\n",
      "98m 5s (- 378m 31s) (4255 20%) 1.1459\n",
      "98m 19s (- 378m 18s) (4265 20%) 1.1946\n",
      "98m 33s (- 378m 4s) (4275 20%) 1.2106\n",
      "98m 46s (- 377m 50s) (4285 20%) 1.1814\n",
      "99m 0s (- 377m 36s) (4295 20%) 1.1698\n",
      "99m 14s (- 377m 22s) (4305 20%) 1.1182\n",
      "99m 28s (- 377m 8s) (4315 20%) 1.1595\n",
      "99m 42s (- 376m 54s) (4325 20%) 1.1652\n",
      "99m 55s (- 376m 40s) (4335 20%) 1.2732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100m 9s (- 376m 26s) (4345 21%) 1.2235\n",
      "100m 23s (- 376m 12s) (4355 21%) 1.1786\n",
      "100m 37s (- 375m 58s) (4365 21%) 1.1739\n",
      "100m 51s (- 375m 45s) (4375 21%) 1.2166\n",
      "101m 5s (- 375m 31s) (4385 21%) 1.1190\n",
      "101m 18s (- 375m 17s) (4395 21%) 1.1456\n",
      "101m 32s (- 375m 3s) (4405 21%) 1.1867\n",
      "101m 46s (- 374m 49s) (4415 21%) 1.2930\n",
      "102m 0s (- 374m 35s) (4425 21%) 1.1638\n",
      "102m 14s (- 374m 21s) (4435 21%) 1.1949\n",
      "102m 27s (- 374m 7s) (4445 21%) 1.2372\n",
      "102m 41s (- 373m 53s) (4455 21%) 1.1441\n",
      "102m 55s (- 373m 40s) (4465 21%) 1.1612\n",
      "103m 9s (- 373m 26s) (4475 21%) 1.1855\n",
      "103m 23s (- 373m 12s) (4485 21%) 1.1919\n",
      "103m 37s (- 372m 58s) (4495 21%) 1.2077\n",
      "103m 50s (- 372m 44s) (4505 21%) 1.1595\n",
      "104m 4s (- 372m 30s) (4515 21%) 1.1929\n",
      "104m 18s (- 372m 16s) (4525 21%) 1.2069\n",
      "104m 32s (- 372m 2s) (4535 21%) 1.1965\n",
      "104m 45s (- 371m 48s) (4545 21%) 1.1912\n",
      "104m 59s (- 371m 34s) (4555 22%) 1.1707\n",
      "105m 13s (- 371m 20s) (4565 22%) 1.2100\n",
      "105m 27s (- 371m 6s) (4575 22%) 1.1935\n",
      "105m 41s (- 370m 52s) (4585 22%) 1.1141\n",
      "105m 54s (- 370m 38s) (4595 22%) 1.2503\n",
      "106m 8s (- 370m 24s) (4605 22%) 1.2099\n",
      "106m 22s (- 370m 11s) (4615 22%) 1.1914\n",
      "106m 36s (- 369m 57s) (4625 22%) 1.1439\n",
      "106m 50s (- 369m 43s) (4635 22%) 1.1495\n",
      "107m 3s (- 369m 29s) (4645 22%) 1.1623\n",
      "107m 17s (- 369m 15s) (4655 22%) 1.1382\n",
      "107m 31s (- 369m 1s) (4665 22%) 1.1703\n",
      "107m 45s (- 368m 47s) (4675 22%) 1.1244\n",
      "107m 59s (- 368m 33s) (4685 22%) 1.1562\n",
      "108m 12s (- 368m 19s) (4695 22%) 1.3035\n",
      "108m 26s (- 368m 5s) (4705 22%) 1.1150\n",
      "108m 40s (- 367m 51s) (4715 22%) 1.2605\n",
      "108m 54s (- 367m 37s) (4725 22%) 1.1346\n",
      "109m 8s (- 367m 23s) (4735 22%) 1.1470\n",
      "109m 21s (- 367m 9s) (4745 22%) 1.1152\n",
      "109m 35s (- 366m 55s) (4755 22%) 1.0804\n",
      "109m 49s (- 366m 42s) (4765 23%) 1.1782\n",
      "110m 3s (- 366m 28s) (4775 23%) 1.1296\n",
      "110m 17s (- 366m 14s) (4785 23%) 1.1879\n",
      "110m 30s (- 366m 0s) (4795 23%) 1.1819\n",
      "110m 44s (- 365m 46s) (4805 23%) 1.1849\n",
      "110m 58s (- 365m 32s) (4815 23%) 1.1771\n",
      "111m 12s (- 365m 18s) (4825 23%) 1.1117\n",
      "111m 26s (- 365m 4s) (4835 23%) 1.2161\n",
      "111m 39s (- 364m 50s) (4845 23%) 1.1641\n",
      "111m 53s (- 364m 36s) (4855 23%) 1.1333\n",
      "112m 7s (- 364m 22s) (4865 23%) 1.1720\n",
      "112m 21s (- 364m 8s) (4875 23%) 1.2097\n",
      "112m 35s (- 363m 54s) (4885 23%) 1.1684\n",
      "112m 48s (- 363m 40s) (4895 23%) 1.1586\n",
      "113m 2s (- 363m 26s) (4905 23%) 1.2560\n",
      "113m 16s (- 363m 12s) (4915 23%) 1.1523\n",
      "113m 30s (- 362m 59s) (4925 23%) 1.1023\n",
      "113m 44s (- 362m 45s) (4935 23%) 1.1489\n",
      "113m 57s (- 362m 31s) (4945 23%) 1.1977\n",
      "114m 11s (- 362m 17s) (4955 23%) 1.1179\n",
      "114m 25s (- 362m 3s) (4965 24%) 1.2576\n",
      "114m 39s (- 361m 49s) (4975 24%) 1.1728\n",
      "114m 53s (- 361m 35s) (4985 24%) 1.1238\n",
      "115m 6s (- 361m 21s) (4995 24%) 1.1228\n",
      "115m 20s (- 361m 7s) (5005 24%) 1.1864\n",
      "115m 34s (- 360m 53s) (5015 24%) 1.0797\n",
      "115m 48s (- 360m 39s) (5025 24%) 1.1814\n",
      "116m 2s (- 360m 26s) (5035 24%) 1.1176\n",
      "116m 15s (- 360m 12s) (5045 24%) 1.1652\n",
      "116m 29s (- 359m 58s) (5055 24%) 1.1338\n",
      "116m 43s (- 359m 44s) (5065 24%) 1.0935\n",
      "116m 57s (- 359m 30s) (5075 24%) 1.1564\n",
      "117m 11s (- 359m 16s) (5085 24%) 1.1708\n",
      "117m 24s (- 359m 2s) (5095 24%) 1.0924\n",
      "117m 38s (- 358m 48s) (5105 24%) 1.1673\n",
      "117m 52s (- 358m 35s) (5115 24%) 1.1736\n",
      "118m 6s (- 358m 21s) (5125 24%) 1.1978\n",
      "118m 20s (- 358m 7s) (5135 24%) 1.1381\n",
      "118m 34s (- 357m 53s) (5145 24%) 1.1462\n",
      "118m 47s (- 357m 39s) (5155 24%) 1.1424\n",
      "119m 1s (- 357m 25s) (5165 24%) 1.0727\n",
      "119m 15s (- 357m 11s) (5175 25%) 1.1255\n",
      "119m 29s (- 356m 57s) (5185 25%) 1.1973\n",
      "119m 43s (- 356m 44s) (5195 25%) 1.1776\n",
      "119m 56s (- 356m 30s) (5205 25%) 1.1052\n",
      "120m 10s (- 356m 16s) (5215 25%) 1.2061\n",
      "120m 24s (- 356m 2s) (5225 25%) 1.1155\n",
      "120m 38s (- 355m 48s) (5235 25%) 1.1519\n",
      "120m 52s (- 355m 35s) (5245 25%) 1.1047\n",
      "121m 6s (- 355m 21s) (5255 25%) 1.1947\n",
      "121m 19s (- 355m 7s) (5265 25%) 1.2255\n",
      "121m 33s (- 354m 53s) (5275 25%) 1.1230\n",
      "121m 47s (- 354m 39s) (5285 25%) 1.1594\n",
      "122m 1s (- 354m 25s) (5295 25%) 1.1254\n",
      "122m 15s (- 354m 11s) (5305 25%) 1.1818\n",
      "122m 28s (- 353m 58s) (5315 25%) 1.1514\n",
      "122m 42s (- 353m 44s) (5325 25%) 1.0561\n",
      "122m 56s (- 353m 30s) (5335 25%) 1.1080\n",
      "123m 10s (- 353m 16s) (5345 25%) 1.1289\n",
      "123m 24s (- 353m 2s) (5355 25%) 1.2197\n",
      "123m 38s (- 352m 48s) (5365 25%) 1.1670\n",
      "123m 51s (- 352m 35s) (5375 25%) 1.1760\n",
      "124m 5s (- 352m 21s) (5385 26%) 1.1842\n",
      "124m 19s (- 352m 7s) (5395 26%) 1.1354\n",
      "124m 33s (- 351m 53s) (5405 26%) 1.1352\n",
      "124m 47s (- 351m 39s) (5415 26%) 1.1583\n",
      "125m 1s (- 351m 25s) (5425 26%) 1.1788\n",
      "125m 14s (- 351m 12s) (5435 26%) 1.1903\n",
      "125m 28s (- 350m 58s) (5445 26%) 1.1476\n",
      "125m 42s (- 350m 44s) (5455 26%) 1.2301\n",
      "125m 56s (- 350m 30s) (5465 26%) 1.1806\n",
      "126m 10s (- 350m 16s) (5475 26%) 1.1354\n",
      "126m 24s (- 350m 2s) (5485 26%) 1.2152\n",
      "126m 37s (- 349m 49s) (5495 26%) 1.1377\n",
      "126m 51s (- 349m 35s) (5505 26%) 1.1550\n",
      "127m 5s (- 349m 21s) (5515 26%) 1.1552\n",
      "127m 19s (- 349m 7s) (5525 26%) 1.1264\n",
      "127m 33s (- 348m 53s) (5535 26%) 1.1424\n",
      "127m 46s (- 348m 39s) (5545 26%) 1.1030\n",
      "128m 0s (- 348m 26s) (5555 26%) 1.1199\n",
      "128m 14s (- 348m 12s) (5565 26%) 1.1580\n",
      "128m 28s (- 347m 58s) (5575 26%) 1.1884\n",
      "128m 42s (- 347m 44s) (5585 27%) 1.1033\n",
      "128m 56s (- 347m 30s) (5595 27%) 1.0562\n",
      "129m 9s (- 347m 16s) (5605 27%) 1.1161\n",
      "129m 23s (- 347m 3s) (5615 27%) 1.1409\n",
      "129m 37s (- 346m 49s) (5625 27%) 1.1784\n",
      "129m 51s (- 346m 35s) (5635 27%) 1.0950\n",
      "130m 5s (- 346m 21s) (5645 27%) 1.1281\n",
      "130m 19s (- 346m 7s) (5655 27%) 1.1563\n",
      "130m 32s (- 345m 53s) (5665 27%) 1.1026\n",
      "130m 46s (- 345m 39s) (5675 27%) 1.0817\n",
      "131m 0s (- 345m 26s) (5685 27%) 1.1631\n",
      "131m 14s (- 345m 12s) (5695 27%) 1.1018\n",
      "131m 28s (- 344m 58s) (5705 27%) 1.1630\n",
      "131m 41s (- 344m 44s) (5715 27%) 1.1321\n",
      "131m 55s (- 344m 30s) (5725 27%) 1.2264\n",
      "132m 9s (- 344m 16s) (5735 27%) 1.1669\n",
      "132m 23s (- 344m 3s) (5745 27%) 1.1006\n",
      "132m 37s (- 343m 49s) (5755 27%) 1.1444\n",
      "132m 51s (- 343m 35s) (5765 27%) 1.1257\n",
      "133m 4s (- 343m 21s) (5775 27%) 1.1442\n",
      "133m 18s (- 343m 7s) (5785 27%) 1.0349\n",
      "133m 32s (- 342m 54s) (5795 28%) 1.2050\n",
      "133m 46s (- 342m 40s) (5805 28%) 1.1192\n",
      "134m 0s (- 342m 26s) (5815 28%) 1.1102\n",
      "134m 14s (- 342m 12s) (5825 28%) 1.1232\n",
      "134m 27s (- 341m 58s) (5835 28%) 1.1960\n",
      "134m 41s (- 341m 44s) (5845 28%) 1.1740\n",
      "134m 55s (- 341m 31s) (5855 28%) 1.0878\n",
      "135m 9s (- 341m 17s) (5865 28%) 1.1303\n",
      "135m 23s (- 341m 3s) (5875 28%) 1.1772\n",
      "135m 37s (- 340m 49s) (5885 28%) 1.1324\n",
      "135m 50s (- 340m 35s) (5895 28%) 1.0864\n",
      "136m 4s (- 340m 22s) (5905 28%) 1.1130\n",
      "136m 18s (- 340m 8s) (5915 28%) 1.1170\n",
      "136m 32s (- 339m 54s) (5925 28%) 1.1282\n",
      "136m 46s (- 339m 40s) (5935 28%) 1.1624\n",
      "136m 59s (- 339m 26s) (5945 28%) 1.1676\n",
      "137m 13s (- 339m 12s) (5955 28%) 1.0795\n",
      "137m 27s (- 338m 58s) (5965 28%) 1.2369\n",
      "137m 41s (- 338m 45s) (5975 28%) 1.0457\n",
      "137m 55s (- 338m 31s) (5985 28%) 1.1791\n",
      "138m 9s (- 338m 17s) (5995 28%) 1.0739\n",
      "138m 22s (- 338m 3s) (6005 29%) 1.2023\n",
      "138m 36s (- 337m 49s) (6015 29%) 1.2157\n",
      "138m 50s (- 337m 35s) (6025 29%) 1.1571\n",
      "139m 4s (- 337m 21s) (6035 29%) 1.1382\n",
      "139m 18s (- 337m 7s) (6045 29%) 1.1372\n",
      "139m 31s (- 336m 54s) (6055 29%) 1.1298\n",
      "139m 45s (- 336m 40s) (6065 29%) 1.0202\n",
      "139m 59s (- 336m 26s) (6075 29%) 1.1151\n",
      "140m 13s (- 336m 12s) (6085 29%) 1.1245\n",
      "140m 27s (- 335m 58s) (6095 29%) 1.1909\n",
      "140m 40s (- 335m 44s) (6105 29%) 1.0923\n",
      "140m 54s (- 335m 30s) (6115 29%) 1.0934\n",
      "141m 8s (- 335m 16s) (6125 29%) 1.0394\n",
      "141m 22s (- 335m 3s) (6135 29%) 1.0775\n",
      "141m 36s (- 334m 49s) (6145 29%) 1.0721\n",
      "141m 49s (- 334m 35s) (6155 29%) 1.0602\n",
      "142m 3s (- 334m 21s) (6165 29%) 1.1526\n",
      "142m 17s (- 334m 7s) (6175 29%) 1.0463\n",
      "142m 31s (- 333m 53s) (6185 29%) 1.1681\n",
      "142m 45s (- 333m 39s) (6195 29%) 1.0317\n",
      "142m 58s (- 333m 26s) (6205 30%) 1.1238\n",
      "143m 12s (- 333m 12s) (6215 30%) 1.0865\n",
      "143m 26s (- 332m 58s) (6225 30%) 1.0316\n",
      "143m 40s (- 332m 44s) (6235 30%) 1.1478\n",
      "143m 54s (- 332m 30s) (6245 30%) 1.1404\n",
      "144m 7s (- 332m 16s) (6255 30%) 1.0361\n",
      "144m 21s (- 332m 2s) (6265 30%) 1.0724\n",
      "144m 35s (- 331m 48s) (6275 30%) 1.0958\n",
      "144m 49s (- 331m 35s) (6285 30%) 1.0576\n",
      "145m 3s (- 331m 21s) (6295 30%) 1.0689\n",
      "145m 16s (- 331m 7s) (6305 30%) 1.0986\n",
      "145m 30s (- 330m 53s) (6315 30%) 1.2054\n",
      "145m 44s (- 330m 39s) (6325 30%) 1.1456\n",
      "145m 58s (- 330m 25s) (6335 30%) 1.2439\n",
      "146m 12s (- 330m 11s) (6345 30%) 1.0528\n",
      "146m 25s (- 329m 57s) (6355 30%) 1.1693\n",
      "146m 39s (- 329m 43s) (6365 30%) 1.0786\n",
      "146m 53s (- 329m 29s) (6375 30%) 1.1113\n",
      "147m 7s (- 329m 15s) (6385 30%) 1.1360\n",
      "147m 21s (- 329m 1s) (6395 30%) 1.0698\n",
      "147m 34s (- 328m 48s) (6405 30%) 1.0673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147m 48s (- 328m 34s) (6415 31%) 1.0896\n",
      "148m 2s (- 328m 20s) (6425 31%) 1.0082\n",
      "148m 16s (- 328m 6s) (6435 31%) 1.1233\n",
      "148m 30s (- 327m 52s) (6445 31%) 1.1017\n",
      "148m 43s (- 327m 38s) (6455 31%) 1.0840\n",
      "148m 57s (- 327m 24s) (6465 31%) 1.1324\n",
      "149m 11s (- 327m 10s) (6475 31%) 1.0484\n",
      "149m 25s (- 326m 56s) (6485 31%) 1.0382\n",
      "149m 39s (- 326m 43s) (6495 31%) 1.1062\n",
      "149m 52s (- 326m 29s) (6505 31%) 1.0657\n",
      "150m 6s (- 326m 15s) (6515 31%) 1.1974\n",
      "150m 20s (- 326m 1s) (6525 31%) 1.1019\n",
      "150m 34s (- 325m 47s) (6535 31%) 1.1568\n",
      "150m 48s (- 325m 33s) (6545 31%) 1.0679\n",
      "151m 1s (- 325m 19s) (6555 31%) 1.1683\n",
      "151m 15s (- 325m 6s) (6565 31%) 1.0541\n",
      "151m 29s (- 324m 52s) (6575 31%) 1.0862\n",
      "151m 43s (- 324m 38s) (6585 31%) 1.1093\n",
      "151m 57s (- 324m 24s) (6595 31%) 1.0973\n",
      "152m 10s (- 324m 10s) (6605 31%) 1.0119\n",
      "152m 24s (- 323m 56s) (6615 31%) 1.1784\n",
      "152m 38s (- 323m 42s) (6625 32%) 1.0863\n",
      "152m 52s (- 323m 28s) (6635 32%) 0.9475\n",
      "153m 6s (- 323m 15s) (6645 32%) 1.0932\n",
      "153m 19s (- 323m 1s) (6655 32%) 1.1468\n",
      "153m 33s (- 322m 47s) (6665 32%) 1.1769\n",
      "153m 47s (- 322m 33s) (6675 32%) 1.1194\n",
      "154m 1s (- 322m 19s) (6685 32%) 1.1626\n",
      "154m 14s (- 322m 5s) (6695 32%) 1.0697\n",
      "154m 28s (- 321m 51s) (6705 32%) 1.1454\n",
      "154m 42s (- 321m 37s) (6715 32%) 1.0511\n",
      "154m 56s (- 321m 23s) (6725 32%) 1.0683\n",
      "155m 10s (- 321m 9s) (6735 32%) 1.0923\n",
      "155m 23s (- 320m 56s) (6745 32%) 1.1284\n",
      "155m 37s (- 320m 42s) (6755 32%) 1.1195\n",
      "155m 51s (- 320m 28s) (6765 32%) 1.1006\n",
      "156m 5s (- 320m 14s) (6775 32%) 1.0873\n",
      "156m 19s (- 320m 0s) (6785 32%) 1.1678\n",
      "156m 32s (- 319m 46s) (6795 32%) 1.0472\n",
      "156m 46s (- 319m 32s) (6805 32%) 1.1595\n",
      "157m 0s (- 319m 18s) (6815 32%) 1.1192\n",
      "157m 14s (- 319m 4s) (6825 33%) 1.0798\n",
      "157m 27s (- 318m 50s) (6835 33%) 1.1257\n",
      "157m 41s (- 318m 37s) (6845 33%) 1.0471\n",
      "157m 55s (- 318m 23s) (6855 33%) 1.1502\n",
      "158m 9s (- 318m 9s) (6865 33%) 1.1300\n",
      "158m 23s (- 317m 55s) (6875 33%) 1.0216\n",
      "158m 36s (- 317m 41s) (6885 33%) 1.0904\n",
      "158m 50s (- 317m 27s) (6895 33%) 1.1270\n",
      "159m 4s (- 317m 13s) (6905 33%) 1.0345\n",
      "159m 18s (- 316m 59s) (6915 33%) 1.0713\n",
      "159m 32s (- 316m 45s) (6925 33%) 1.0710\n",
      "159m 45s (- 316m 32s) (6935 33%) 1.1250\n",
      "159m 59s (- 316m 18s) (6945 33%) 1.1067\n",
      "160m 13s (- 316m 4s) (6955 33%) 1.1309\n",
      "160m 27s (- 315m 50s) (6965 33%) 1.1481\n",
      "160m 41s (- 315m 36s) (6975 33%) 1.0693\n",
      "160m 54s (- 315m 22s) (6985 33%) 1.0502\n",
      "161m 8s (- 315m 8s) (6995 33%) 1.0150\n",
      "161m 22s (- 314m 54s) (7005 33%) 1.1053\n",
      "161m 36s (- 314m 41s) (7015 33%) 1.0211\n",
      "161m 50s (- 314m 27s) (7025 33%) 1.0752\n",
      "162m 3s (- 314m 13s) (7035 34%) 1.0236\n",
      "162m 17s (- 313m 59s) (7045 34%) 1.0827\n",
      "162m 31s (- 313m 45s) (7055 34%) 1.0907\n",
      "162m 45s (- 313m 31s) (7065 34%) 1.0782\n",
      "162m 59s (- 313m 17s) (7075 34%) 1.1029\n",
      "163m 12s (- 313m 3s) (7085 34%) 1.1205\n",
      "163m 26s (- 312m 50s) (7095 34%) 1.1158\n",
      "163m 40s (- 312m 36s) (7105 34%) 1.0081\n",
      "163m 54s (- 312m 22s) (7115 34%) 1.0608\n",
      "164m 8s (- 312m 8s) (7125 34%) 1.0533\n",
      "164m 21s (- 311m 54s) (7135 34%) 1.0863\n",
      "164m 35s (- 311m 40s) (7145 34%) 1.1429\n",
      "164m 49s (- 311m 26s) (7155 34%) 1.1612\n",
      "165m 3s (- 311m 13s) (7165 34%) 1.0531\n",
      "165m 17s (- 310m 59s) (7175 34%) 1.0877\n",
      "165m 30s (- 310m 45s) (7185 34%) 1.1000\n",
      "165m 44s (- 310m 31s) (7195 34%) 1.0440\n",
      "165m 58s (- 310m 17s) (7205 34%) 1.0623\n",
      "166m 12s (- 310m 3s) (7215 34%) 1.0847\n",
      "166m 26s (- 309m 50s) (7225 34%) 1.0927\n",
      "166m 39s (- 309m 36s) (7235 34%) 1.0514\n",
      "166m 53s (- 309m 22s) (7245 35%) 1.0913\n",
      "167m 7s (- 309m 8s) (7255 35%) 1.0766\n",
      "167m 21s (- 308m 54s) (7265 35%) 1.1105\n",
      "167m 35s (- 308m 40s) (7275 35%) 1.1066\n",
      "167m 49s (- 308m 27s) (7285 35%) 1.0483\n",
      "168m 2s (- 308m 13s) (7295 35%) 1.0728\n",
      "168m 16s (- 307m 59s) (7305 35%) 1.0800\n",
      "168m 30s (- 307m 45s) (7315 35%) 1.0908\n",
      "168m 44s (- 307m 31s) (7325 35%) 1.0237\n",
      "168m 58s (- 307m 17s) (7335 35%) 1.0663\n",
      "169m 11s (- 307m 3s) (7345 35%) 1.0193\n",
      "169m 25s (- 306m 50s) (7355 35%) 0.9957\n",
      "169m 39s (- 306m 36s) (7365 35%) 1.0767\n",
      "169m 53s (- 306m 22s) (7375 35%) 1.0068\n",
      "170m 7s (- 306m 8s) (7385 35%) 1.0287\n",
      "170m 20s (- 305m 54s) (7395 35%) 1.1149\n",
      "170m 34s (- 305m 40s) (7405 35%) 1.1131\n",
      "170m 48s (- 305m 27s) (7415 35%) 1.0221\n",
      "171m 2s (- 305m 13s) (7425 35%) 1.1448\n",
      "171m 16s (- 304m 59s) (7435 35%) 1.0784\n",
      "171m 29s (- 304m 45s) (7445 36%) 1.0633\n",
      "171m 43s (- 304m 31s) (7455 36%) 1.0297\n",
      "171m 57s (- 304m 17s) (7465 36%) 1.0318\n",
      "172m 11s (- 304m 4s) (7475 36%) 1.0859\n",
      "172m 25s (- 303m 50s) (7485 36%) 1.0371\n",
      "172m 39s (- 303m 36s) (7495 36%) 1.1287\n",
      "172m 52s (- 303m 22s) (7505 36%) 1.0561\n",
      "173m 6s (- 303m 8s) (7515 36%) 1.1437\n",
      "173m 20s (- 302m 54s) (7525 36%) 1.0568\n",
      "173m 34s (- 302m 40s) (7535 36%) 1.0809\n",
      "173m 48s (- 302m 27s) (7545 36%) 1.1372\n",
      "174m 1s (- 302m 13s) (7555 36%) 1.1195\n",
      "174m 15s (- 301m 59s) (7565 36%) 1.0682\n",
      "174m 29s (- 301m 45s) (7575 36%) 1.0165\n",
      "174m 43s (- 301m 31s) (7585 36%) 1.1079\n",
      "174m 56s (- 301m 17s) (7595 36%) 1.1034\n",
      "175m 10s (- 301m 3s) (7605 36%) 1.0196\n",
      "175m 24s (- 300m 50s) (7615 36%) 1.0807\n",
      "175m 38s (- 300m 36s) (7625 36%) 1.0615\n",
      "175m 52s (- 300m 22s) (7635 36%) 1.0754\n",
      "176m 6s (- 300m 8s) (7645 36%) 1.1540\n",
      "176m 19s (- 299m 54s) (7655 37%) 1.1216\n",
      "176m 33s (- 299m 40s) (7665 37%) 1.0192\n",
      "176m 47s (- 299m 27s) (7675 37%) 1.0356\n",
      "177m 1s (- 299m 13s) (7685 37%) 0.9422\n",
      "177m 15s (- 298m 59s) (7695 37%) 0.9981\n",
      "177m 28s (- 298m 45s) (7705 37%) 1.0871\n",
      "177m 42s (- 298m 31s) (7715 37%) 0.9849\n",
      "177m 56s (- 298m 17s) (7725 37%) 1.0102\n",
      "178m 10s (- 298m 4s) (7735 37%) 1.0425\n",
      "178m 24s (- 297m 50s) (7745 37%) 1.0781\n",
      "178m 38s (- 297m 36s) (7755 37%) 1.0801\n",
      "178m 51s (- 297m 22s) (7765 37%) 1.0430\n",
      "179m 5s (- 297m 8s) (7775 37%) 1.1879\n",
      "179m 19s (- 296m 54s) (7785 37%) 1.0075\n",
      "179m 33s (- 296m 41s) (7795 37%) 1.0464\n",
      "179m 47s (- 296m 27s) (7805 37%) 1.1856\n",
      "180m 0s (- 296m 13s) (7815 37%) 1.0696\n",
      "180m 14s (- 295m 59s) (7825 37%) 1.0662\n",
      "180m 28s (- 295m 45s) (7835 37%) 1.0147\n",
      "180m 42s (- 295m 31s) (7845 37%) 1.0751\n",
      "180m 56s (- 295m 18s) (7855 37%) 1.0710\n",
      "181m 9s (- 295m 4s) (7865 38%) 1.0237\n",
      "181m 23s (- 294m 50s) (7875 38%) 1.0527\n",
      "181m 37s (- 294m 36s) (7885 38%) 1.0317\n",
      "181m 51s (- 294m 22s) (7895 38%) 1.1042\n",
      "182m 5s (- 294m 8s) (7905 38%) 1.0987\n",
      "182m 19s (- 293m 55s) (7915 38%) 1.0318\n",
      "182m 32s (- 293m 41s) (7925 38%) 1.1070\n",
      "182m 46s (- 293m 27s) (7935 38%) 1.0556\n",
      "183m 0s (- 293m 13s) (7945 38%) 1.0979\n",
      "183m 14s (- 292m 59s) (7955 38%) 1.0479\n",
      "183m 28s (- 292m 45s) (7965 38%) 1.0735\n",
      "183m 41s (- 292m 32s) (7975 38%) 1.0665\n",
      "183m 55s (- 292m 18s) (7985 38%) 1.0220\n",
      "184m 9s (- 292m 4s) (7995 38%) 1.0463\n",
      "184m 23s (- 291m 50s) (8005 38%) 1.0382\n",
      "184m 37s (- 291m 36s) (8015 38%) 1.0621\n",
      "184m 51s (- 291m 23s) (8025 38%) 1.1099\n",
      "185m 4s (- 291m 9s) (8035 38%) 1.0986\n",
      "185m 18s (- 290m 55s) (8045 38%) 1.0592\n",
      "185m 32s (- 290m 41s) (8055 38%) 1.0692\n",
      "185m 46s (- 290m 27s) (8065 39%) 1.0448\n",
      "186m 0s (- 290m 13s) (8075 39%) 1.0375\n",
      "186m 13s (- 290m 0s) (8085 39%) 1.0516\n",
      "186m 27s (- 289m 46s) (8095 39%) 1.0785\n",
      "186m 41s (- 289m 32s) (8105 39%) 0.9960\n",
      "186m 55s (- 289m 18s) (8115 39%) 1.0785\n",
      "187m 9s (- 289m 4s) (8125 39%) 1.0474\n",
      "187m 22s (- 288m 50s) (8135 39%) 1.0475\n",
      "187m 36s (- 288m 36s) (8145 39%) 1.1148\n",
      "187m 50s (- 288m 23s) (8155 39%) 0.9833\n",
      "188m 4s (- 288m 9s) (8165 39%) 1.1195\n",
      "188m 18s (- 287m 55s) (8175 39%) 0.9798\n",
      "188m 31s (- 287m 41s) (8185 39%) 1.0103\n",
      "188m 45s (- 287m 27s) (8195 39%) 1.1015\n",
      "188m 59s (- 287m 13s) (8205 39%) 1.0642\n",
      "189m 13s (- 287m 0s) (8215 39%) 0.9843\n",
      "189m 27s (- 286m 46s) (8225 39%) 1.0105\n",
      "189m 40s (- 286m 32s) (8235 39%) 1.0149\n",
      "189m 54s (- 286m 18s) (8245 39%) 1.0259\n",
      "190m 8s (- 286m 4s) (8255 39%) 0.9526\n",
      "190m 22s (- 285m 50s) (8265 39%) 1.0912\n",
      "190m 43s (- 285m 31s) (8280 40%) 1.5749\n",
      "190m 57s (- 285m 17s) (8290 40%) 1.0939\n",
      "191m 11s (- 285m 3s) (8300 40%) 1.0468\n",
      "191m 25s (- 284m 49s) (8310 40%) 0.9774\n",
      "191m 39s (- 284m 36s) (8320 40%) 0.9265\n",
      "191m 52s (- 284m 22s) (8330 40%) 1.0259\n",
      "192m 6s (- 284m 8s) (8340 40%) 1.0277\n",
      "192m 20s (- 283m 54s) (8350 40%) 0.9527\n",
      "192m 34s (- 283m 40s) (8360 40%) 0.9726\n",
      "192m 48s (- 283m 26s) (8370 40%) 1.0328\n",
      "193m 2s (- 283m 13s) (8380 40%) 1.0663\n",
      "193m 15s (- 282m 59s) (8390 40%) 0.9769\n",
      "193m 29s (- 282m 45s) (8400 40%) 0.9733\n",
      "193m 43s (- 282m 31s) (8410 40%) 0.9982\n",
      "193m 57s (- 282m 17s) (8420 40%) 1.0128\n",
      "194m 11s (- 282m 3s) (8430 40%) 1.0574\n",
      "194m 24s (- 281m 49s) (8440 40%) 0.9480\n",
      "194m 38s (- 281m 36s) (8450 40%) 1.0166\n",
      "194m 52s (- 281m 22s) (8460 40%) 1.0073\n",
      "195m 6s (- 281m 8s) (8470 40%) 1.0370\n",
      "195m 20s (- 280m 54s) (8480 41%) 1.0241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195m 33s (- 280m 40s) (8490 41%) 1.0037\n",
      "195m 47s (- 280m 26s) (8500 41%) 0.9952\n",
      "196m 1s (- 280m 13s) (8510 41%) 0.9995\n",
      "196m 15s (- 279m 59s) (8520 41%) 0.9702\n",
      "196m 29s (- 279m 45s) (8530 41%) 1.0186\n",
      "196m 43s (- 279m 31s) (8540 41%) 0.9927\n",
      "196m 56s (- 279m 17s) (8550 41%) 1.0153\n",
      "197m 10s (- 279m 3s) (8560 41%) 1.0368\n",
      "197m 24s (- 278m 50s) (8570 41%) 1.0390\n",
      "197m 38s (- 278m 36s) (8580 41%) 1.0566\n",
      "197m 52s (- 278m 22s) (8590 41%) 0.9987\n",
      "198m 5s (- 278m 8s) (8600 41%) 1.0223\n",
      "198m 19s (- 277m 54s) (8610 41%) 1.0238\n",
      "198m 33s (- 277m 40s) (8620 41%) 0.9576\n",
      "198m 47s (- 277m 27s) (8630 41%) 0.9314\n",
      "199m 1s (- 277m 13s) (8640 41%) 1.0070\n",
      "199m 14s (- 276m 59s) (8650 41%) 1.0224\n",
      "199m 28s (- 276m 45s) (8660 41%) 0.9712\n",
      "199m 42s (- 276m 31s) (8670 41%) 1.0511\n",
      "199m 56s (- 276m 17s) (8680 41%) 0.9347\n",
      "200m 10s (- 276m 3s) (8690 42%) 1.0696\n",
      "200m 23s (- 275m 50s) (8700 42%) 1.0024\n",
      "200m 37s (- 275m 36s) (8710 42%) 1.0165\n",
      "200m 51s (- 275m 22s) (8720 42%) 1.0697\n",
      "201m 5s (- 275m 8s) (8730 42%) 0.9856\n",
      "201m 19s (- 274m 54s) (8740 42%) 1.0416\n",
      "201m 32s (- 274m 40s) (8750 42%) 1.0527\n",
      "201m 46s (- 274m 26s) (8760 42%) 1.0576\n",
      "202m 0s (- 274m 13s) (8770 42%) 0.9693\n",
      "202m 14s (- 273m 59s) (8780 42%) 0.9777\n",
      "202m 28s (- 273m 45s) (8790 42%) 1.0147\n",
      "202m 41s (- 273m 31s) (8800 42%) 0.9809\n",
      "202m 55s (- 273m 17s) (8810 42%) 0.9954\n",
      "203m 9s (- 273m 3s) (8820 42%) 0.9590\n",
      "203m 23s (- 272m 49s) (8830 42%) 0.9923\n",
      "203m 36s (- 272m 36s) (8840 42%) 1.0587\n",
      "203m 50s (- 272m 22s) (8850 42%) 1.0316\n",
      "204m 4s (- 272m 8s) (8860 42%) 1.0138\n",
      "204m 18s (- 271m 54s) (8870 42%) 1.0654\n",
      "204m 32s (- 271m 40s) (8880 42%) 0.9262\n",
      "204m 46s (- 271m 26s) (8890 42%) 1.0183\n",
      "204m 59s (- 271m 13s) (8900 43%) 1.1207\n",
      "205m 13s (- 270m 59s) (8910 43%) 0.9681\n",
      "205m 27s (- 270m 45s) (8920 43%) 1.0397\n",
      "205m 41s (- 270m 31s) (8930 43%) 1.0000\n",
      "205m 54s (- 270m 17s) (8940 43%) 1.0037\n",
      "206m 8s (- 270m 3s) (8950 43%) 1.0140\n",
      "206m 22s (- 269m 49s) (8960 43%) 1.0143\n",
      "206m 36s (- 269m 36s) (8970 43%) 1.0719\n",
      "206m 50s (- 269m 22s) (8980 43%) 1.0374\n",
      "207m 3s (- 269m 8s) (8990 43%) 0.9954\n",
      "207m 17s (- 268m 54s) (9000 43%) 1.0093\n",
      "207m 31s (- 268m 40s) (9010 43%) 1.0222\n",
      "207m 45s (- 268m 26s) (9020 43%) 0.9959\n",
      "207m 59s (- 268m 13s) (9030 43%) 0.9986\n",
      "208m 12s (- 267m 59s) (9040 43%) 0.9993\n",
      "208m 26s (- 267m 45s) (9050 43%) 0.9953\n",
      "208m 40s (- 267m 31s) (9060 43%) 0.9690\n",
      "208m 54s (- 267m 17s) (9070 43%) 1.0161\n",
      "209m 8s (- 267m 3s) (9080 43%) 0.9214\n",
      "209m 21s (- 266m 49s) (9090 43%) 0.9984\n",
      "209m 35s (- 266m 36s) (9100 44%) 1.0576\n",
      "209m 49s (- 266m 22s) (9110 44%) 0.9488\n",
      "210m 3s (- 266m 8s) (9120 44%) 1.0599\n",
      "210m 17s (- 265m 54s) (9130 44%) 0.9873\n",
      "210m 30s (- 265m 40s) (9140 44%) 0.9979\n",
      "210m 44s (- 265m 26s) (9150 44%) 1.0146\n",
      "210m 58s (- 265m 12s) (9160 44%) 0.9347\n",
      "211m 12s (- 264m 59s) (9170 44%) 0.9768\n",
      "211m 26s (- 264m 45s) (9180 44%) 0.9692\n",
      "211m 39s (- 264m 31s) (9190 44%) 0.9476\n",
      "211m 53s (- 264m 17s) (9200 44%) 1.1232\n",
      "212m 7s (- 264m 3s) (9210 44%) 0.9725\n",
      "212m 21s (- 263m 49s) (9220 44%) 1.0534\n",
      "212m 35s (- 263m 36s) (9230 44%) 0.9399\n",
      "212m 48s (- 263m 22s) (9240 44%) 1.0597\n",
      "213m 2s (- 263m 8s) (9250 44%) 0.9841\n",
      "213m 16s (- 262m 54s) (9260 44%) 1.0689\n",
      "213m 30s (- 262m 40s) (9270 44%) 0.9793\n",
      "213m 44s (- 262m 26s) (9280 44%) 1.0126\n",
      "213m 57s (- 262m 13s) (9290 44%) 0.9921\n",
      "214m 11s (- 261m 59s) (9300 44%) 0.9965\n",
      "214m 25s (- 261m 45s) (9310 45%) 1.0129\n",
      "214m 39s (- 261m 31s) (9320 45%) 0.9449\n",
      "214m 53s (- 261m 17s) (9330 45%) 0.9943\n",
      "215m 7s (- 261m 3s) (9340 45%) 1.0707\n",
      "215m 20s (- 260m 50s) (9350 45%) 0.9928\n",
      "215m 34s (- 260m 36s) (9360 45%) 1.0168\n",
      "215m 48s (- 260m 22s) (9370 45%) 0.9952\n",
      "216m 2s (- 260m 8s) (9380 45%) 0.9852\n",
      "216m 16s (- 259m 54s) (9390 45%) 0.9941\n",
      "216m 29s (- 259m 40s) (9400 45%) 0.9505\n",
      "216m 43s (- 259m 27s) (9410 45%) 0.9954\n",
      "216m 57s (- 259m 13s) (9420 45%) 0.9437\n",
      "217m 11s (- 258m 59s) (9430 45%) 0.9939\n",
      "217m 25s (- 258m 45s) (9440 45%) 0.9692\n",
      "217m 38s (- 258m 31s) (9450 45%) 1.0269\n",
      "217m 52s (- 258m 17s) (9460 45%) 0.9934\n",
      "218m 6s (- 258m 4s) (9470 45%) 0.9523\n",
      "218m 20s (- 257m 50s) (9480 45%) 0.9718\n",
      "218m 34s (- 257m 36s) (9490 45%) 1.0528\n",
      "218m 47s (- 257m 22s) (9500 45%) 0.9199\n",
      "219m 1s (- 257m 8s) (9510 45%) 0.9619\n",
      "219m 15s (- 256m 54s) (9520 46%) 0.9199\n",
      "219m 29s (- 256m 41s) (9530 46%) 1.0770\n",
      "219m 43s (- 256m 27s) (9540 46%) 1.0139\n",
      "219m 56s (- 256m 13s) (9550 46%) 0.9680\n",
      "220m 10s (- 255m 59s) (9560 46%) 0.9993\n",
      "220m 24s (- 255m 45s) (9570 46%) 0.9984\n",
      "220m 38s (- 255m 31s) (9580 46%) 0.9590\n",
      "220m 52s (- 255m 17s) (9590 46%) 0.9522\n",
      "221m 5s (- 255m 4s) (9600 46%) 0.9911\n",
      "221m 19s (- 254m 50s) (9610 46%) 0.9470\n",
      "221m 33s (- 254m 36s) (9620 46%) 0.9165\n",
      "221m 47s (- 254m 22s) (9630 46%) 0.9652\n",
      "222m 1s (- 254m 8s) (9640 46%) 0.9126\n",
      "222m 14s (- 253m 54s) (9650 46%) 1.0042\n",
      "222m 28s (- 253m 41s) (9660 46%) 0.9391\n",
      "222m 42s (- 253m 27s) (9670 46%) 0.9458\n",
      "222m 56s (- 253m 13s) (9680 46%) 0.9723\n",
      "223m 10s (- 252m 59s) (9690 46%) 0.9814\n",
      "223m 23s (- 252m 45s) (9700 46%) 0.9967\n",
      "223m 37s (- 252m 31s) (9710 46%) 1.0143\n",
      "223m 51s (- 252m 18s) (9720 47%) 1.0057\n",
      "224m 5s (- 252m 4s) (9730 47%) 0.9383\n",
      "224m 19s (- 251m 50s) (9740 47%) 1.0708\n",
      "224m 32s (- 251m 36s) (9750 47%) 0.9993\n",
      "224m 46s (- 251m 22s) (9760 47%) 0.9967\n",
      "225m 0s (- 251m 8s) (9770 47%) 1.0358\n",
      "225m 14s (- 250m 55s) (9780 47%) 0.9909\n",
      "225m 28s (- 250m 41s) (9790 47%) 1.0159\n",
      "225m 41s (- 250m 27s) (9800 47%) 0.9261\n",
      "225m 55s (- 250m 13s) (9810 47%) 1.0062\n",
      "226m 9s (- 249m 59s) (9820 47%) 1.0024\n",
      "226m 23s (- 249m 45s) (9830 47%) 1.0310\n",
      "226m 36s (- 249m 31s) (9840 47%) 1.0322\n",
      "226m 50s (- 249m 17s) (9850 47%) 0.9585\n",
      "227m 4s (- 249m 4s) (9860 47%) 1.0108\n",
      "227m 18s (- 248m 50s) (9870 47%) 0.9763\n",
      "227m 32s (- 248m 36s) (9880 47%) 0.9261\n",
      "227m 45s (- 248m 22s) (9890 47%) 1.0720\n",
      "227m 59s (- 248m 8s) (9900 47%) 0.9429\n",
      "228m 13s (- 247m 54s) (9910 47%) 0.9395\n",
      "228m 27s (- 247m 40s) (9920 47%) 1.0429\n",
      "228m 40s (- 247m 27s) (9930 48%) 0.9602\n",
      "228m 54s (- 247m 13s) (9940 48%) 0.9555\n",
      "229m 8s (- 246m 59s) (9950 48%) 0.9955\n",
      "229m 22s (- 246m 45s) (9960 48%) 1.0307\n",
      "229m 36s (- 246m 31s) (9970 48%) 0.9931\n",
      "229m 49s (- 246m 17s) (9980 48%) 0.8883\n",
      "230m 3s (- 246m 3s) (9990 48%) 1.0170\n",
      "230m 17s (- 245m 50s) (10000 48%) 0.9375\n",
      "230m 31s (- 245m 36s) (10010 48%) 0.9450\n",
      "230m 45s (- 245m 22s) (10020 48%) 0.9780\n",
      "230m 58s (- 245m 8s) (10030 48%) 0.9260\n",
      "231m 12s (- 244m 54s) (10040 48%) 1.0320\n",
      "231m 26s (- 244m 40s) (10050 48%) 0.9951\n",
      "231m 40s (- 244m 27s) (10060 48%) 0.9842\n",
      "231m 54s (- 244m 13s) (10070 48%) 1.0004\n",
      "232m 7s (- 243m 59s) (10080 48%) 0.9613\n",
      "232m 21s (- 243m 45s) (10090 48%) 0.9364\n",
      "232m 35s (- 243m 31s) (10100 48%) 0.9230\n",
      "232m 49s (- 243m 17s) (10110 48%) 0.9445\n",
      "233m 2s (- 243m 4s) (10120 48%) 1.0479\n",
      "233m 16s (- 242m 50s) (10130 48%) 1.0377\n",
      "233m 30s (- 242m 36s) (10140 49%) 0.9821\n",
      "233m 44s (- 242m 22s) (10150 49%) 0.9599\n",
      "233m 58s (- 242m 8s) (10160 49%) 1.0208\n",
      "234m 11s (- 241m 54s) (10170 49%) 1.0003\n",
      "234m 25s (- 241m 40s) (10180 49%) 0.9938\n",
      "234m 39s (- 241m 27s) (10190 49%) 1.0861\n",
      "234m 53s (- 241m 13s) (10200 49%) 0.9419\n",
      "235m 7s (- 240m 59s) (10210 49%) 0.9828\n",
      "235m 20s (- 240m 45s) (10220 49%) 1.0645\n",
      "235m 34s (- 240m 31s) (10230 49%) 0.9804\n",
      "235m 48s (- 240m 17s) (10240 49%) 1.0157\n",
      "236m 2s (- 240m 4s) (10250 49%) 0.9741\n",
      "236m 16s (- 239m 50s) (10260 49%) 0.9104\n",
      "236m 29s (- 239m 36s) (10270 49%) 0.9969\n",
      "236m 43s (- 239m 22s) (10280 49%) 0.9975\n",
      "236m 57s (- 239m 8s) (10290 49%) 0.9010\n",
      "237m 11s (- 238m 54s) (10300 49%) 0.9686\n",
      "237m 25s (- 238m 41s) (10310 49%) 1.0350\n",
      "237m 38s (- 238m 27s) (10320 49%) 0.9536\n",
      "237m 52s (- 238m 13s) (10330 49%) 1.0153\n",
      "238m 6s (- 237m 59s) (10340 50%) 1.0338\n",
      "238m 20s (- 237m 45s) (10350 50%) 1.0437\n",
      "238m 34s (- 237m 31s) (10360 50%) 0.9169\n",
      "238m 47s (- 237m 18s) (10370 50%) 1.0132\n",
      "239m 1s (- 237m 4s) (10380 50%) 1.0119\n",
      "239m 15s (- 236m 50s) (10390 50%) 0.9647\n",
      "239m 29s (- 236m 36s) (10400 50%) 1.0183\n",
      "239m 42s (- 236m 22s) (10410 50%) 0.9425\n",
      "239m 56s (- 236m 8s) (10420 50%) 0.9912\n",
      "240m 10s (- 235m 54s) (10430 50%) 1.0083\n",
      "240m 24s (- 235m 41s) (10440 50%) 0.9907\n",
      "240m 38s (- 235m 27s) (10450 50%) 0.9833\n",
      "240m 51s (- 235m 13s) (10460 50%) 1.0447\n",
      "241m 5s (- 234m 59s) (10470 50%) 0.9986\n",
      "241m 19s (- 234m 45s) (10480 50%) 0.9827\n",
      "241m 33s (- 234m 31s) (10490 50%) 1.0464\n",
      "241m 47s (- 234m 18s) (10500 50%) 0.9604\n",
      "242m 0s (- 234m 4s) (10510 50%) 0.9940\n",
      "242m 14s (- 233m 50s) (10520 50%) 0.9840\n",
      "242m 28s (- 233m 36s) (10530 50%) 0.9859\n",
      "242m 42s (- 233m 22s) (10540 50%) 0.9501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242m 55s (- 233m 8s) (10550 51%) 0.9057\n",
      "243m 9s (- 232m 54s) (10560 51%) 0.8996\n",
      "243m 23s (- 232m 41s) (10570 51%) 0.9312\n",
      "243m 37s (- 232m 27s) (10580 51%) 0.9321\n",
      "243m 51s (- 232m 13s) (10590 51%) 0.9647\n",
      "244m 4s (- 231m 59s) (10600 51%) 1.0444\n",
      "244m 18s (- 231m 45s) (10610 51%) 1.0280\n",
      "244m 32s (- 231m 31s) (10620 51%) 0.9134\n",
      "244m 46s (- 231m 17s) (10630 51%) 0.9579\n",
      "244m 59s (- 231m 4s) (10640 51%) 0.9465\n",
      "245m 13s (- 230m 50s) (10650 51%) 0.9512\n",
      "245m 27s (- 230m 36s) (10660 51%) 0.9872\n",
      "245m 41s (- 230m 22s) (10670 51%) 1.0280\n",
      "245m 55s (- 230m 8s) (10680 51%) 0.9980\n",
      "246m 8s (- 229m 54s) (10690 51%) 0.9734\n",
      "246m 22s (- 229m 41s) (10700 51%) 1.0026\n",
      "246m 36s (- 229m 27s) (10710 51%) 0.9982\n",
      "246m 50s (- 229m 13s) (10720 51%) 1.0577\n",
      "247m 4s (- 228m 59s) (10730 51%) 0.9892\n",
      "247m 17s (- 228m 45s) (10740 51%) 1.0370\n",
      "247m 31s (- 228m 31s) (10750 51%) 0.9489\n",
      "247m 45s (- 228m 18s) (10760 52%) 1.0047\n",
      "247m 59s (- 228m 4s) (10770 52%) 0.9794\n",
      "248m 13s (- 227m 50s) (10780 52%) 1.0088\n",
      "248m 26s (- 227m 36s) (10790 52%) 0.9742\n",
      "248m 40s (- 227m 22s) (10800 52%) 0.9312\n",
      "248m 54s (- 227m 8s) (10810 52%) 0.9584\n",
      "249m 8s (- 226m 55s) (10820 52%) 0.9512\n",
      "249m 22s (- 226m 41s) (10830 52%) 1.0457\n",
      "249m 35s (- 226m 27s) (10840 52%) 0.9832\n",
      "249m 49s (- 226m 13s) (10850 52%) 0.9211\n",
      "250m 3s (- 225m 59s) (10860 52%) 0.9443\n",
      "250m 17s (- 225m 45s) (10870 52%) 0.9468\n",
      "250m 31s (- 225m 32s) (10880 52%) 1.0024\n",
      "250m 44s (- 225m 18s) (10890 52%) 1.0473\n",
      "250m 58s (- 225m 4s) (10900 52%) 0.9444\n",
      "251m 12s (- 224m 50s) (10910 52%) 1.0391\n",
      "251m 26s (- 224m 36s) (10920 52%) 0.9835\n",
      "251m 39s (- 224m 22s) (10930 52%) 0.9998\n",
      "251m 53s (- 224m 9s) (10940 52%) 0.9885\n",
      "252m 7s (- 223m 55s) (10950 52%) 0.9981\n",
      "252m 21s (- 223m 41s) (10960 53%) 0.9337\n",
      "252m 35s (- 223m 27s) (10970 53%) 0.9228\n",
      "252m 48s (- 223m 13s) (10980 53%) 0.9933\n",
      "253m 2s (- 222m 59s) (10990 53%) 0.9298\n",
      "253m 16s (- 222m 46s) (11000 53%) 1.0335\n",
      "253m 30s (- 222m 32s) (11010 53%) 1.0080\n",
      "253m 44s (- 222m 18s) (11020 53%) 0.9439\n",
      "253m 57s (- 222m 4s) (11030 53%) 0.9735\n",
      "254m 11s (- 221m 50s) (11040 53%) 0.9486\n",
      "254m 25s (- 221m 36s) (11050 53%) 0.9593\n",
      "254m 39s (- 221m 22s) (11060 53%) 0.9343\n",
      "254m 52s (- 221m 9s) (11070 53%) 0.9519\n",
      "255m 6s (- 220m 55s) (11080 53%) 1.0149\n",
      "255m 20s (- 220m 41s) (11090 53%) 0.8913\n",
      "255m 34s (- 220m 27s) (11100 53%) 1.0680\n",
      "255m 48s (- 220m 13s) (11110 53%) 0.9325\n",
      "256m 1s (- 219m 59s) (11120 53%) 0.9818\n",
      "256m 15s (- 219m 46s) (11130 53%) 1.0057\n",
      "256m 29s (- 219m 32s) (11140 53%) 0.9486\n",
      "256m 43s (- 219m 18s) (11150 53%) 0.9573\n",
      "256m 57s (- 219m 4s) (11160 53%) 1.1002\n",
      "257m 10s (- 218m 50s) (11170 54%) 0.9654\n",
      "257m 24s (- 218m 36s) (11180 54%) 1.0331\n",
      "257m 38s (- 218m 23s) (11190 54%) 0.9822\n",
      "257m 52s (- 218m 9s) (11200 54%) 0.9748\n",
      "258m 5s (- 217m 55s) (11210 54%) 0.9588\n",
      "258m 19s (- 217m 41s) (11220 54%) 1.0279\n",
      "258m 33s (- 217m 27s) (11230 54%) 0.9819\n",
      "258m 47s (- 217m 13s) (11240 54%) 0.9486\n",
      "259m 1s (- 216m 59s) (11250 54%) 1.0060\n",
      "259m 14s (- 216m 46s) (11260 54%) 0.9361\n",
      "259m 28s (- 216m 32s) (11270 54%) 0.9709\n",
      "259m 42s (- 216m 18s) (11280 54%) 0.9637\n",
      "259m 56s (- 216m 4s) (11290 54%) 0.9480\n",
      "260m 9s (- 215m 50s) (11300 54%) 1.0085\n",
      "260m 23s (- 215m 36s) (11310 54%) 0.9777\n",
      "260m 37s (- 215m 23s) (11320 54%) 0.8968\n",
      "260m 51s (- 215m 9s) (11330 54%) 0.9447\n",
      "261m 5s (- 214m 55s) (11340 54%) 0.9678\n",
      "261m 18s (- 214m 41s) (11350 54%) 0.9387\n",
      "261m 32s (- 214m 27s) (11360 54%) 0.9376\n",
      "261m 46s (- 214m 13s) (11370 54%) 0.9785\n",
      "262m 0s (- 214m 0s) (11380 55%) 0.9260\n",
      "262m 14s (- 213m 46s) (11390 55%) 0.9452\n",
      "262m 27s (- 213m 32s) (11400 55%) 0.9591\n",
      "262m 41s (- 213m 18s) (11410 55%) 0.9396\n",
      "262m 55s (- 213m 4s) (11420 55%) 0.9992\n",
      "263m 9s (- 212m 50s) (11430 55%) 0.9885\n",
      "263m 23s (- 212m 37s) (11440 55%) 0.9470\n",
      "263m 36s (- 212m 23s) (11450 55%) 0.9506\n",
      "263m 50s (- 212m 9s) (11460 55%) 1.0088\n",
      "264m 4s (- 211m 55s) (11470 55%) 0.9649\n",
      "264m 18s (- 211m 41s) (11480 55%) 0.9310\n",
      "264m 31s (- 211m 27s) (11490 55%) 0.9478\n",
      "264m 45s (- 211m 14s) (11500 55%) 0.9344\n",
      "264m 59s (- 211m 0s) (11510 55%) 0.9715\n",
      "265m 13s (- 210m 46s) (11520 55%) 0.9146\n",
      "265m 27s (- 210m 32s) (11530 55%) 0.9674\n",
      "265m 41s (- 210m 18s) (11540 55%) 1.0044\n",
      "265m 54s (- 210m 4s) (11550 55%) 1.0354\n",
      "266m 8s (- 209m 51s) (11560 55%) 0.9316\n",
      "266m 22s (- 209m 37s) (11570 55%) 1.0275\n",
      "266m 36s (- 209m 23s) (11580 56%) 1.0169\n",
      "266m 49s (- 209m 9s) (11590 56%) 1.0074\n",
      "267m 3s (- 208m 55s) (11600 56%) 0.9381\n",
      "267m 17s (- 208m 41s) (11610 56%) 0.9873\n",
      "267m 31s (- 208m 28s) (11620 56%) 0.9594\n",
      "267m 45s (- 208m 14s) (11630 56%) 0.9693\n",
      "267m 58s (- 208m 0s) (11640 56%) 0.9308\n",
      "268m 12s (- 207m 46s) (11650 56%) 0.9578\n",
      "268m 26s (- 207m 32s) (11660 56%) 0.9263\n",
      "268m 40s (- 207m 19s) (11670 56%) 0.9418\n",
      "268m 54s (- 207m 5s) (11680 56%) 0.9060\n",
      "269m 7s (- 206m 51s) (11690 56%) 0.9690\n",
      "269m 21s (- 206m 37s) (11700 56%) 0.9940\n",
      "269m 35s (- 206m 23s) (11710 56%) 0.9286\n",
      "269m 49s (- 206m 9s) (11720 56%) 0.9642\n",
      "270m 3s (- 205m 56s) (11730 56%) 0.9156\n",
      "270m 16s (- 205m 42s) (11740 56%) 0.9404\n",
      "270m 30s (- 205m 28s) (11750 56%) 0.9523\n",
      "270m 44s (- 205m 14s) (11760 56%) 0.9683\n",
      "270m 58s (- 205m 0s) (11770 56%) 1.0521\n",
      "271m 11s (- 204m 46s) (11780 56%) 0.9699\n",
      "271m 25s (- 204m 33s) (11790 57%) 0.9718\n",
      "271m 39s (- 204m 19s) (11800 57%) 0.9896\n",
      "271m 53s (- 204m 5s) (11810 57%) 0.9516\n",
      "272m 7s (- 203m 51s) (11820 57%) 0.9793\n",
      "272m 20s (- 203m 37s) (11830 57%) 0.9660\n",
      "272m 34s (- 203m 23s) (11840 57%) 1.0210\n",
      "272m 48s (- 203m 10s) (11850 57%) 0.9461\n",
      "273m 2s (- 202m 56s) (11860 57%) 0.9117\n",
      "273m 16s (- 202m 42s) (11870 57%) 1.0058\n",
      "273m 29s (- 202m 28s) (11880 57%) 0.9701\n",
      "273m 43s (- 202m 14s) (11890 57%) 0.9646\n",
      "273m 57s (- 202m 0s) (11900 57%) 0.9128\n",
      "274m 11s (- 201m 47s) (11910 57%) 0.9390\n",
      "274m 25s (- 201m 33s) (11920 57%) 0.9597\n",
      "274m 38s (- 201m 19s) (11930 57%) 1.0058\n",
      "274m 52s (- 201m 5s) (11940 57%) 0.9673\n",
      "275m 6s (- 200m 51s) (11950 57%) 0.9534\n",
      "275m 20s (- 200m 37s) (11960 57%) 0.9217\n",
      "275m 34s (- 200m 24s) (11970 57%) 0.9730\n",
      "275m 47s (- 200m 10s) (11980 57%) 0.9578\n",
      "276m 1s (- 199m 56s) (11990 57%) 0.9039\n",
      "276m 15s (- 199m 42s) (12000 58%) 0.9899\n",
      "276m 29s (- 199m 28s) (12010 58%) 1.0368\n",
      "276m 43s (- 199m 15s) (12020 58%) 0.9335\n",
      "276m 56s (- 199m 1s) (12030 58%) 0.9929\n",
      "277m 10s (- 198m 47s) (12040 58%) 0.9395\n",
      "277m 24s (- 198m 33s) (12050 58%) 1.0343\n",
      "277m 38s (- 198m 19s) (12060 58%) 0.9288\n",
      "277m 51s (- 198m 5s) (12070 58%) 0.9399\n",
      "278m 5s (- 197m 52s) (12080 58%) 1.0036\n",
      "278m 19s (- 197m 38s) (12090 58%) 0.9831\n",
      "278m 33s (- 197m 24s) (12100 58%) 1.0229\n",
      "278m 47s (- 197m 10s) (12110 58%) 0.9538\n",
      "279m 0s (- 196m 56s) (12120 58%) 0.9506\n",
      "279m 14s (- 196m 42s) (12130 58%) 0.8985\n",
      "279m 28s (- 196m 29s) (12140 58%) 0.9428\n",
      "279m 42s (- 196m 15s) (12150 58%) 0.8904\n",
      "279m 56s (- 196m 1s) (12160 58%) 0.9944\n",
      "280m 9s (- 195m 47s) (12170 58%) 0.9283\n",
      "280m 23s (- 195m 33s) (12180 58%) 0.9263\n",
      "280m 37s (- 195m 19s) (12190 58%) 0.9534\n",
      "280m 51s (- 195m 6s) (12200 59%) 0.9557\n",
      "281m 5s (- 194m 52s) (12210 59%) 0.9134\n",
      "281m 18s (- 194m 38s) (12220 59%) 0.9919\n",
      "281m 32s (- 194m 24s) (12230 59%) 0.9664\n",
      "281m 46s (- 194m 10s) (12240 59%) 0.9883\n",
      "282m 0s (- 193m 57s) (12250 59%) 0.9631\n",
      "282m 14s (- 193m 43s) (12260 59%) 0.9597\n",
      "282m 27s (- 193m 29s) (12270 59%) 0.8888\n",
      "282m 41s (- 193m 15s) (12280 59%) 1.0000\n",
      "282m 55s (- 193m 1s) (12290 59%) 0.9422\n",
      "283m 9s (- 192m 47s) (12300 59%) 0.8739\n",
      "283m 23s (- 192m 34s) (12310 59%) 0.9652\n",
      "283m 36s (- 192m 20s) (12320 59%) 0.9421\n",
      "283m 50s (- 192m 6s) (12330 59%) 0.9454\n",
      "284m 4s (- 191m 52s) (12340 59%) 0.9636\n",
      "284m 18s (- 191m 38s) (12350 59%) 0.9816\n",
      "284m 31s (- 191m 24s) (12360 59%) 1.0034\n",
      "284m 45s (- 191m 11s) (12370 59%) 0.8384\n",
      "284m 59s (- 190m 57s) (12380 59%) 1.0208\n",
      "285m 13s (- 190m 43s) (12390 59%) 0.9139\n",
      "285m 27s (- 190m 29s) (12400 59%) 0.9393\n",
      "285m 48s (- 190m 9s) (12415 60%) 1.3429\n",
      "286m 2s (- 189m 55s) (12425 60%) 0.9050\n",
      "286m 16s (- 189m 41s) (12435 60%) 0.9947\n",
      "286m 30s (- 189m 27s) (12445 60%) 0.9659\n",
      "286m 43s (- 189m 14s) (12455 60%) 0.8794\n",
      "286m 57s (- 189m 0s) (12465 60%) 0.8482\n",
      "287m 11s (- 188m 46s) (12475 60%) 0.9056\n",
      "287m 25s (- 188m 32s) (12485 60%) 0.9078\n",
      "287m 38s (- 188m 18s) (12495 60%) 0.8832\n",
      "287m 52s (- 188m 4s) (12505 60%) 0.9083\n",
      "288m 6s (- 187m 51s) (12515 60%) 0.9553\n",
      "288m 20s (- 187m 37s) (12525 60%) 0.8899\n",
      "288m 33s (- 187m 23s) (12535 60%) 0.8753\n",
      "288m 47s (- 187m 9s) (12545 60%) 0.9232\n",
      "289m 1s (- 186m 55s) (12555 60%) 0.9199\n",
      "289m 15s (- 186m 41s) (12565 60%) 0.8981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289m 29s (- 186m 27s) (12575 60%) 0.9419\n",
      "289m 42s (- 186m 14s) (12585 60%) 0.9371\n",
      "289m 56s (- 186m 0s) (12595 60%) 0.9084\n",
      "290m 10s (- 185m 46s) (12605 60%) 0.9234\n",
      "290m 24s (- 185m 32s) (12615 61%) 0.8913\n",
      "290m 38s (- 185m 18s) (12625 61%) 0.9254\n",
      "290m 51s (- 185m 5s) (12635 61%) 1.0265\n",
      "291m 5s (- 184m 51s) (12645 61%) 0.8459\n",
      "291m 19s (- 184m 37s) (12655 61%) 0.8899\n",
      "291m 33s (- 184m 23s) (12665 61%) 0.9207\n",
      "291m 47s (- 184m 9s) (12675 61%) 0.9233\n",
      "292m 1s (- 183m 56s) (12685 61%) 0.9144\n",
      "292m 14s (- 183m 42s) (12695 61%) 0.8948\n",
      "292m 28s (- 183m 28s) (12705 61%) 0.9201\n",
      "292m 42s (- 183m 14s) (12715 61%) 0.8955\n",
      "292m 56s (- 183m 0s) (12725 61%) 0.8396\n",
      "293m 10s (- 182m 47s) (12735 61%) 0.9376\n",
      "293m 24s (- 182m 33s) (12745 61%) 0.8477\n",
      "293m 37s (- 182m 19s) (12755 61%) 0.8952\n",
      "293m 51s (- 182m 5s) (12765 61%) 0.9660\n",
      "294m 5s (- 181m 51s) (12775 61%) 0.8940\n",
      "294m 19s (- 181m 38s) (12785 61%) 0.9024\n",
      "294m 33s (- 181m 24s) (12795 61%) 0.8943\n",
      "294m 46s (- 181m 10s) (12805 61%) 0.9079\n",
      "295m 0s (- 180m 56s) (12815 61%) 0.8576\n",
      "295m 14s (- 180m 42s) (12825 62%) 0.8702\n",
      "295m 28s (- 180m 29s) (12835 62%) 0.8472\n",
      "295m 42s (- 180m 15s) (12845 62%) 0.9150\n",
      "295m 55s (- 180m 1s) (12855 62%) 0.8926\n",
      "296m 9s (- 179m 47s) (12865 62%) 0.9355\n",
      "296m 23s (- 179m 33s) (12875 62%) 0.8954\n",
      "296m 37s (- 179m 19s) (12885 62%) 0.8619\n",
      "296m 51s (- 179m 6s) (12895 62%) 0.9486\n",
      "297m 5s (- 178m 52s) (12905 62%) 0.9380\n",
      "297m 18s (- 178m 38s) (12915 62%) 0.9087\n",
      "297m 32s (- 178m 24s) (12925 62%) 0.8966\n",
      "297m 46s (- 178m 10s) (12935 62%) 0.8595\n",
      "298m 0s (- 177m 57s) (12945 62%) 0.8644\n",
      "298m 14s (- 177m 43s) (12955 62%) 0.9592\n",
      "298m 27s (- 177m 29s) (12965 62%) 0.9367\n",
      "298m 41s (- 177m 15s) (12975 62%) 0.8918\n",
      "298m 55s (- 177m 1s) (12985 62%) 0.8684\n",
      "299m 9s (- 176m 47s) (12995 62%) 0.9003\n",
      "299m 23s (- 176m 34s) (13005 62%) 0.8874\n",
      "299m 36s (- 176m 20s) (13015 62%) 0.9955\n",
      "299m 50s (- 176m 6s) (13025 62%) 0.9259\n",
      "300m 4s (- 175m 52s) (13035 63%) 0.9638\n",
      "300m 18s (- 175m 38s) (13045 63%) 0.9113\n",
      "300m 32s (- 175m 25s) (13055 63%) 0.8531\n",
      "300m 45s (- 175m 11s) (13065 63%) 0.9045\n",
      "300m 59s (- 174m 57s) (13075 63%) 0.8751\n",
      "301m 13s (- 174m 43s) (13085 63%) 0.9266\n",
      "301m 27s (- 174m 29s) (13095 63%) 0.8824\n",
      "301m 41s (- 174m 16s) (13105 63%) 0.8625\n",
      "301m 54s (- 174m 2s) (13115 63%) 0.9036\n",
      "302m 8s (- 173m 48s) (13125 63%) 0.9703\n",
      "302m 22s (- 173m 34s) (13135 63%) 0.9087\n",
      "302m 36s (- 173m 20s) (13145 63%) 0.9281\n",
      "302m 50s (- 173m 6s) (13155 63%) 0.9531\n",
      "303m 4s (- 172m 53s) (13165 63%) 0.9290\n",
      "303m 17s (- 172m 39s) (13175 63%) 0.8921\n",
      "303m 31s (- 172m 25s) (13185 63%) 0.9527\n",
      "303m 45s (- 172m 11s) (13195 63%) 0.9045\n",
      "303m 59s (- 171m 57s) (13205 63%) 0.9290\n",
      "304m 13s (- 171m 44s) (13215 63%) 0.8857\n",
      "304m 27s (- 171m 30s) (13225 63%) 0.9322\n",
      "304m 40s (- 171m 16s) (13235 64%) 0.9006\n",
      "304m 54s (- 171m 2s) (13245 64%) 0.8376\n",
      "305m 8s (- 170m 48s) (13255 64%) 0.8111\n",
      "305m 22s (- 170m 35s) (13265 64%) 0.9605\n",
      "305m 36s (- 170m 21s) (13275 64%) 0.8802\n",
      "305m 49s (- 170m 7s) (13285 64%) 0.8716\n",
      "306m 3s (- 169m 53s) (13295 64%) 0.9376\n",
      "306m 17s (- 169m 39s) (13305 64%) 0.8373\n",
      "306m 31s (- 169m 26s) (13315 64%) 0.8954\n",
      "306m 45s (- 169m 12s) (13325 64%) 0.9235\n",
      "306m 59s (- 168m 58s) (13335 64%) 0.9385\n",
      "307m 12s (- 168m 44s) (13345 64%) 0.9373\n",
      "307m 26s (- 168m 30s) (13355 64%) 0.8792\n",
      "307m 40s (- 168m 16s) (13365 64%) 0.9275\n",
      "307m 54s (- 168m 3s) (13375 64%) 0.9056\n",
      "308m 8s (- 167m 49s) (13385 64%) 0.9048\n",
      "308m 21s (- 167m 35s) (13395 64%) 0.9048\n",
      "308m 35s (- 167m 21s) (13405 64%) 0.9416\n",
      "308m 49s (- 167m 7s) (13415 64%) 0.8511\n",
      "309m 3s (- 166m 54s) (13425 64%) 0.9300\n",
      "309m 17s (- 166m 40s) (13435 64%) 0.8791\n",
      "309m 30s (- 166m 26s) (13445 65%) 0.9270\n",
      "309m 44s (- 166m 12s) (13455 65%) 0.9307\n",
      "309m 58s (- 165m 58s) (13465 65%) 0.9421\n",
      "310m 12s (- 165m 45s) (13475 65%) 0.9436\n",
      "310m 26s (- 165m 31s) (13485 65%) 0.8941\n",
      "310m 40s (- 165m 17s) (13495 65%) 0.9129\n",
      "310m 53s (- 165m 3s) (13505 65%) 0.9646\n",
      "311m 7s (- 164m 49s) (13515 65%) 0.8578\n",
      "311m 21s (- 164m 35s) (13525 65%) 0.9287\n",
      "311m 35s (- 164m 22s) (13535 65%) 0.8909\n",
      "311m 49s (- 164m 8s) (13545 65%) 0.9506\n",
      "312m 2s (- 163m 54s) (13555 65%) 0.9001\n",
      "312m 16s (- 163m 40s) (13565 65%) 0.8976\n",
      "312m 30s (- 163m 26s) (13575 65%) 0.9467\n",
      "312m 44s (- 163m 13s) (13585 65%) 0.8650\n",
      "312m 58s (- 162m 59s) (13595 65%) 0.8482\n",
      "313m 11s (- 162m 45s) (13605 65%) 0.9359\n",
      "313m 25s (- 162m 31s) (13615 65%) 0.8560\n",
      "313m 39s (- 162m 17s) (13625 65%) 0.9113\n",
      "313m 53s (- 162m 3s) (13635 65%) 0.8122\n",
      "314m 6s (- 161m 50s) (13645 65%) 0.8464\n",
      "314m 20s (- 161m 36s) (13655 66%) 0.8676\n",
      "314m 34s (- 161m 22s) (13665 66%) 0.8794\n",
      "314m 48s (- 161m 8s) (13675 66%) 0.8415\n",
      "315m 2s (- 160m 54s) (13685 66%) 0.9657\n",
      "315m 15s (- 160m 40s) (13695 66%) 0.9197\n",
      "315m 29s (- 160m 27s) (13705 66%) 0.9436\n",
      "315m 43s (- 160m 13s) (13715 66%) 0.9282\n",
      "315m 57s (- 159m 59s) (13725 66%) 0.9459\n",
      "316m 11s (- 159m 45s) (13735 66%) 0.8506\n",
      "316m 24s (- 159m 31s) (13745 66%) 0.8592\n",
      "316m 38s (- 159m 18s) (13755 66%) 0.9370\n",
      "316m 52s (- 159m 4s) (13765 66%) 0.9303\n",
      "317m 6s (- 158m 50s) (13775 66%) 0.9204\n",
      "317m 20s (- 158m 36s) (13785 66%) 0.8591\n",
      "317m 33s (- 158m 22s) (13795 66%) 0.8144\n",
      "317m 47s (- 158m 8s) (13805 66%) 0.9101\n",
      "318m 1s (- 157m 55s) (13815 66%) 0.9735\n",
      "318m 15s (- 157m 41s) (13825 66%) 0.8546\n",
      "318m 29s (- 157m 27s) (13835 66%) 0.8891\n",
      "318m 42s (- 157m 13s) (13845 66%) 0.9316\n",
      "318m 56s (- 156m 59s) (13855 67%) 0.9706\n",
      "319m 10s (- 156m 46s) (13865 67%) 0.8474\n",
      "319m 24s (- 156m 32s) (13875 67%) 0.9110\n",
      "319m 38s (- 156m 18s) (13885 67%) 0.8491\n",
      "319m 51s (- 156m 4s) (13895 67%) 0.9059\n",
      "320m 5s (- 155m 50s) (13905 67%) 0.8569\n",
      "320m 19s (- 155m 36s) (13915 67%) 0.8819\n",
      "320m 33s (- 155m 23s) (13925 67%) 0.8819\n",
      "320m 47s (- 155m 9s) (13935 67%) 0.8735\n",
      "321m 1s (- 154m 55s) (13945 67%) 0.8522\n",
      "321m 14s (- 154m 41s) (13955 67%) 0.8771\n",
      "321m 28s (- 154m 27s) (13965 67%) 0.9986\n",
      "321m 42s (- 154m 14s) (13975 67%) 0.8968\n",
      "321m 56s (- 154m 0s) (13985 67%) 0.9384\n",
      "322m 10s (- 153m 46s) (13995 67%) 0.8579\n",
      "322m 23s (- 153m 32s) (14005 67%) 0.8345\n",
      "322m 37s (- 153m 18s) (14015 67%) 0.9057\n",
      "322m 51s (- 153m 5s) (14025 67%) 0.9042\n",
      "323m 5s (- 152m 51s) (14035 67%) 0.9742\n",
      "323m 19s (- 152m 37s) (14045 67%) 0.8737\n",
      "323m 32s (- 152m 23s) (14055 67%) 0.9255\n",
      "323m 46s (- 152m 9s) (14065 68%) 0.8907\n",
      "324m 0s (- 151m 56s) (14075 68%) 0.9222\n",
      "324m 14s (- 151m 42s) (14085 68%) 0.8557\n",
      "324m 28s (- 151m 28s) (14095 68%) 0.8971\n",
      "324m 42s (- 151m 14s) (14105 68%) 0.8912\n",
      "324m 55s (- 151m 0s) (14115 68%) 0.9432\n",
      "325m 9s (- 150m 46s) (14125 68%) 0.9311\n",
      "325m 23s (- 150m 33s) (14135 68%) 0.8547\n",
      "325m 37s (- 150m 19s) (14145 68%) 0.8694\n",
      "325m 51s (- 150m 5s) (14155 68%) 0.8456\n",
      "326m 4s (- 149m 51s) (14165 68%) 0.8729\n",
      "326m 18s (- 149m 37s) (14175 68%) 0.8564\n",
      "326m 32s (- 149m 24s) (14185 68%) 0.8995\n",
      "326m 46s (- 149m 10s) (14195 68%) 0.8735\n",
      "327m 0s (- 148m 56s) (14205 68%) 0.8493\n",
      "327m 13s (- 148m 42s) (14215 68%) 0.9011\n",
      "327m 27s (- 148m 28s) (14225 68%) 0.9111\n",
      "327m 41s (- 148m 15s) (14235 68%) 0.9481\n",
      "327m 55s (- 148m 1s) (14245 68%) 0.9108\n",
      "328m 9s (- 147m 47s) (14255 68%) 0.9592\n",
      "328m 22s (- 147m 33s) (14265 68%) 0.9581\n",
      "328m 36s (- 147m 19s) (14275 69%) 0.8682\n",
      "328m 50s (- 147m 5s) (14285 69%) 0.9655\n",
      "329m 4s (- 146m 52s) (14295 69%) 0.8673\n",
      "329m 18s (- 146m 38s) (14305 69%) 0.9366\n",
      "329m 31s (- 146m 24s) (14315 69%) 0.9237\n",
      "329m 45s (- 146m 10s) (14325 69%) 0.9325\n",
      "329m 59s (- 145m 56s) (14335 69%) 0.9497\n",
      "330m 13s (- 145m 43s) (14345 69%) 0.9257\n",
      "330m 27s (- 145m 29s) (14355 69%) 0.8832\n",
      "330m 40s (- 145m 15s) (14365 69%) 0.9514\n",
      "330m 54s (- 145m 1s) (14375 69%) 0.9273\n",
      "331m 8s (- 144m 47s) (14385 69%) 0.8814\n",
      "331m 22s (- 144m 33s) (14395 69%) 0.9006\n",
      "331m 36s (- 144m 20s) (14405 69%) 0.8817\n",
      "331m 49s (- 144m 6s) (14415 69%) 0.8367\n",
      "332m 3s (- 143m 52s) (14425 69%) 0.9131\n",
      "332m 17s (- 143m 38s) (14435 69%) 0.8900\n",
      "332m 31s (- 143m 24s) (14445 69%) 0.8942\n",
      "332m 45s (- 143m 11s) (14455 69%) 0.9228\n",
      "332m 58s (- 142m 57s) (14465 69%) 0.9798\n",
      "333m 12s (- 142m 43s) (14475 70%) 0.8846\n",
      "333m 26s (- 142m 29s) (14485 70%) 0.8826\n",
      "333m 40s (- 142m 15s) (14495 70%) 0.8716\n",
      "333m 54s (- 142m 1s) (14505 70%) 0.8994\n",
      "334m 7s (- 141m 48s) (14515 70%) 0.9172\n",
      "334m 21s (- 141m 34s) (14525 70%) 0.9470\n",
      "334m 35s (- 141m 20s) (14535 70%) 0.8599\n",
      "334m 49s (- 141m 6s) (14545 70%) 0.8989\n",
      "335m 3s (- 140m 52s) (14555 70%) 0.8992\n",
      "335m 17s (- 140m 39s) (14565 70%) 0.9279\n",
      "335m 30s (- 140m 25s) (14575 70%) 0.9299\n",
      "335m 44s (- 140m 11s) (14585 70%) 0.9687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335m 58s (- 139m 57s) (14595 70%) 0.8935\n",
      "336m 12s (- 139m 43s) (14605 70%) 0.8856\n",
      "336m 26s (- 139m 30s) (14615 70%) 0.8951\n",
      "336m 39s (- 139m 16s) (14625 70%) 0.8390\n",
      "336m 53s (- 139m 2s) (14635 70%) 0.8308\n",
      "337m 7s (- 138m 48s) (14645 70%) 0.9253\n",
      "337m 21s (- 138m 34s) (14655 70%) 0.8933\n",
      "337m 35s (- 138m 20s) (14665 70%) 0.9254\n",
      "337m 48s (- 138m 7s) (14675 70%) 0.9440\n",
      "338m 2s (- 137m 53s) (14685 71%) 0.9277\n",
      "338m 16s (- 137m 39s) (14695 71%) 0.8786\n",
      "338m 30s (- 137m 25s) (14705 71%) 0.8081\n",
      "338m 44s (- 137m 11s) (14715 71%) 0.9194\n",
      "338m 58s (- 136m 58s) (14725 71%) 0.9856\n",
      "339m 11s (- 136m 44s) (14735 71%) 0.8411\n",
      "339m 25s (- 136m 30s) (14745 71%) 0.9005\n",
      "339m 39s (- 136m 16s) (14755 71%) 0.9027\n",
      "339m 53s (- 136m 2s) (14765 71%) 0.9404\n",
      "340m 7s (- 135m 49s) (14775 71%) 0.9064\n",
      "340m 20s (- 135m 35s) (14785 71%) 0.9052\n",
      "340m 34s (- 135m 21s) (14795 71%) 0.9784\n",
      "340m 48s (- 135m 7s) (14805 71%) 0.8932\n",
      "341m 2s (- 134m 53s) (14815 71%) 0.9010\n",
      "341m 16s (- 134m 39s) (14825 71%) 0.8835\n",
      "341m 29s (- 134m 26s) (14835 71%) 0.8650\n",
      "341m 43s (- 134m 12s) (14845 71%) 0.8803\n",
      "341m 57s (- 133m 58s) (14855 71%) 0.8850\n",
      "342m 11s (- 133m 44s) (14865 71%) 0.8908\n",
      "342m 25s (- 133m 30s) (14875 71%) 0.8904\n",
      "342m 38s (- 133m 17s) (14885 71%) 0.9138\n",
      "342m 52s (- 133m 3s) (14895 72%) 0.8565\n",
      "343m 6s (- 132m 49s) (14905 72%) 0.9196\n",
      "343m 20s (- 132m 35s) (14915 72%) 0.9167\n",
      "343m 34s (- 132m 21s) (14925 72%) 0.9135\n",
      "343m 48s (- 132m 8s) (14935 72%) 0.9423\n",
      "344m 1s (- 131m 54s) (14945 72%) 0.8884\n",
      "344m 15s (- 131m 40s) (14955 72%) 0.8856\n",
      "344m 29s (- 131m 26s) (14965 72%) 0.9184\n",
      "344m 43s (- 131m 12s) (14975 72%) 0.8640\n",
      "344m 57s (- 130m 58s) (14985 72%) 0.9088\n",
      "345m 10s (- 130m 45s) (14995 72%) 0.9406\n",
      "345m 24s (- 130m 31s) (15005 72%) 0.8647\n",
      "345m 38s (- 130m 17s) (15015 72%) 0.9364\n",
      "345m 52s (- 130m 3s) (15025 72%) 0.9460\n",
      "346m 6s (- 129m 49s) (15035 72%) 0.9833\n",
      "346m 19s (- 129m 36s) (15045 72%) 0.8924\n",
      "346m 33s (- 129m 22s) (15055 72%) 0.8770\n",
      "346m 47s (- 129m 8s) (15065 72%) 0.8829\n",
      "347m 1s (- 128m 54s) (15075 72%) 0.8763\n",
      "347m 15s (- 128m 40s) (15085 72%) 0.8957\n",
      "347m 28s (- 128m 27s) (15095 73%) 0.8466\n",
      "347m 42s (- 128m 13s) (15105 73%) 0.8940\n",
      "347m 56s (- 127m 59s) (15115 73%) 0.8432\n",
      "348m 10s (- 127m 45s) (15125 73%) 0.8782\n",
      "348m 24s (- 127m 31s) (15135 73%) 0.9074\n",
      "348m 38s (- 127m 17s) (15145 73%) 0.8743\n",
      "348m 51s (- 127m 4s) (15155 73%) 0.8875\n",
      "349m 5s (- 126m 50s) (15165 73%) 0.9502\n",
      "349m 19s (- 126m 36s) (15175 73%) 0.7973\n",
      "349m 33s (- 126m 22s) (15185 73%) 0.9517\n",
      "349m 47s (- 126m 8s) (15195 73%) 0.8875\n",
      "350m 0s (- 125m 55s) (15205 73%) 0.9060\n",
      "350m 14s (- 125m 41s) (15215 73%) 0.8766\n",
      "350m 28s (- 125m 27s) (15225 73%) 0.9361\n",
      "350m 42s (- 125m 13s) (15235 73%) 0.9038\n",
      "350m 56s (- 124m 59s) (15245 73%) 0.9714\n",
      "351m 10s (- 124m 46s) (15255 73%) 0.9452\n",
      "351m 23s (- 124m 32s) (15265 73%) 0.9459\n",
      "351m 37s (- 124m 18s) (15275 73%) 0.9219\n",
      "351m 51s (- 124m 4s) (15285 73%) 0.8780\n",
      "352m 5s (- 123m 50s) (15295 73%) 0.9151\n",
      "352m 19s (- 123m 36s) (15305 74%) 0.8798\n",
      "352m 32s (- 123m 23s) (15315 74%) 0.8877\n",
      "352m 46s (- 123m 9s) (15325 74%) 0.8441\n",
      "353m 0s (- 122m 55s) (15335 74%) 0.9246\n",
      "353m 14s (- 122m 41s) (15345 74%) 0.9308\n",
      "353m 28s (- 122m 27s) (15355 74%) 0.9544\n",
      "353m 41s (- 122m 14s) (15365 74%) 0.8427\n",
      "353m 55s (- 122m 0s) (15375 74%) 0.9508\n",
      "354m 9s (- 121m 46s) (15385 74%) 0.8994\n",
      "354m 23s (- 121m 32s) (15395 74%) 0.8474\n",
      "354m 37s (- 121m 18s) (15405 74%) 0.9696\n",
      "354m 50s (- 121m 5s) (15415 74%) 0.9088\n",
      "355m 4s (- 120m 51s) (15425 74%) 0.9234\n",
      "355m 18s (- 120m 37s) (15435 74%) 0.8890\n",
      "355m 32s (- 120m 23s) (15445 74%) 0.9201\n",
      "355m 46s (- 120m 9s) (15455 74%) 0.8745\n",
      "356m 0s (- 119m 55s) (15465 74%) 0.8709\n",
      "356m 13s (- 119m 42s) (15475 74%) 0.8859\n",
      "356m 27s (- 119m 28s) (15485 74%) 0.8603\n",
      "356m 41s (- 119m 14s) (15495 74%) 0.8403\n",
      "356m 55s (- 119m 0s) (15505 74%) 0.9281\n",
      "357m 9s (- 118m 46s) (15515 75%) 0.9373\n",
      "357m 22s (- 118m 33s) (15525 75%) 0.8754\n",
      "357m 36s (- 118m 19s) (15535 75%) 0.9278\n",
      "357m 50s (- 118m 5s) (15545 75%) 0.9235\n",
      "358m 4s (- 117m 51s) (15555 75%) 0.8603\n",
      "358m 18s (- 117m 37s) (15565 75%) 0.8295\n",
      "358m 31s (- 117m 24s) (15575 75%) 0.8794\n",
      "358m 45s (- 117m 10s) (15585 75%) 0.9442\n",
      "358m 59s (- 116m 56s) (15595 75%) 0.8942\n",
      "359m 13s (- 116m 42s) (15605 75%) 0.8717\n",
      "359m 27s (- 116m 28s) (15615 75%) 0.8692\n",
      "359m 41s (- 116m 14s) (15625 75%) 0.9152\n",
      "359m 54s (- 116m 1s) (15635 75%) 0.8635\n",
      "360m 8s (- 115m 47s) (15645 75%) 0.9249\n",
      "360m 22s (- 115m 33s) (15655 75%) 0.8964\n",
      "360m 36s (- 115m 19s) (15665 75%) 0.9604\n",
      "360m 50s (- 115m 5s) (15675 75%) 0.8333\n",
      "361m 3s (- 114m 52s) (15685 75%) 0.8949\n",
      "361m 17s (- 114m 38s) (15695 75%) 0.8604\n",
      "361m 31s (- 114m 24s) (15705 75%) 0.8733\n",
      "361m 45s (- 114m 10s) (15715 76%) 0.9094\n",
      "361m 59s (- 113m 56s) (15725 76%) 0.8744\n",
      "362m 13s (- 113m 43s) (15735 76%) 0.9318\n",
      "362m 26s (- 113m 29s) (15745 76%) 0.9468\n",
      "362m 40s (- 113m 15s) (15755 76%) 0.9192\n",
      "362m 54s (- 113m 1s) (15765 76%) 0.9056\n",
      "363m 8s (- 112m 47s) (15775 76%) 0.9224\n",
      "363m 22s (- 112m 34s) (15785 76%) 0.8992\n",
      "363m 35s (- 112m 20s) (15795 76%) 0.9156\n",
      "363m 49s (- 112m 6s) (15805 76%) 0.8794\n",
      "364m 3s (- 111m 52s) (15815 76%) 0.8674\n",
      "364m 17s (- 111m 38s) (15825 76%) 0.9424\n",
      "364m 31s (- 111m 24s) (15835 76%) 0.9300\n",
      "364m 44s (- 111m 11s) (15845 76%) 0.9095\n",
      "364m 58s (- 110m 57s) (15855 76%) 0.8835\n",
      "365m 12s (- 110m 43s) (15865 76%) 0.8092\n",
      "365m 26s (- 110m 29s) (15875 76%) 0.8827\n",
      "365m 40s (- 110m 15s) (15885 76%) 0.8886\n",
      "365m 53s (- 110m 2s) (15895 76%) 0.9375\n",
      "366m 7s (- 109m 48s) (15905 76%) 0.9223\n",
      "366m 21s (- 109m 34s) (15915 76%) 0.9559\n",
      "366m 35s (- 109m 20s) (15925 77%) 0.9141\n",
      "366m 49s (- 109m 6s) (15935 77%) 0.8626\n",
      "367m 2s (- 108m 52s) (15945 77%) 0.8618\n",
      "367m 16s (- 108m 39s) (15955 77%) 0.9134\n",
      "367m 30s (- 108m 25s) (15965 77%) 0.9204\n",
      "367m 44s (- 108m 11s) (15975 77%) 0.9110\n",
      "367m 58s (- 107m 57s) (15985 77%) 0.9362\n",
      "368m 11s (- 107m 43s) (15995 77%) 0.9589\n",
      "368m 25s (- 107m 30s) (16005 77%) 0.9264\n",
      "368m 39s (- 107m 16s) (16015 77%) 0.9247\n",
      "368m 53s (- 107m 2s) (16025 77%) 0.9353\n",
      "369m 7s (- 106m 48s) (16035 77%) 0.8968\n",
      "369m 20s (- 106m 34s) (16045 77%) 0.8514\n",
      "369m 34s (- 106m 21s) (16055 77%) 0.8898\n",
      "369m 48s (- 106m 7s) (16065 77%) 0.9105\n",
      "370m 2s (- 105m 53s) (16075 77%) 0.8313\n",
      "370m 16s (- 105m 39s) (16085 77%) 0.9008\n",
      "370m 29s (- 105m 25s) (16095 77%) 0.9329\n",
      "370m 43s (- 105m 11s) (16105 77%) 0.8782\n",
      "370m 57s (- 104m 58s) (16115 77%) 0.9130\n",
      "371m 11s (- 104m 44s) (16125 77%) 0.8686\n",
      "371m 25s (- 104m 30s) (16135 78%) 0.8984\n",
      "371m 38s (- 104m 16s) (16145 78%) 0.9185\n",
      "371m 52s (- 104m 2s) (16155 78%) 0.8478\n",
      "372m 6s (- 103m 49s) (16165 78%) 0.8672\n",
      "372m 20s (- 103m 35s) (16175 78%) 0.9045\n",
      "372m 34s (- 103m 21s) (16185 78%) 0.9028\n",
      "372m 47s (- 103m 7s) (16195 78%) 0.8709\n",
      "373m 1s (- 102m 53s) (16205 78%) 0.9952\n",
      "373m 15s (- 102m 39s) (16215 78%) 0.9039\n",
      "373m 29s (- 102m 26s) (16225 78%) 0.8647\n",
      "373m 43s (- 102m 12s) (16235 78%) 0.9567\n",
      "373m 56s (- 101m 58s) (16245 78%) 0.8698\n",
      "374m 10s (- 101m 44s) (16255 78%) 0.8986\n",
      "374m 24s (- 101m 30s) (16265 78%) 0.9103\n",
      "374m 38s (- 101m 17s) (16275 78%) 0.8633\n",
      "374m 52s (- 101m 3s) (16285 78%) 0.8564\n",
      "375m 5s (- 100m 49s) (16295 78%) 0.8839\n",
      "375m 19s (- 100m 35s) (16305 78%) 0.8745\n",
      "375m 33s (- 100m 21s) (16315 78%) 0.9025\n",
      "375m 47s (- 100m 7s) (16325 78%) 0.9093\n",
      "376m 1s (- 99m 54s) (16335 79%) 0.9026\n",
      "376m 14s (- 99m 40s) (16345 79%) 0.8553\n",
      "376m 28s (- 99m 26s) (16355 79%) 0.8987\n",
      "376m 42s (- 99m 12s) (16365 79%) 0.9183\n",
      "376m 56s (- 98m 58s) (16375 79%) 0.9061\n",
      "377m 10s (- 98m 45s) (16385 79%) 0.8309\n",
      "377m 23s (- 98m 31s) (16395 79%) 0.8110\n",
      "377m 37s (- 98m 17s) (16405 79%) 0.8795\n",
      "377m 51s (- 98m 3s) (16415 79%) 0.9148\n",
      "378m 5s (- 97m 49s) (16425 79%) 0.8823\n",
      "378m 19s (- 97m 36s) (16435 79%) 0.8848\n",
      "378m 32s (- 97m 22s) (16445 79%) 0.8915\n",
      "378m 46s (- 97m 8s) (16455 79%) 0.9203\n",
      "379m 0s (- 96m 54s) (16465 79%) 0.9020\n",
      "379m 14s (- 96m 40s) (16475 79%) 0.8705\n",
      "379m 28s (- 96m 27s) (16485 79%) 0.9155\n",
      "379m 42s (- 96m 13s) (16495 79%) 0.9130\n",
      "379m 55s (- 95m 59s) (16505 79%) 0.9076\n",
      "380m 9s (- 95m 45s) (16515 79%) 0.8412\n",
      "380m 23s (- 95m 31s) (16525 79%) 0.8631\n",
      "380m 37s (- 95m 17s) (16535 79%) 0.8495\n",
      "380m 59s (- 94m 57s) (16550 80%) 1.2523\n",
      "381m 12s (- 94m 43s) (16560 80%) 0.7840\n",
      "381m 26s (- 94m 29s) (16570 80%) 0.8373\n",
      "381m 40s (- 94m 16s) (16580 80%) 0.8458\n",
      "381m 54s (- 94m 2s) (16590 80%) 0.8753\n",
      "382m 8s (- 93m 48s) (16600 80%) 0.8309\n",
      "382m 22s (- 93m 34s) (16610 80%) 0.9055\n",
      "382m 35s (- 93m 20s) (16620 80%) 0.7902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382m 49s (- 93m 7s) (16630 80%) 0.8331\n",
      "383m 3s (- 92m 53s) (16640 80%) 0.8512\n",
      "383m 17s (- 92m 39s) (16650 80%) 0.8687\n",
      "383m 31s (- 92m 25s) (16660 80%) 0.9374\n",
      "383m 45s (- 92m 11s) (16670 80%) 0.8955\n",
      "383m 58s (- 91m 57s) (16680 80%) 0.8356\n",
      "384m 12s (- 91m 44s) (16690 80%) 0.8799\n",
      "384m 26s (- 91m 30s) (16700 80%) 0.8608\n",
      "384m 40s (- 91m 16s) (16710 80%) 0.8143\n",
      "384m 54s (- 91m 2s) (16720 80%) 0.8454\n",
      "385m 7s (- 90m 48s) (16730 80%) 0.8509\n",
      "385m 21s (- 90m 35s) (16740 80%) 0.9055\n",
      "385m 35s (- 90m 21s) (16750 81%) 0.7746\n",
      "385m 49s (- 90m 7s) (16760 81%) 0.8126\n",
      "386m 3s (- 89m 53s) (16770 81%) 0.8360\n",
      "386m 17s (- 89m 39s) (16780 81%) 0.8284\n",
      "386m 30s (- 89m 26s) (16790 81%) 0.8641\n",
      "386m 44s (- 89m 12s) (16800 81%) 0.8673\n",
      "386m 58s (- 88m 58s) (16810 81%) 0.8261\n",
      "387m 12s (- 88m 44s) (16820 81%) 0.8178\n",
      "387m 26s (- 88m 30s) (16830 81%) 0.8310\n",
      "387m 39s (- 88m 17s) (16840 81%) 0.8542\n",
      "387m 53s (- 88m 3s) (16850 81%) 0.8710\n",
      "388m 7s (- 87m 49s) (16860 81%) 0.8585\n",
      "388m 21s (- 87m 35s) (16870 81%) 0.8050\n",
      "388m 35s (- 87m 21s) (16880 81%) 0.8172\n",
      "388m 49s (- 87m 7s) (16890 81%) 0.7952\n",
      "389m 2s (- 86m 54s) (16900 81%) 0.8414\n",
      "389m 16s (- 86m 40s) (16910 81%) 0.8414\n",
      "389m 30s (- 86m 26s) (16920 81%) 0.8315\n",
      "389m 44s (- 86m 12s) (16930 81%) 0.8106\n",
      "389m 58s (- 85m 58s) (16940 81%) 0.8515\n",
      "390m 11s (- 85m 45s) (16950 81%) 0.8605\n",
      "390m 25s (- 85m 31s) (16960 82%) 0.8102\n",
      "390m 39s (- 85m 17s) (16970 82%) 0.8147\n",
      "390m 53s (- 85m 3s) (16980 82%) 0.8253\n",
      "391m 7s (- 84m 49s) (16990 82%) 0.8047\n",
      "391m 21s (- 84m 36s) (17000 82%) 0.8221\n",
      "391m 34s (- 84m 22s) (17010 82%) 0.8971\n",
      "391m 48s (- 84m 8s) (17020 82%) 0.8440\n",
      "392m 2s (- 83m 54s) (17030 82%) 0.8569\n",
      "392m 16s (- 83m 40s) (17040 82%) 0.8384\n",
      "392m 30s (- 83m 27s) (17050 82%) 0.8059\n",
      "392m 44s (- 83m 13s) (17060 82%) 0.8502\n",
      "392m 57s (- 82m 59s) (17070 82%) 0.8607\n",
      "393m 11s (- 82m 45s) (17080 82%) 0.9410\n",
      "393m 25s (- 82m 31s) (17090 82%) 0.8298\n",
      "393m 39s (- 82m 17s) (17100 82%) 0.8751\n",
      "393m 53s (- 82m 4s) (17110 82%) 0.8919\n",
      "394m 7s (- 81m 50s) (17120 82%) 0.8016\n",
      "394m 20s (- 81m 36s) (17130 82%) 0.7977\n",
      "394m 34s (- 81m 22s) (17140 82%) 0.9130\n",
      "394m 48s (- 81m 8s) (17150 82%) 0.8435\n",
      "395m 2s (- 80m 55s) (17160 82%) 0.8938\n",
      "395m 16s (- 80m 41s) (17170 83%) 0.8428\n",
      "395m 30s (- 80m 27s) (17180 83%) 0.8613\n",
      "395m 43s (- 80m 13s) (17190 83%) 0.8707\n",
      "395m 57s (- 79m 59s) (17200 83%) 0.8501\n",
      "396m 11s (- 79m 46s) (17210 83%) 0.8496\n",
      "396m 25s (- 79m 32s) (17220 83%) 0.9024\n",
      "396m 39s (- 79m 18s) (17230 83%) 0.8672\n",
      "396m 53s (- 79m 4s) (17240 83%) 0.8795\n",
      "397m 6s (- 78m 50s) (17250 83%) 0.8710\n",
      "397m 20s (- 78m 37s) (17260 83%) 0.8166\n",
      "397m 34s (- 78m 23s) (17270 83%) 0.8546\n",
      "397m 48s (- 78m 9s) (17280 83%) 0.8567\n",
      "398m 2s (- 77m 55s) (17290 83%) 0.8733\n",
      "398m 15s (- 77m 41s) (17300 83%) 0.8130\n",
      "398m 29s (- 77m 27s) (17310 83%) 0.8268\n",
      "398m 43s (- 77m 14s) (17320 83%) 0.8171\n",
      "398m 57s (- 77m 0s) (17330 83%) 0.8450\n",
      "399m 11s (- 76m 46s) (17340 83%) 0.8690\n",
      "399m 24s (- 76m 32s) (17350 83%) 0.8302\n",
      "399m 38s (- 76m 18s) (17360 83%) 0.8369\n",
      "399m 52s (- 76m 5s) (17370 84%) 0.8917\n",
      "400m 6s (- 75m 51s) (17380 84%) 0.8007\n",
      "400m 19s (- 75m 37s) (17390 84%) 0.8748\n",
      "400m 33s (- 75m 23s) (17400 84%) 0.9092\n",
      "400m 47s (- 75m 9s) (17410 84%) 0.8849\n",
      "401m 1s (- 74m 55s) (17420 84%) 0.8259\n",
      "401m 15s (- 74m 42s) (17430 84%) 0.8421\n",
      "401m 29s (- 74m 28s) (17440 84%) 0.8404\n",
      "401m 42s (- 74m 14s) (17450 84%) 0.8040\n",
      "401m 56s (- 74m 0s) (17460 84%) 0.9032\n",
      "402m 10s (- 73m 46s) (17470 84%) 0.8371\n",
      "402m 24s (- 73m 33s) (17480 84%) 0.8316\n",
      "402m 38s (- 73m 19s) (17490 84%) 0.8635\n",
      "402m 51s (- 73m 5s) (17500 84%) 0.8175\n",
      "403m 5s (- 72m 51s) (17510 84%) 0.8488\n",
      "403m 19s (- 72m 37s) (17520 84%) 0.8495\n",
      "403m 33s (- 72m 24s) (17530 84%) 0.7960\n",
      "403m 47s (- 72m 10s) (17540 84%) 0.8699\n",
      "404m 0s (- 71m 56s) (17550 84%) 0.8373\n",
      "404m 14s (- 71m 42s) (17560 84%) 0.8484\n",
      "404m 28s (- 71m 28s) (17570 84%) 0.8344\n",
      "404m 42s (- 71m 14s) (17580 85%) 0.7970\n",
      "404m 56s (- 71m 1s) (17590 85%) 0.8241\n",
      "405m 9s (- 70m 47s) (17600 85%) 0.9089\n",
      "405m 23s (- 70m 33s) (17610 85%) 0.8388\n",
      "405m 37s (- 70m 19s) (17620 85%) 0.8409\n",
      "405m 51s (- 70m 5s) (17630 85%) 0.8927\n",
      "406m 5s (- 69m 52s) (17640 85%) 0.8856\n",
      "406m 18s (- 69m 38s) (17650 85%) 0.7995\n",
      "406m 32s (- 69m 24s) (17660 85%) 0.8369\n",
      "406m 46s (- 69m 10s) (17670 85%) 0.8827\n",
      "407m 0s (- 68m 56s) (17680 85%) 0.9013\n",
      "407m 14s (- 68m 43s) (17690 85%) 0.8671\n",
      "407m 27s (- 68m 29s) (17700 85%) 0.8050\n",
      "407m 41s (- 68m 15s) (17710 85%) 0.7862\n",
      "407m 55s (- 68m 1s) (17720 85%) 0.8580\n",
      "408m 9s (- 67m 47s) (17730 85%) 0.8304\n",
      "408m 23s (- 67m 33s) (17740 85%) 0.8010\n",
      "408m 37s (- 67m 20s) (17750 85%) 0.8810\n",
      "408m 50s (- 67m 6s) (17760 85%) 0.8386\n",
      "409m 4s (- 66m 52s) (17770 85%) 0.8406\n",
      "409m 18s (- 66m 38s) (17780 85%) 0.8071\n",
      "409m 32s (- 66m 24s) (17790 86%) 0.8380\n",
      "409m 46s (- 66m 11s) (17800 86%) 0.8091\n",
      "409m 59s (- 65m 57s) (17810 86%) 0.7932\n",
      "410m 13s (- 65m 43s) (17820 86%) 0.8875\n",
      "410m 27s (- 65m 29s) (17830 86%) 0.8112\n",
      "410m 41s (- 65m 15s) (17840 86%) 0.8390\n",
      "410m 55s (- 65m 2s) (17850 86%) 0.8747\n",
      "411m 9s (- 64m 48s) (17860 86%) 0.8412\n",
      "411m 22s (- 64m 34s) (17870 86%) 0.8649\n",
      "411m 36s (- 64m 20s) (17880 86%) 0.7959\n",
      "411m 50s (- 64m 6s) (17890 86%) 0.8837\n",
      "412m 4s (- 63m 52s) (17900 86%) 0.8400\n",
      "412m 18s (- 63m 39s) (17910 86%) 0.8189\n",
      "412m 31s (- 63m 25s) (17920 86%) 0.8822\n",
      "412m 45s (- 63m 11s) (17930 86%) 0.8490\n",
      "412m 59s (- 62m 57s) (17940 86%) 0.8190\n",
      "413m 13s (- 62m 43s) (17950 86%) 0.8197\n",
      "413m 27s (- 62m 30s) (17960 86%) 0.8106\n",
      "413m 40s (- 62m 16s) (17970 86%) 0.8143\n",
      "413m 54s (- 62m 2s) (17980 86%) 0.8424\n",
      "414m 8s (- 61m 48s) (17990 87%) 0.8439\n",
      "414m 22s (- 61m 34s) (18000 87%) 0.8815\n",
      "414m 36s (- 61m 21s) (18010 87%) 0.8256\n",
      "414m 50s (- 61m 7s) (18020 87%) 0.8044\n",
      "415m 3s (- 60m 53s) (18030 87%) 0.8765\n",
      "415m 17s (- 60m 39s) (18040 87%) 0.8391\n",
      "415m 31s (- 60m 25s) (18050 87%) 0.8447\n",
      "415m 45s (- 60m 11s) (18060 87%) 0.8506\n",
      "415m 59s (- 59m 58s) (18070 87%) 0.8471\n",
      "416m 12s (- 59m 44s) (18080 87%) 0.8473\n",
      "416m 26s (- 59m 30s) (18090 87%) 0.8470\n",
      "416m 40s (- 59m 16s) (18100 87%) 0.8394\n",
      "416m 54s (- 59m 2s) (18110 87%) 0.8234\n",
      "417m 8s (- 58m 49s) (18120 87%) 0.8110\n",
      "417m 21s (- 58m 35s) (18130 87%) 0.8624\n",
      "417m 35s (- 58m 21s) (18140 87%) 0.8952\n",
      "417m 49s (- 58m 7s) (18150 87%) 0.8613\n",
      "418m 3s (- 57m 53s) (18160 87%) 0.8024\n",
      "418m 17s (- 57m 40s) (18170 87%) 0.8022\n",
      "418m 31s (- 57m 26s) (18180 87%) 0.8568\n",
      "418m 44s (- 57m 12s) (18190 87%) 0.8198\n",
      "418m 58s (- 56m 58s) (18200 88%) 0.8273\n",
      "419m 12s (- 56m 44s) (18210 88%) 0.8706\n",
      "419m 26s (- 56m 30s) (18220 88%) 0.8290\n",
      "419m 40s (- 56m 17s) (18230 88%) 0.9157\n",
      "419m 53s (- 56m 3s) (18240 88%) 0.7881\n",
      "420m 7s (- 55m 49s) (18250 88%) 0.8609\n",
      "420m 21s (- 55m 35s) (18260 88%) 0.7565\n",
      "420m 35s (- 55m 21s) (18270 88%) 0.8330\n",
      "420m 49s (- 55m 8s) (18280 88%) 0.8930\n",
      "421m 3s (- 54m 54s) (18290 88%) 0.8998\n",
      "421m 16s (- 54m 40s) (18300 88%) 0.8382\n",
      "421m 30s (- 54m 26s) (18310 88%) 0.8325\n",
      "421m 44s (- 54m 12s) (18320 88%) 0.8402\n",
      "421m 58s (- 53m 59s) (18330 88%) 0.8290\n",
      "422m 12s (- 53m 45s) (18340 88%) 0.9297\n",
      "422m 25s (- 53m 31s) (18350 88%) 0.8817\n",
      "422m 39s (- 53m 17s) (18360 88%) 0.8793\n",
      "422m 53s (- 53m 3s) (18370 88%) 0.8312\n",
      "423m 7s (- 52m 49s) (18380 88%) 0.7838\n",
      "423m 21s (- 52m 36s) (18390 88%) 0.9158\n",
      "423m 34s (- 52m 22s) (18400 88%) 0.8632\n",
      "423m 48s (- 52m 8s) (18410 89%) 0.8613\n",
      "424m 2s (- 51m 54s) (18420 89%) 0.7888\n",
      "424m 16s (- 51m 40s) (18430 89%) 0.8723\n",
      "424m 30s (- 51m 27s) (18440 89%) 0.8479\n",
      "424m 44s (- 51m 13s) (18450 89%) 0.8519\n",
      "424m 57s (- 50m 59s) (18460 89%) 0.8263\n",
      "425m 11s (- 50m 45s) (18470 89%) 0.9283\n",
      "425m 25s (- 50m 31s) (18480 89%) 0.7911\n",
      "425m 39s (- 50m 18s) (18490 89%) 0.8005\n",
      "425m 53s (- 50m 4s) (18500 89%) 0.8483\n",
      "426m 7s (- 49m 50s) (18510 89%) 0.8029\n",
      "426m 20s (- 49m 36s) (18520 89%) 0.8166\n",
      "426m 34s (- 49m 22s) (18530 89%) 0.8028\n",
      "426m 48s (- 49m 8s) (18540 89%) 0.8865\n",
      "427m 2s (- 48m 55s) (18550 89%) 0.8663\n",
      "427m 16s (- 48m 41s) (18560 89%) 0.8791\n",
      "427m 30s (- 48m 27s) (18570 89%) 0.8572\n",
      "427m 43s (- 48m 13s) (18580 89%) 0.8209\n",
      "427m 57s (- 47m 59s) (18590 89%) 0.8463\n",
      "428m 11s (- 47m 46s) (18600 89%) 0.8606\n",
      "428m 25s (- 47m 32s) (18610 90%) 0.8042\n",
      "428m 39s (- 47m 18s) (18620 90%) 0.8917\n",
      "428m 52s (- 47m 4s) (18630 90%) 0.8210\n",
      "429m 6s (- 46m 50s) (18640 90%) 0.8382\n",
      "429m 20s (- 46m 37s) (18650 90%) 0.8267\n",
      "429m 34s (- 46m 23s) (18660 90%) 0.8411\n",
      "429m 48s (- 46m 9s) (18670 90%) 0.8034\n",
      "430m 1s (- 45m 55s) (18680 90%) 0.8754\n",
      "430m 15s (- 45m 41s) (18690 90%) 0.9176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430m 29s (- 45m 27s) (18700 90%) 0.8846\n",
      "430m 43s (- 45m 14s) (18710 90%) 0.8546\n",
      "430m 57s (- 45m 0s) (18720 90%) 0.7725\n",
      "431m 10s (- 44m 46s) (18730 90%) 0.8590\n",
      "431m 24s (- 44m 32s) (18740 90%) 0.8575\n",
      "431m 38s (- 44m 18s) (18750 90%) 0.8372\n",
      "431m 52s (- 44m 5s) (18760 90%) 0.8604\n",
      "432m 6s (- 43m 51s) (18770 90%) 0.8161\n",
      "432m 19s (- 43m 37s) (18780 90%) 0.8643\n",
      "432m 33s (- 43m 23s) (18790 90%) 0.8648\n",
      "432m 47s (- 43m 9s) (18800 90%) 0.8832\n",
      "433m 1s (- 42m 56s) (18810 90%) 0.7920\n",
      "433m 14s (- 42m 42s) (18820 91%) 0.8601\n",
      "433m 28s (- 42m 28s) (18830 91%) 0.8942\n",
      "433m 42s (- 42m 14s) (18840 91%) 0.8116\n",
      "433m 56s (- 42m 0s) (18850 91%) 0.8708\n",
      "434m 10s (- 41m 46s) (18860 91%) 0.8645\n",
      "434m 23s (- 41m 33s) (18870 91%) 0.8549\n",
      "434m 37s (- 41m 19s) (18880 91%) 0.8046\n",
      "434m 51s (- 41m 5s) (18890 91%) 0.8731\n",
      "435m 5s (- 40m 51s) (18900 91%) 0.8166\n",
      "435m 19s (- 40m 37s) (18910 91%) 0.8899\n",
      "435m 32s (- 40m 24s) (18920 91%) 0.8110\n",
      "435m 46s (- 40m 10s) (18930 91%) 0.8787\n",
      "436m 0s (- 39m 56s) (18940 91%) 0.8432\n",
      "436m 14s (- 39m 42s) (18950 91%) 0.8462\n",
      "436m 28s (- 39m 28s) (18960 91%) 0.8130\n",
      "436m 42s (- 39m 15s) (18970 91%) 0.8926\n",
      "436m 55s (- 39m 1s) (18980 91%) 0.8587\n",
      "437m 9s (- 38m 47s) (18990 91%) 0.8322\n",
      "437m 23s (- 38m 33s) (19000 91%) 0.8522\n",
      "437m 37s (- 38m 19s) (19010 91%) 0.8535\n",
      "437m 51s (- 38m 5s) (19020 91%) 0.8638\n",
      "438m 4s (- 37m 52s) (19030 92%) 0.7978\n",
      "438m 18s (- 37m 38s) (19040 92%) 0.8245\n",
      "438m 32s (- 37m 24s) (19050 92%) 0.8273\n",
      "438m 46s (- 37m 10s) (19060 92%) 0.8527\n",
      "439m 0s (- 36m 56s) (19070 92%) 0.8220\n",
      "439m 13s (- 36m 43s) (19080 92%) 0.8791\n",
      "439m 27s (- 36m 29s) (19090 92%) 0.8522\n",
      "439m 41s (- 36m 15s) (19100 92%) 0.8923\n",
      "439m 55s (- 36m 1s) (19110 92%) 0.8640\n",
      "440m 9s (- 35m 47s) (19120 92%) 0.8818\n",
      "440m 22s (- 35m 33s) (19130 92%) 0.7777\n",
      "440m 36s (- 35m 20s) (19140 92%) 0.8412\n",
      "440m 50s (- 35m 6s) (19150 92%) 0.8615\n",
      "441m 4s (- 34m 52s) (19160 92%) 0.8652\n",
      "441m 17s (- 34m 38s) (19170 92%) 0.8429\n",
      "441m 31s (- 34m 24s) (19180 92%) 0.7752\n",
      "441m 45s (- 34m 11s) (19190 92%) 0.7838\n",
      "441m 59s (- 33m 57s) (19200 92%) 0.8106\n",
      "442m 13s (- 33m 43s) (19210 92%) 0.8410\n",
      "442m 26s (- 33m 29s) (19220 92%) 0.8877\n",
      "442m 40s (- 33m 15s) (19230 93%) 0.7903\n",
      "442m 54s (- 33m 2s) (19240 93%) 0.8168\n",
      "443m 8s (- 32m 48s) (19250 93%) 0.8273\n",
      "443m 22s (- 32m 34s) (19260 93%) 0.8171\n",
      "443m 35s (- 32m 20s) (19270 93%) 0.8519\n",
      "443m 49s (- 32m 6s) (19280 93%) 0.8291\n",
      "444m 3s (- 31m 52s) (19290 93%) 0.8505\n",
      "444m 17s (- 31m 39s) (19300 93%) 0.7792\n",
      "444m 31s (- 31m 25s) (19310 93%) 0.8870\n",
      "444m 44s (- 31m 11s) (19320 93%) 0.8334\n",
      "444m 58s (- 30m 57s) (19330 93%) 0.8418\n",
      "445m 12s (- 30m 43s) (19340 93%) 0.8707\n",
      "445m 26s (- 30m 30s) (19350 93%) 0.8798\n",
      "445m 40s (- 30m 16s) (19360 93%) 0.8705\n",
      "445m 53s (- 30m 2s) (19370 93%) 0.8326\n",
      "446m 7s (- 29m 48s) (19380 93%) 0.8294\n",
      "446m 21s (- 29m 34s) (19390 93%) 0.8214\n",
      "446m 35s (- 29m 21s) (19400 93%) 0.8033\n",
      "446m 49s (- 29m 7s) (19410 93%) 0.7960\n",
      "447m 2s (- 28m 53s) (19420 93%) 0.8552\n",
      "447m 16s (- 28m 39s) (19430 93%) 0.8352\n",
      "447m 30s (- 28m 25s) (19440 94%) 0.8877\n",
      "447m 44s (- 28m 11s) (19450 94%) 0.8326\n",
      "447m 58s (- 27m 58s) (19460 94%) 0.8624\n",
      "448m 12s (- 27m 44s) (19470 94%) 0.7561\n",
      "448m 25s (- 27m 30s) (19480 94%) 0.8035\n",
      "448m 39s (- 27m 16s) (19490 94%) 0.7938\n",
      "448m 53s (- 27m 2s) (19500 94%) 0.7956\n",
      "449m 7s (- 26m 49s) (19510 94%) 0.8388\n",
      "449m 21s (- 26m 35s) (19520 94%) 0.8686\n",
      "449m 34s (- 26m 21s) (19530 94%) 0.9007\n",
      "449m 48s (- 26m 7s) (19540 94%) 0.8681\n",
      "450m 2s (- 25m 53s) (19550 94%) 0.9010\n",
      "450m 16s (- 25m 40s) (19560 94%) 0.8328\n",
      "450m 30s (- 25m 26s) (19570 94%) 0.9024\n",
      "450m 43s (- 25m 12s) (19580 94%) 0.7921\n",
      "450m 57s (- 24m 58s) (19590 94%) 0.8083\n",
      "451m 11s (- 24m 44s) (19600 94%) 0.9225\n",
      "451m 25s (- 24m 30s) (19610 94%) 0.8869\n",
      "451m 38s (- 24m 17s) (19620 94%) 0.8686\n",
      "451m 52s (- 24m 3s) (19630 94%) 0.8563\n",
      "452m 6s (- 23m 49s) (19640 94%) 0.8290\n",
      "452m 20s (- 23m 35s) (19650 95%) 0.8737\n",
      "452m 34s (- 23m 21s) (19660 95%) 0.8714\n",
      "452m 47s (- 23m 8s) (19670 95%) 0.8333\n",
      "453m 1s (- 22m 54s) (19680 95%) 0.8413\n",
      "453m 15s (- 22m 40s) (19690 95%) 0.8977\n",
      "453m 29s (- 22m 26s) (19700 95%) 0.8434\n",
      "453m 43s (- 22m 12s) (19710 95%) 0.8477\n",
      "453m 56s (- 21m 59s) (19720 95%) 0.7799\n",
      "454m 10s (- 21m 45s) (19730 95%) 0.9121\n",
      "454m 24s (- 21m 31s) (19740 95%) 0.8414\n",
      "454m 38s (- 21m 17s) (19750 95%) 0.8621\n",
      "454m 52s (- 21m 3s) (19760 95%) 0.8563\n",
      "455m 5s (- 20m 49s) (19770 95%) 0.7957\n",
      "455m 19s (- 20m 36s) (19780 95%) 0.8546\n",
      "455m 33s (- 20m 22s) (19790 95%) 0.8170\n",
      "455m 47s (- 20m 8s) (19800 95%) 0.7847\n",
      "456m 1s (- 19m 54s) (19810 95%) 0.8129\n",
      "456m 14s (- 19m 40s) (19820 95%) 0.8477\n",
      "456m 28s (- 19m 27s) (19830 95%) 0.8758\n",
      "456m 42s (- 19m 13s) (19840 95%) 0.8120\n",
      "456m 56s (- 18m 59s) (19850 96%) 0.9011\n",
      "457m 10s (- 18m 45s) (19860 96%) 0.8576\n",
      "457m 23s (- 18m 31s) (19870 96%) 0.8667\n",
      "457m 37s (- 18m 18s) (19880 96%) 0.8087\n",
      "457m 51s (- 18m 4s) (19890 96%) 0.7719\n",
      "458m 5s (- 17m 50s) (19900 96%) 0.9296\n",
      "458m 19s (- 17m 36s) (19910 96%) 0.8184\n",
      "458m 32s (- 17m 22s) (19920 96%) 0.8280\n",
      "458m 46s (- 17m 8s) (19930 96%) 0.9120\n",
      "459m 0s (- 16m 55s) (19940 96%) 0.8940\n",
      "459m 14s (- 16m 41s) (19950 96%) 0.8563\n",
      "459m 28s (- 16m 27s) (19960 96%) 0.8708\n",
      "459m 41s (- 16m 13s) (19970 96%) 0.8587\n",
      "459m 55s (- 15m 59s) (19980 96%) 0.8638\n",
      "460m 9s (- 15m 46s) (19990 96%) 0.8477\n",
      "460m 23s (- 15m 32s) (20000 96%) 0.8624\n",
      "460m 37s (- 15m 18s) (20010 96%) 0.8629\n",
      "460m 50s (- 15m 4s) (20020 96%) 0.8555\n",
      "461m 4s (- 14m 50s) (20030 96%) 0.8654\n",
      "461m 18s (- 14m 37s) (20040 96%) 0.8702\n",
      "461m 32s (- 14m 23s) (20050 96%) 0.8912\n",
      "461m 45s (- 14m 9s) (20060 97%) 0.8041\n",
      "461m 59s (- 13m 55s) (20070 97%) 0.7860\n",
      "462m 13s (- 13m 41s) (20080 97%) 0.8776\n",
      "462m 27s (- 13m 27s) (20090 97%) 0.9024\n",
      "462m 41s (- 13m 14s) (20100 97%) 0.9336\n",
      "462m 54s (- 13m 0s) (20110 97%) 0.8262\n",
      "463m 8s (- 12m 46s) (20120 97%) 0.8659\n",
      "463m 22s (- 12m 32s) (20130 97%) 0.8630\n",
      "463m 36s (- 12m 18s) (20140 97%) 0.8912\n",
      "463m 50s (- 12m 5s) (20150 97%) 0.8514\n",
      "464m 4s (- 11m 51s) (20160 97%) 0.8466\n",
      "464m 17s (- 11m 37s) (20170 97%) 0.8266\n",
      "464m 31s (- 11m 23s) (20180 97%) 0.8612\n",
      "464m 45s (- 11m 9s) (20190 97%) 0.8559\n",
      "464m 59s (- 10m 56s) (20200 97%) 0.8862\n",
      "465m 13s (- 10m 42s) (20210 97%) 0.8750\n",
      "465m 26s (- 10m 28s) (20220 97%) 0.8871\n",
      "465m 40s (- 10m 14s) (20230 97%) 0.8105\n",
      "465m 54s (- 10m 0s) (20240 97%) 0.8394\n",
      "466m 8s (- 9m 46s) (20250 97%) 0.8545\n",
      "466m 22s (- 9m 33s) (20260 97%) 0.8339\n",
      "466m 35s (- 9m 19s) (20270 98%) 0.8071\n",
      "466m 49s (- 9m 5s) (20280 98%) 0.8566\n",
      "467m 3s (- 8m 51s) (20290 98%) 0.8264\n",
      "467m 17s (- 8m 37s) (20300 98%) 0.8505\n",
      "467m 31s (- 8m 24s) (20310 98%) 0.8592\n",
      "467m 45s (- 8m 10s) (20320 98%) 0.9265\n",
      "467m 58s (- 7m 56s) (20330 98%) 0.9244\n",
      "468m 12s (- 7m 42s) (20340 98%) 0.8369\n",
      "468m 26s (- 7m 28s) (20350 98%) 0.8271\n",
      "468m 40s (- 7m 15s) (20360 98%) 0.8331\n",
      "468m 54s (- 7m 1s) (20370 98%) 0.8469\n",
      "469m 7s (- 6m 47s) (20380 98%) 0.8366\n",
      "469m 21s (- 6m 33s) (20390 98%) 0.8472\n",
      "469m 35s (- 6m 19s) (20400 98%) 0.8177\n",
      "469m 49s (- 6m 6s) (20410 98%) 0.8176\n",
      "470m 3s (- 5m 52s) (20420 98%) 0.8310\n",
      "470m 16s (- 5m 38s) (20430 98%) 0.8178\n",
      "470m 30s (- 5m 24s) (20440 98%) 0.8582\n",
      "470m 44s (- 5m 10s) (20450 98%) 0.9145\n",
      "470m 58s (- 4m 56s) (20460 98%) 0.8324\n",
      "471m 12s (- 4m 43s) (20470 99%) 0.8207\n",
      "471m 25s (- 4m 29s) (20480 99%) 0.8372\n",
      "471m 39s (- 4m 15s) (20490 99%) 0.8682\n",
      "471m 53s (- 4m 1s) (20500 99%) 0.8183\n",
      "472m 7s (- 3m 47s) (20510 99%) 0.8183\n",
      "472m 21s (- 3m 34s) (20520 99%) 0.8169\n",
      "472m 35s (- 3m 20s) (20530 99%) 0.8420\n",
      "472m 48s (- 3m 6s) (20540 99%) 0.8418\n",
      "473m 2s (- 2m 52s) (20550 99%) 0.8285\n",
      "473m 16s (- 2m 38s) (20560 99%) 0.8785\n",
      "473m 30s (- 2m 25s) (20570 99%) 0.8008\n",
      "473m 44s (- 2m 11s) (20580 99%) 0.8309\n",
      "473m 57s (- 1m 57s) (20590 99%) 0.8434\n",
      "474m 11s (- 1m 43s) (20600 99%) 0.8392\n",
      "474m 25s (- 1m 29s) (20610 99%) 0.7943\n",
      "474m 39s (- 1m 15s) (20620 99%) 0.9207\n",
      "474m 53s (- 1m 2s) (20630 99%) 0.8484\n",
      "475m 7s (- 0m 48s) (20640 99%) 0.8729\n",
      "475m 20s (- 0m 34s) (20650 99%) 0.8336\n",
      "475m 34s (- 0m 20s) (20660 99%) 0.8330\n",
      "475m 48s (- 0m 6s) (20670 99%) 0.8543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2267380981445317,\n",
       " 1.5309978647232052,\n",
       " 1.5039132919311518,\n",
       " 1.4784453029632567,\n",
       " 1.464867513656617,\n",
       " 1.4707394542694092,\n",
       " 1.418556421279907,\n",
       " 1.4216064434051519,\n",
       " 1.4176494274139408,\n",
       " 1.4037039632797244,\n",
       " 1.4115832071304317,\n",
       " 1.4347862081527714,\n",
       " 1.392030652046204,\n",
       " 1.4094985542297362,\n",
       " 1.4140250177383424,\n",
       " 1.3699999494552617,\n",
       " 1.3676111764907832,\n",
       " 1.356375834465027,\n",
       " 1.3938911361694335,\n",
       " 1.3529297828674323,\n",
       " 1.3737692365646357,\n",
       " 1.3690073575973511,\n",
       " 1.3608892688751217,\n",
       " 1.3484640855789187,\n",
       " 1.371564226150513,\n",
       " 1.3607674436569217,\n",
       " 1.3786814928054807,\n",
       " 1.338022820472717,\n",
       " 1.3246196937561026,\n",
       " 1.318066648483277,\n",
       " 1.2928564739227295,\n",
       " 1.2917645702362064,\n",
       " 1.307009921073914,\n",
       " 1.2724993705749512,\n",
       " 1.2481625375747678,\n",
       " 1.2462514991760254,\n",
       " 1.2544990348815916,\n",
       " 1.2482392654418943,\n",
       " 1.2434556112289432,\n",
       " 1.2099094018936158,\n",
       " 1.2309175033569335,\n",
       " 1.6430073003768926,\n",
       " 1.181596532821655,\n",
       " 1.1895738086700438,\n",
       " 1.1883351640701287,\n",
       " 1.1824496498107913,\n",
       " 1.1712074890136719,\n",
       " 1.1562964105606077,\n",
       " 1.1665651197433464,\n",
       " 1.1557705192565921,\n",
       " 1.1488857707977298,\n",
       " 1.144036237716675,\n",
       " 1.142993611335754,\n",
       " 1.1673722486495977,\n",
       " 1.162560158729553,\n",
       " 1.1259275150299073,\n",
       " 1.1422027063369748,\n",
       " 1.1303394093513495,\n",
       " 1.1308725824356076,\n",
       " 1.1496017637252807,\n",
       " 1.1020261077880862,\n",
       " 1.0920534887313837,\n",
       " 1.116467921257019,\n",
       " 1.0906214361190796,\n",
       " 1.1032572917938233,\n",
       " 1.0807138252258306,\n",
       " 1.1125728731155398,\n",
       " 1.1135104537010196,\n",
       " 1.0867966575622559,\n",
       " 1.0745411648750305,\n",
       " 1.0799440989494327,\n",
       " 1.0880023398399354,\n",
       " 1.076692458629608,\n",
       " 1.0600501279830934,\n",
       " 1.0714068946838373,\n",
       " 1.0789813575744631,\n",
       " 1.0395312814712525,\n",
       " 1.077914036273956,\n",
       " 1.0651546430587773,\n",
       " 1.066285268783569,\n",
       " 1.0510266389846805,\n",
       " 1.0383022580146788,\n",
       " 1.3701105914115899,\n",
       " 1.009394919872284,\n",
       " 1.0095022091865544,\n",
       " 1.0042130107879634,\n",
       " 1.0199751887321475,\n",
       " 1.008956384658813,\n",
       " 1.017685492038727,\n",
       " 1.0038333153724674,\n",
       " 0.9897395176887512,\n",
       " 1.0097831335067748,\n",
       " 1.002875425815582,\n",
       " 0.9804555654525758,\n",
       " 0.9882951145172121,\n",
       " 0.9532759613990782,\n",
       " 1.0011266503334046,\n",
       " 0.9950293850898743,\n",
       " 0.9858393387794494,\n",
       " 0.970349589824676,\n",
       " 0.9813914723396295,\n",
       " 0.9946630620956423,\n",
       " 0.9878645129203797,\n",
       " 0.9954039058685303,\n",
       " 0.9639972457885743,\n",
       " 0.9753517761230472,\n",
       " 0.9989046869277959,\n",
       " 0.9664864411354062,\n",
       " 0.9859601368904114,\n",
       " 0.9676189646720892,\n",
       " 0.9865765705108646,\n",
       " 0.9820336713790894,\n",
       " 0.9562001008987433,\n",
       " 0.962897232055664,\n",
       " 0.9665549540519714,\n",
       " 0.9635165886878968,\n",
       " 0.9590447235107428,\n",
       " 0.9712885780334471,\n",
       " 0.9567418131828309,\n",
       " 0.9657479834556579,\n",
       " 0.9568328146934509,\n",
       " 0.9506974291801454,\n",
       " 0.9455806427001952,\n",
       " 1.2415065755844112,\n",
       " 0.9172518033981321,\n",
       " 0.9152265291213992,\n",
       " 0.8980100746154785,\n",
       " 0.8962053890228274,\n",
       " 0.8973018326759333,\n",
       " 0.9100716390609747,\n",
       " 0.9271003074645995,\n",
       " 0.8854443249702453,\n",
       " 0.9158262348175051,\n",
       " 0.9175174622535712,\n",
       " 0.9021381931304933,\n",
       " 0.8843217949867247,\n",
       " 0.895513071060181,\n",
       " 0.8989584393501285,\n",
       " 0.8892661957740782,\n",
       " 0.904005794048309,\n",
       " 0.8795457448959354,\n",
       " 0.9226092658042908,\n",
       " 0.9157261123657228,\n",
       " 0.897494785308838,\n",
       " 0.9127737326622011,\n",
       " 0.8867235903739927,\n",
       " 0.9173037939071658,\n",
       " 0.8885802164077756,\n",
       " 0.9043026103973386,\n",
       " 0.9030635323524474,\n",
       " 0.8883286314010616,\n",
       " 0.917385051727295,\n",
       " 0.9051428742408753,\n",
       " 0.8901339497566221,\n",
       " 0.894306001186371,\n",
       " 0.8891418480873107,\n",
       " 0.910388101577759,\n",
       " 0.8973118972778319,\n",
       " 0.9160704970359801,\n",
       " 0.8951750969886781,\n",
       " 0.8986914262771607,\n",
       " 0.8882174382209778,\n",
       " 0.8814171156883236,\n",
       " 0.9002392334938046,\n",
       " 1.135937514781952,\n",
       " 0.8694066910743711,\n",
       " 0.8312038865089412,\n",
       " 0.8323330121040339,\n",
       " 0.8373838725090024,\n",
       " 0.8566821556091305,\n",
       " 0.8660862789154057,\n",
       " 0.8443051033020024,\n",
       " 0.8536955728530881,\n",
       " 0.8421246228218077,\n",
       " 0.8508059902191164,\n",
       " 0.8368234462738036,\n",
       " 0.8345371661186216,\n",
       " 0.846947928905487,\n",
       " 0.8358014655113222,\n",
       " 0.8467903890609744,\n",
       " 0.8373363609313962,\n",
       " 0.8512799677848814,\n",
       " 0.8525329184532161,\n",
       " 0.8355267672538755,\n",
       " 0.8485380091667181,\n",
       " 0.8492463111877437,\n",
       " 0.8483804297447206,\n",
       " 0.8507284765243529,\n",
       " 0.8434469385147098,\n",
       " 0.8490303974151608,\n",
       " 0.8275048637390137,\n",
       " 0.8387945985794069,\n",
       " 0.8410996580123903,\n",
       " 0.832004086494446,\n",
       " 0.8599688181877133,\n",
       " 0.8541771144866946,\n",
       " 0.8318577828407289,\n",
       " 0.8588012819290161,\n",
       " 0.8613719878196713,\n",
       " 0.8641214570999144,\n",
       " 0.8539943914413456,\n",
       " 0.8575992913246152,\n",
       " 0.8333269529342653,\n",
       " 0.841037734031677,\n",
       " 0.845760307788849]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "self_encoder = make_model(loaded_vi_embeddings, len(vi_ordered_words)).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, len(vi_ordered_words)).to(device)\n",
    "\n",
    "AttnTrainIters(train_vi_loader, self_encoder, attn_decoder, 5, print_every=10, plot_every=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reference lab8 1-nmt\n",
    "# def greedy_selfattn_evaluate(val_loader, encoder, decoder, en_id2words):\n",
    "#     #Will generate sentences 1 by 1. \n",
    "#     # process input sentence\n",
    "#     decoded_words_all = []\n",
    "#     decoder_attentions_all = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         encoder.eval()\n",
    "#         decoder.eval()\n",
    "        \n",
    "#         for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "#             #if i == 5:\n",
    "#             #    break\n",
    "#             #batch_size, max_len = output_list.size()\n",
    "#             #print(input_list.size())\n",
    "            \n",
    "#             # encode the source lanugage\n",
    "#             encoder_outputs = encoder(input_list)\n",
    "\n",
    "#             decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS\n",
    "#             decoder_hidden = torch.zeros(attn_decoder.n_layers, curr_batch, attn_decoder.hidden_size)\n",
    "\n",
    "#             # decode the context vector\n",
    "            \n",
    "#             # output of this function\n",
    "#             decoded_words = []\n",
    "#             decoder_attentions = torch.zeros(100, 100)\n",
    "\n",
    "#             for di in range(MAX_LENGTH):\n",
    "#                 # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "#                 decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                     decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "#                 top_score, topi = decoder_output.data.topk(1)\n",
    "#                 decoder_attentions[di, :decoder_attention.size(-1)] = decoder_attention\n",
    "#                 decoded_words.append(en_id2words[topi.item()])\n",
    "#                 if topi.item() == EOS_TOKEN:\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     decoder_input = topi.squeeze().detach()\n",
    "                    \n",
    "#             decoded_words_all.append(decoded_words)\n",
    "#             decoder_attentions_all.append(decoder_attentions[:di+1])\n",
    "\n",
    "#         return decoded_words_all, decoder_attentions_all\n",
    "\n",
    "#Reference lab8 1-nmt\n",
    "def greedy_attn_evaluate(val_loader, encoder, decoder, en_id2words ):\n",
    "    #Will generate sentences 1 by 1. \n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    decoded_words_all = []\n",
    "    decoder_attentions_all = []\n",
    "    \n",
    "#     batch_size, max_input_length = input_list.size()\n",
    "#     max_output_length = output_list.size(1)\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     encoder_outputs = self_encoder(input_list)\n",
    "\n",
    "#     #Initialize for decoding process\n",
    "# #     curr_batch = input_list.size(0)#Take the current batch size\n",
    "#     decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "#     #decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "#     decoder_hidden = torch.zeros(1, batch_size, attn_decoder.hidden_size).to(device)\n",
    "#     decoder_outputs = torch.zeros(max_output_length, batch_size, attn_decoder.output_size).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "            if i %100 == 0:\n",
    "                print(\"%d/%d\"%(i,len(val_loader)))\n",
    "                \n",
    "            batch_size, max_input_length = input_list.size()\n",
    "            max_output_length = output_list.size(1)\n",
    "            \n",
    "            #    break\n",
    "            #batch_size, max_len = output_list.size()\n",
    "#             print(input_list.size())\n",
    "            \n",
    "            encoder_outputs = self_encoder(input_list)\n",
    "\n",
    "            decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "#             decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS\n",
    "            # decode the context vector\n",
    "            decoder_hidden = torch.zeros(1, batch_size, attn_decoder.hidden_size).to(device)\n",
    "#             decoder_hidden = encoder_hidden[:decoder.n_layers] # decoder starts from the last encoding sentence\n",
    "            # output of this function\n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input.reshape(1, batch_size), decoder_hidden, encoder_outputs)\n",
    "\n",
    "                top_score, topi = decoder_output.data.topk(1)\n",
    "                decoder_attentions[di, :decoder_attention.size(-1)] = decoder_attention\n",
    "                decoded_words.append(en_id2words[topi.item()])\n",
    "                if topi.item() == EOS_TOKEN:\n",
    "                    break\n",
    "                else:\n",
    "                    decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "                    \n",
    "            decoded_words_all.append(decoded_words)\n",
    "            decoder_attentions_all.append(decoder_attentions[:di+1])\n",
    "\n",
    "        return decoded_words_all, decoder_attentions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(decoded_words_all):\n",
    "    cleaned_decoded_words_all = []\n",
    "    \n",
    "    for sentence in decoded_words_all:\n",
    "        cleaned_sentence = []\n",
    "        for word in sentence:\n",
    "            if word == '<PAD>':\n",
    "                continue\n",
    "            else:\n",
    "                cleaned_sentence.append(word)\n",
    "        if cleaned_sentence[-1] != '<EOS>':\n",
    "            cleaned_sentence.append(' <EOS>')\n",
    "            \n",
    "        cleaned_decoded_words_all.append(cleaned_sentence)\n",
    "        \n",
    "    return cleaned_decoded_words_all\n",
    "\n",
    "#Translate the test and val lists back to english\n",
    "def en_translate(index_list, en_id2words):\n",
    "    translated_sentence_list = []\n",
    "    for sentence in index_list:\n",
    "        translated_sentence = []\n",
    "        for index in sentence:\n",
    "            translated_sentence.append(en_id2words[index])\n",
    "        #translated_sentence.append('<EOS>')\n",
    "        translated_sentence_list.append(translated_sentence)\n",
    "    return translated_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'little',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'my',\n",
       " 'country',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'on',\n",
       " 'the',\n",
       " 'planet',\n",
       " 'and',\n",
       " 'i',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'singing',\n",
       " 'a',\n",
       " 'song',\n",
       " 'called',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'envy',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_en_val_list = [pair[1] for pair in vi_val_pairs_cleaned]\n",
    "translated_sentence_list = en_translate(vi_en_val_list, en_id2words)\n",
    "translated_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1262\n",
      "100/1262\n",
      "200/1262\n",
      "300/1262\n",
      "400/1262\n",
      "500/1262\n",
      "600/1262\n",
      "700/1262\n",
      "800/1262\n",
      "900/1262\n",
      "1000/1262\n",
      "1100/1262\n",
      "1200/1262\n"
     ]
    }
   ],
   "source": [
    "decoded_val, decoder_attentions = greedy_attn_evaluate(val_vi_loader, self_encoder, attn_decoder, en_id2words)\n",
    "decoded_clean = post_process(decoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu,raw_corpus_bleu\n",
    "\n",
    "def bleu_score(predicted_list,translated_list):\n",
    "    predicted_list_nopad = []\n",
    "    for ii in range(len(predicted_list)):\n",
    "        line = ''\n",
    "        for jj in predicted_list[ii]:\n",
    "            if jj != '<pad>':\n",
    "                line = line + ' ' + jj\n",
    "        predicted_list_nopad.append(line)\n",
    "    labels = []\n",
    "    for ii in range(len(translated_list)):\n",
    "        line = ''\n",
    "        for jj in translated_list[ii]:\n",
    "            if jj != '<pad>':\n",
    "                line = line + ' ' + jj\n",
    "        labels.append(line)\n",
    "    #print(len(labels))\n",
    "    #print(len(predicted_list_nopad))\n",
    "    print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [labels]).score)\n",
    "    print('bleu score for test dataset [raw]:', raw_corpus_bleu(predicted_list_nopad, [labels]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 15.055388775328003\n",
      "bleu score for test dataset [raw]: 6.6046179332195605\n"
     ]
    }
   ],
   "source": [
    "bleu_score(decoded_clean,translated_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
