{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from queue import PriorityQueue\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from sacrebleu import raw_corpus_bleu\n",
    "import math, copy, time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define constants here\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 3\n",
    "words_to_load = 80000\n",
    "d_model = emb_size = 300\n",
    "wiki_size = 300\n",
    "CUDA = True\n",
    "MAX_LENGTH = 50\n",
    "hidden_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftdir = '/scratch/yz4499/fasttext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_word2id, emb_id2word, emb_ordered_words):\n",
    "        self.name = name\n",
    "        self.word2index = emb_word2id\n",
    "        self.word2count = {}\n",
    "        self.index2word = emb_id2word #Dict\n",
    "        self.n_words = 4  # Count SOS and EOS +(batch: pad and unk)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2count:\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #This line is commented out since it will not properly deal with Chinese Letters\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "#reference: LAB4 hw2\n",
    "def indexesFromSentences(lang1, lang2, pairs):\n",
    "    id_list1 = []\n",
    "    id_list2 = []\n",
    "    for i in range(len(pairs)):\n",
    "        sentence1 = pairs[i][0]\n",
    "        sentence2 = pairs[i][1]\n",
    "        \n",
    "        sentence1 = sentence1.replace('quot','')\n",
    "        sentence1 = sentence1.replace('apos', '')\n",
    "        sentence2 = sentence2.replace('quot','')\n",
    "        sentence2 = sentence2.replace('apos', '')\n",
    "        #If either sentence is empty, then remove the pair\n",
    "        if sentence1 == '' or sentence2 == '':\n",
    "            continue;\n",
    "        \n",
    "        id_sentence1 = [lang1.word2index[word] if word in lang1.word2index else UNK_TOKEN \n",
    "                        for word in sentence1.split()] + [EOS_TOKEN]\n",
    "        id_list1.append(id_sentence1)\n",
    "        id_sentence2 = [lang2.word2index[word] if word in lang2.word2index else UNK_TOKEN \n",
    "                        for word in sentence2.split()] + [EOS_TOKEN]\n",
    "        id_list2.append(id_sentence2)\n",
    "        \n",
    "   \n",
    "        \n",
    "    return id_list1,id_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "datadir = os.getcwd()\n",
    "words_to_load = 50000\n",
    "# with open(datadir + '/data/wiki-news-300d-1M.vec') as f:\n",
    "with open(ftdir + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_en_embeddings = np.zeros(((words_to_load+4), wiki_size))\n",
    "    en_word2id = {}\n",
    "    en_id2words = {}\n",
    "    \n",
    "    en_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    en_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    en_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    en_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    en_word2id['<PAD>'] = PAD_TOKEN\n",
    "    en_word2id['<SOS>'] = SOS_TOKEN\n",
    "    en_word2id['<EOS>'] = EOS_TOKEN\n",
    "    en_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    en_ordered_words= []\n",
    "    en_ordered_words.append('<PAD>')\n",
    "    en_ordered_words.append('<SOS>')\n",
    "    en_ordered_words.append('<EOS>')\n",
    "    en_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        if i ==0:#Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        #print(len(s))\n",
    "        loaded_en_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        en_word2id[s[0]] = i+4 #for extra pad and unk eos and unk\n",
    "        en_id2words[i+4] = s[0]\n",
    "        en_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "#Over 200000 loaded words, only 10 has wrong dimensions, simply discarded\n",
    "words_to_load = 50000\n",
    "# datadir = os.getcwd()\n",
    "# with open(datadir + '/data/wiki.zh.vec') as f:\n",
    "with open(ftdir + 'cc.zh.300.vec') as f:\n",
    "    loaded_zh_embeddings = np.zeros(((words_to_load+4),wiki_size))\n",
    "    zh_word2id = {}\n",
    "    zh_id2words = {}\n",
    "    \n",
    "    zh_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    zh_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    zh_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    zh_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    zh_word2id['<PAD>'] = PAD_TOKEN\n",
    "    zh_word2id['<SOS>'] = SOS_TOKEN\n",
    "    zh_word2id['<EOS>'] = EOS_TOKEN\n",
    "    zh_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    zh_ordered_words= []\n",
    "    zh_ordered_words.append('<PAD>')\n",
    "    zh_ordered_words.append('<SOS>')\n",
    "    zh_ordered_words.append('<EOS>')\n",
    "    zh_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        #print(i)\n",
    "        if i >= words_to_load:\n",
    "            break;\n",
    "        if i == 0: #Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        if len(s) != 301:\n",
    "            print('Wrong dimension, skip')\n",
    "            continue;\n",
    "        loaded_zh_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        zh_word2id[s[0]] = i+4 #for extra pad and unk \n",
    "        zh_id2words[i+4] = s[0]\n",
    "        zh_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #loading data\n",
    "zh_train_pairs_cleaned = pkl.load(open('./data/zh_train_pairs_cleaned.p', 'rb'))\n",
    "zh_val_pairs_cleaned = pkl.load(open('./data/zh_val_pairs_cleaned.p', 'rb'))\n",
    "zh_test_pairs_cleaned = pkl.load(open('./data/zh_test_pairs_cleaned.p', 'rb'))\n",
    "\n",
    "# vi_train_pairs_cleaned = pkl.load(open('./data/vi_train_pairs_cleaned.p', 'rb'))\n",
    "# vi_val_pairs_cleaned = pkl.load(open('./data/vi_val_pairs_cleaned.p', 'rb'))\n",
    "# vi_test_pairs_cleaned = pkl.load(open('./data/vi_test_pairs_cleaned.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs):#Needs the index pairs\n",
    "        self.pairs = pairs\n",
    "#         self.input_lang = input_lang\n",
    "#         self.output_lang = output_lang\n",
    "        self.input_seqs = [pairs[i][0] for i in range(len(self.pairs))]\n",
    "        self.output_seqs = [pairs[i][1] for i in range(len(self.pairs))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)#Returning number of pairs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.input_seqs[index]\n",
    "        output_seq = self.output_seqs[index]\n",
    "        return [input_seq, len(input_seq), output_seq, len(output_seq)]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    #Reference: lab8_3_mri\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "#         padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        padded_seqs = torch.zeros(len(seqs), MAX_LENGTH).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    batch_input_seqs = [datum[0] for datum in batch]\n",
    "    batch_output_seqs = [datum[2] for datum in batch]\n",
    "    #batch_input_length = [datum[1] for datum in batch]\n",
    "    #batch_output_length = [datum[3] for datum in batch]\n",
    "\n",
    "    sorted_pairs = sorted(zip(batch_input_seqs, batch_output_seqs), key=lambda x: len(x[0]), reverse = True)\n",
    "    in_seq_sorted, out_seq_sorted = zip(*sorted_pairs)\n",
    "    \n",
    "    padded_input,input_lens = _pad_sequences(in_seq_sorted)\n",
    "    padded_output,output_lens = _pad_sequences(out_seq_sorted)\n",
    "    \n",
    "    input_list = torch.from_numpy(np.array(padded_input))\n",
    "    input_length = torch.LongTensor(input_lens)\n",
    "    output_list = torch.from_numpy(np.array(padded_output))\n",
    "    output_length = torch.LongTensor(output_lens)\n",
    "    \n",
    "    if CUDA:\n",
    "        input_list = input_list.cuda()\n",
    "        output_list = output_list.cuda()\n",
    "        input_length = input_length.cuda()\n",
    "        output_length = output_length.cuda()\n",
    "            \n",
    "    return [input_list, input_length, output_list, output_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "'''\n",
    "NMTDataset needs index pairs, need to call indexesFromPairs functions beforehand\n",
    "The dataLoader is sorted according to length of the input_length, and padded to\n",
    "max length of input and output list repectively\n",
    "TODO: output_list is not sorted, hence need to sort (maybe) in the rnn sequence.\n",
    "'''\n",
    "# train_zh_dataset = NMTDataset(zh_train_pairs_cleaned, input_zh, output_zh_en)\n",
    "# train_vi_dataset = NMTDataset(vi_train_pairs_cleaned, input_vi, output_vi_en)\n",
    "# val_zh_dataset = NMTDataset(zh_val_pairs_cleaned, input_zh, output_zh_en)\n",
    "# val_vi_dataset = NMTDataset(vi_val_pairs_cleaned, input_vi, output_vi_en)\n",
    "# test_zh_dataset = NMTDataset(zh_test_pairs_cleaned, input_zh, output_zh_en)\n",
    "# test_vi_dataset = NMTDataset(vi_test_pairs_cleaned, input_vi, output_vi_en)\n",
    "\n",
    "train_zh_dataset = NMTDataset(zh_train_pairs_cleaned)\n",
    "# train_vi_dataset = NMTDataset(vi_train_pairs_cleaned)\n",
    "val_zh_dataset = NMTDataset(zh_val_pairs_cleaned)\n",
    "# val_vi_dataset = NMTDataset(vi_val_pairs_cleaned)\n",
    "test_zh_dataset = NMTDataset(zh_test_pairs_cleaned)\n",
    "# test_vi_dataset = NMTDataset(vi_test_pairs_cleaned)\n",
    "\n",
    "\n",
    "train_zh_loader = torch.utils.data.DataLoader(dataset = train_zh_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "\n",
    "# train_vi_loader = torch.utils.data.DataLoader(dataset = train_vi_dataset, \n",
    "#                                           batch_size = BATCH_SIZE,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = True)\n",
    "\n",
    "#Will use batch size 1 for validation and test since the sentence will be translated one by one\n",
    "val_zh_loader = torch.utils.data.DataLoader(dataset = val_zh_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "# val_vi_loader = torch.utils.data.DataLoader(dataset = val_vi_dataset, \n",
    "#                                           batch_size = 1,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = False)\n",
    "test_zh_loader = torch.utils.data.DataLoader(dataset = test_zh_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "# test_vi_loader = torch.utils.data.DataLoader(dataset = test_vi_dataset, \n",
    "#                                           batch_size = 1,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = False)\n",
    "#Input_batch in size Batch x maxLen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (input_list, input_length, output_list, output_length) in enumerate(train_zh_loader):\n",
    "    if i== 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "# MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 300\n",
    "max_length = 10\n",
    "BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n",
    "# CLIP = 50\n",
    "TEACHER_RATIO = 0.5\n",
    "# EN_ORDERED_NUM = 200003\n",
    "# ZH_ORDERED_NUM = 199945\n",
    "# VI_ORDERED_NUM = 199993\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, emb, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.emb = emb\n",
    "        self.lut = nn.Embedding.from_pretrained(self.emb, False, False)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference: https://stackoverflow.com/questions/52922445/runtimeerror-exp-not-implemented-for-torch-longtensor\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "       \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SelfEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A self attention based Encoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, src_embed):\n",
    "        super(SelfEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        #self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        #self.tgt_embed = tgt_embed\n",
    "        #self.generator = generator\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_model(emb, src_vocab, N=6, \n",
    "               d_model=300, d_ff=2048, h=6, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = SelfEncoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(emb, d_model, src_vocab), c(position)))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Reference: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "# #Reference: lab8 1_nmt, lab8 3_mri\n",
    "# class PreAttnDecoderRNN(nn.Module):\n",
    "#     def __init__(self, emb, emb_size, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "#         super(PreAttnDecoderRNN, self).__init__()\n",
    "#         self.emb = emb\n",
    "#         self.emb_size = emb_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size#vocab size of the output lang\n",
    "#         self.n_layers = n_layers\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "#         self.embedding = nn.Embedding.from_pretrained(self.emb, False, False,)\n",
    "        \n",
    "#         #self.attn = nn.Linear(hidden_size*, hidden_size)\n",
    "#         #self.attn2 = nn.Linear(hidden_size, hidden_size)\n",
    "# #         self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "#         self.concat = nn.Linear(2*hidden_size, hidden_size)\n",
    "#         self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "#     def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "#         #Here encoder_outputs is in size batch x max_seq x d_model\n",
    "#         self.batch_size = encoder_outputs.size(0)\n",
    "#         max_len = encoder_outputs.size(1)\n",
    "#         attn_energies = Variable(torch.zeros(self.batch_size, max_len)).to(device)#B X max_len\n",
    "# #         attn_energies = attn_energies.cuda() if CUDA else attn_energies\n",
    "        \n",
    "        \n",
    "#         embedded = self.embedding(word_input)\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         embedded = embedded.view(1, self.batch_size, -1) # S=1 x B x N\n",
    "        \n",
    "#         # Get current hidden state from input word and last hidden state\n",
    "#         rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "#         #rnn layer x batch x h\n",
    "#         #encoder-outputs  max_len x batch x h\n",
    "        \n",
    "# #         for b in range(self.batch_size):\n",
    "# #             # Calculate energy for each encoder output\n",
    "# #             for i in range(max_len):\n",
    "# #                 attn_energies[b, i] = (rnn_output[:, b].squeeze()).dot(encoder_outputs[i, b])\n",
    "        \n",
    "#         # Calculate attention from current RNN state and all encoder outputs;\n",
    "#         #More efficient\n",
    "#         #attn_energies = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(0,1).transpose(1,2))).squeeze(1)\n",
    "#         attn_energies = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(1,2))).squeeze(1)\n",
    "\n",
    "#         attn_weights = F.softmax(attn_energies) # B x max_len\n",
    "#         attn_weights = attn_weights.unsqueeze(1)\n",
    "#         # apply to encoder outputs to get weighted average\n",
    "#         context = attn_weights.bmm(encoder_outputs) # B x S=1 x N\n",
    "        \n",
    "#         # Attentional vector using the RNN hidden state and context vector\n",
    "\n",
    "#         # concatenated together (Luong eq. 5)\n",
    "#         rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "#         context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "#         concat_input = torch.cat((rnn_output, context), 1)\n",
    "#         concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "#         # Finally predict next token (Luong eq. 6, without softmax)\n",
    "#         output = self.out(concat_output)\n",
    "#         output = F.log_softmax(output)\n",
    "\n",
    "\n",
    "#         # Return final output, hidden state, and attention weights (for visualization)\n",
    "#         return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "#Reference: Lab8 nmt-1\n",
    "class PreBatchAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, emb, emb_size, hidden_size, output_size, n_layers = 1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(PreBatchAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #self.attn = nn.Linear(hidden_size*2, hidden_size)\n",
    "        #self.attn2 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.emb, True)\n",
    "        self.attn = nn.Linear(emb_size + hidden_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(emb_size + hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first = False)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, decoder_input, hidden, encoder_outputs):\n",
    "        self.batch_size = encoder_outputs.size(0)\n",
    "        embedded = self.embedding(decoder_input).view(1, self.batch_size, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        concat_input = torch.cat((embedded, hidden), 2)\n",
    "        attn_weights = F.softmax(self.attn(concat_input), dim=2)\n",
    "        attn_energies = torch.bmm(attn_weights.squeeze(0).unsqueeze(1), encoder_outputs).squeeze(1)#Batch x H\n",
    "\n",
    "        output = torch.cat((embedded.squeeze(0), attn_energies), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        rnn_output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(rnn_output.squeeze(0)),dim=1)\n",
    "        #Size: output b x V, hidden 1 x b x h, attn 1 x b x max_length\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for decoder PLS ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_batch = input_list.size(0)\n",
    "decoder_input = torch.tensor([[SOS_TOKEN]]*curr_batch, device=device)\n",
    "    \n",
    "#decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "decoder_hidden = torch.zeros(attn_decoder.n_layers, curr_batch, attn_decoder.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = encoder_outputs.size(0)\n",
    "max_len = encoder_outputs.size(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_energies = Variable(torch.zeros(batch_size, max_len))#B X max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(attn_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn_weights = attn_weights.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 56]), torch.Size([32, 56, 300]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.size(), encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = attn_weights.bmm(encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 300])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_output = rnn_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = context.squeeze(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 300])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded = loaded_en_embeddings[decoder_input]\n",
    "\n",
    "embedded = embedded.view(1, batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 300])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_output, hidden = attn_decoder.gru(embedded, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 56, 300])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "# Calculate energy for each encoder output\n",
    "    for i in range(max_len):\n",
    "        attn_energies[b, i] = (rnn_output[:, b].squeeze()).dot(encoder_outputs[b, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_test = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(1,2))).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(temp_test, attn_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_input = torch.cat((rnn_output, context), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concat_output = F.tanh(attn_decoder.concat(concat_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 300])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = attn_decoder.out(concat_output)\n",
    "output = F.log_softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of decode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_zh_embeddings = torch.from_numpy(loaded_zh_embeddings).float()\n",
    "# loaded_vi_embeddings = torch.from_numpy(loaded_vi_embeddings).float()\n",
    "loaded_en_embeddings = torch.from_numpy(loaded_en_embeddings).float()\n",
    "\n",
    "if CUDA:\n",
    "    loaded_zh_embeddings = loaded_zh_embeddings.cuda()\n",
    "#     loaded_vi_embeddings = loaded_vi_embeddings.cuda()\n",
    "    loaded_en_embeddings = loaded_en_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Example of encoder and decoder\n",
    "# self_encoder = make_model(loaded_vi_embeddings, len(vi_ordered_words)).to(device)\n",
    "# attn_decoder = PreBatchAttnDecoderRNN(loaded_en_embeddings, emb_size,hidden_size, len(en_ordered_words)).to(device)\n",
    "\n",
    "# #SLOW\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.Adam(self_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder_outputs = self_encoder(input_list)\n",
    "# encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss = self_attn_train(input_list, output_list, output_length, self_encoder, attn_decoder, \n",
    "#                       encoder_optimizer, decoder_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Referenced from lab8 1nmt and modified \n",
    "def self_attn_train(input_list, output_list,output_length, \n",
    "                self_encoder, attn_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    '''\n",
    "    param: @attention is a Boolean variable indicating whether using attention\n",
    "    '''\n",
    "    self_encoder.train()\n",
    "    attn_decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, max_input_length = input_list.size()\n",
    "    max_output_length = output_list.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs = self_encoder(input_list)\n",
    "\n",
    "    #Initialize for decoding process\n",
    "#     curr_batch = input_list.size(0)#Take the current batch size\n",
    "    decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    #decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "    decoder_hidden = torch.zeros(1, batch_size, attn_decoder.hidden_size).to(device)\n",
    "    decoder_outputs = torch.zeros(max_output_length, batch_size, attn_decoder.output_size).to(device)\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "#     if CUDA:\n",
    "#         decoder_input = decoder_input.cuda()\n",
    "#         decoder_outputs = decoder_outputs.cuda()\n",
    "    \n",
    "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = attn_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs[di] = decoder_output\n",
    "            decoder_input = output_list[:,di].unsqueeze(0) # Teacher forcing\n",
    "            loss += criterion(decoder_output, output_list[:,di].contiguous())\n",
    "\n",
    "    else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = attn_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_outputs[di] = decoder_input\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)# detach from history as input: size batch x 1 \n",
    "            loss += criterion(decoder_output, output_list[:,di].contiguous())\n",
    "            \n",
    "    #loss += rnn_mask_loss(decoder_outputs.transpose(0,1).contiguous(), output_list.contiguous(), output_length)\n",
    "            \n",
    "    loss.backward()\n",
    "    #ec = torch.nn.utils.clip_grad_norm(batch_encoder.parameters(), CLIP)\n",
    "    #dc = torch.nn.utils.clip_grad_norm(batch_decoder.parameters(), CLIP)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference LAB8 1-nmt\n",
    "model_path = './model/'\n",
    "def AttnTrainIters(train_loader, self_encoder, attn_decoder, n_iters, \n",
    "                   print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(self_encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    epoch = 0\n",
    "    epoch_total = n_iters*len(train_loader)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        #print(\"Epoch {}/{}\".format(i+1, n_epochs))\n",
    "        for i, (input_list,input_length,output_list, output_length) in enumerate(train_loader):\n",
    "            loss = self_attn_train(input_list, output_list, output_length, self_encoder, attn_decoder, \n",
    "                      encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i > 0 and i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epoch_total),\n",
    "                                             epoch, epoch / epoch_total * 100, print_loss_avg))\n",
    "\n",
    "            if i > 0 and i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "            epoch += 1\n",
    "            \n",
    "        torch.save(self_encoder.state_dict(), model_path + \"zh-self_encoder.pth\")\n",
    "        torch.save(attn_decoder.state_dict(), model_path + \"zh-self_attn_decoder.pth\")\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 27s (- 465m 33s) (100 0%) 3.1725\n",
      "2m 55s (- 463m 46s) (200 0%) 2.3257\n",
      "4m 23s (- 462m 6s) (300 0%) 2.2121\n",
      "5m 51s (- 460m 29s) (400 1%) 2.2091\n",
      "7m 18s (- 459m 0s) (500 1%) 2.1875\n",
      "8m 46s (- 457m 28s) (600 1%) 2.1591\n",
      "10m 14s (- 455m 59s) (700 2%) 2.1636\n",
      "11m 41s (- 454m 31s) (800 2%) 2.1339\n",
      "13m 9s (- 453m 4s) (900 2%) 2.1240\n",
      "14m 37s (- 451m 35s) (1000 3%) 2.1561\n",
      "16m 5s (- 450m 6s) (1100 3%) 2.1017\n",
      "17m 32s (- 448m 40s) (1200 3%) 2.1222\n",
      "19m 0s (- 447m 12s) (1300 4%) 2.0965\n",
      "20m 28s (- 445m 43s) (1400 4%) 2.0628\n",
      "21m 56s (- 444m 15s) (1500 4%) 2.0863\n",
      "23m 23s (- 442m 48s) (1600 5%) 2.0442\n",
      "24m 51s (- 441m 19s) (1700 5%) 2.0645\n",
      "26m 19s (- 439m 52s) (1800 5%) 2.0911\n",
      "27m 47s (- 438m 25s) (1900 5%) 2.0669\n",
      "29m 14s (- 436m 57s) (2000 6%) 2.0243\n",
      "30m 42s (- 435m 30s) (2100 6%) 2.0239\n",
      "32m 10s (- 434m 3s) (2200 6%) 1.9897\n",
      "33m 38s (- 432m 36s) (2300 7%) 1.9822\n",
      "35m 6s (- 431m 9s) (2400 7%) 1.9993\n",
      "36m 33s (- 429m 42s) (2500 7%) 1.9429\n",
      "38m 1s (- 428m 14s) (2600 8%) 1.9495\n",
      "39m 29s (- 426m 46s) (2700 8%) 1.9083\n",
      "40m 57s (- 425m 19s) (2800 8%) 1.9160\n",
      "42m 24s (- 423m 52s) (2900 9%) 1.9022\n",
      "43m 52s (- 422m 23s) (3000 9%) 1.8843\n",
      "45m 20s (- 420m 53s) (3100 9%) 1.8808\n",
      "46m 47s (- 419m 24s) (3200 10%) 1.8286\n",
      "48m 15s (- 417m 56s) (3300 10%) 1.8319\n",
      "49m 43s (- 416m 28s) (3400 10%) 1.8013\n",
      "51m 10s (- 415m 0s) (3500 10%) 1.8194\n",
      "52m 38s (- 413m 32s) (3600 11%) 1.8104\n",
      "54m 6s (- 412m 4s) (3700 11%) 1.8348\n",
      "55m 33s (- 410m 35s) (3800 11%) 1.7834\n",
      "57m 1s (- 409m 7s) (3900 12%) 1.8020\n",
      "58m 29s (- 407m 38s) (4000 12%) 1.7694\n",
      "59m 56s (- 406m 10s) (4100 12%) 1.8145\n",
      "61m 24s (- 404m 42s) (4200 13%) 1.7608\n",
      "62m 52s (- 403m 14s) (4300 13%) 1.7609\n",
      "64m 19s (- 401m 46s) (4400 13%) 1.7423\n",
      "65m 47s (- 400m 19s) (4500 14%) 1.7697\n",
      "67m 15s (- 398m 51s) (4600 14%) 1.7276\n",
      "68m 43s (- 397m 24s) (4700 14%) 1.7547\n",
      "70m 11s (- 395m 57s) (4800 15%) 1.7181\n",
      "71m 38s (- 394m 29s) (4900 15%) 1.7603\n",
      "73m 6s (- 393m 2s) (5000 15%) 1.6895\n",
      "74m 34s (- 391m 35s) (5100 15%) 1.6684\n",
      "76m 2s (- 390m 7s) (5200 16%) 1.6740\n",
      "77m 30s (- 388m 40s) (5300 16%) 1.6880\n",
      "78m 57s (- 387m 12s) (5400 16%) 1.6997\n",
      "80m 25s (- 385m 45s) (5500 17%) 1.7424\n",
      "81m 53s (- 384m 17s) (5600 17%) 1.6496\n",
      "83m 21s (- 382m 50s) (5700 17%) 1.6641\n",
      "84m 49s (- 381m 23s) (5800 18%) 1.6604\n",
      "86m 16s (- 379m 55s) (5900 18%) 1.6767\n",
      "87m 44s (- 378m 27s) (6000 18%) 1.6719\n",
      "89m 12s (- 376m 59s) (6100 19%) 1.6434\n",
      "90m 40s (- 375m 32s) (6200 19%) 1.6578\n",
      "92m 7s (- 374m 4s) (6300 19%) 1.6452\n",
      "94m 42s (- 371m 32s) (6476 20%) 2.8587\n",
      "96m 10s (- 370m 3s) (6576 20%) 1.5911\n",
      "97m 37s (- 368m 35s) (6676 20%) 1.6033\n",
      "99m 5s (- 367m 7s) (6776 21%) 1.5729\n",
      "100m 33s (- 365m 39s) (6876 21%) 1.6145\n",
      "102m 0s (- 364m 10s) (6976 21%) 1.6041\n",
      "103m 28s (- 362m 42s) (7076 22%) 1.6130\n",
      "104m 55s (- 361m 13s) (7176 22%) 1.5692\n",
      "106m 23s (- 359m 45s) (7276 22%) 1.5734\n",
      "107m 50s (- 358m 16s) (7376 23%) 1.5902\n",
      "109m 18s (- 356m 48s) (7476 23%) 1.5732\n",
      "110m 46s (- 355m 20s) (7576 23%) 1.5419\n",
      "112m 13s (- 353m 52s) (7676 24%) 1.5888\n",
      "113m 41s (- 352m 24s) (7776 24%) 1.5529\n",
      "115m 9s (- 350m 56s) (7876 24%) 1.5866\n",
      "116m 36s (- 349m 29s) (7976 25%) 1.5864\n",
      "118m 4s (- 348m 1s) (8076 25%) 1.5703\n",
      "119m 31s (- 346m 33s) (8176 25%) 1.5677\n",
      "120m 59s (- 345m 5s) (8276 25%) 1.5583\n",
      "122m 27s (- 343m 37s) (8376 26%) 1.5336\n",
      "123m 54s (- 342m 9s) (8476 26%) 1.5567\n",
      "125m 22s (- 340m 41s) (8576 26%) 1.5462\n",
      "126m 50s (- 339m 14s) (8676 27%) 1.5455\n",
      "128m 18s (- 337m 46s) (8776 27%) 1.5140\n",
      "129m 45s (- 336m 18s) (8876 27%) 1.5416\n",
      "131m 13s (- 334m 50s) (8976 28%) 1.5517\n",
      "132m 41s (- 333m 23s) (9076 28%) 1.5751\n",
      "134m 8s (- 331m 55s) (9176 28%) 1.5495\n",
      "135m 36s (- 330m 27s) (9276 29%) 1.5409\n",
      "137m 4s (- 328m 59s) (9376 29%) 1.5491\n",
      "138m 32s (- 327m 32s) (9476 29%) 1.5211\n",
      "139m 59s (- 326m 4s) (9576 30%) 1.5296\n",
      "141m 27s (- 324m 36s) (9676 30%) 1.5432\n",
      "142m 55s (- 323m 9s) (9776 30%) 1.5228\n",
      "144m 22s (- 321m 41s) (9876 30%) 1.5164\n",
      "145m 50s (- 320m 13s) (9976 31%) 1.5343\n",
      "147m 18s (- 318m 45s) (10076 31%) 1.5400\n",
      "148m 45s (- 317m 17s) (10176 31%) 1.5119\n",
      "150m 13s (- 315m 49s) (10276 32%) 1.4912\n",
      "151m 41s (- 314m 21s) (10376 32%) 1.5244\n",
      "153m 8s (- 312m 53s) (10476 32%) 1.5255\n",
      "154m 36s (- 311m 25s) (10576 33%) 1.4647\n",
      "156m 3s (- 309m 57s) (10676 33%) 1.5036\n",
      "157m 31s (- 308m 29s) (10776 33%) 1.4956\n",
      "158m 59s (- 307m 2s) (10876 34%) 1.4967\n",
      "160m 26s (- 305m 34s) (10976 34%) 1.5055\n",
      "161m 54s (- 304m 6s) (11076 34%) 1.4843\n",
      "163m 22s (- 302m 38s) (11176 35%) 1.5128\n",
      "164m 49s (- 301m 11s) (11276 35%) 1.4891\n",
      "166m 17s (- 299m 43s) (11376 35%) 1.4976\n",
      "167m 45s (- 298m 15s) (11476 35%) 1.5114\n",
      "169m 12s (- 296m 47s) (11576 36%) 1.4933\n",
      "170m 40s (- 295m 19s) (11676 36%) 1.4733\n",
      "172m 8s (- 293m 52s) (11776 36%) 1.5008\n",
      "173m 35s (- 292m 24s) (11876 37%) 1.4604\n",
      "175m 3s (- 290m 55s) (11976 37%) 1.4896\n",
      "176m 30s (- 289m 27s) (12076 37%) 1.4897\n",
      "177m 57s (- 287m 59s) (12176 38%) 1.4722\n",
      "179m 25s (- 286m 31s) (12276 38%) 1.4847\n",
      "180m 52s (- 285m 3s) (12376 38%) 1.4835\n",
      "182m 20s (- 283m 35s) (12476 39%) 1.4833\n",
      "183m 48s (- 282m 8s) (12576 39%) 1.4728\n",
      "185m 15s (- 280m 40s) (12676 39%) 1.4679\n",
      "187m 50s (- 278m 6s) (12852 40%) 2.4811\n",
      "189m 18s (- 276m 38s) (12952 40%) 1.3989\n",
      "190m 45s (- 275m 10s) (13052 40%) 1.4090\n",
      "192m 13s (- 273m 42s) (13152 41%) 1.3863\n",
      "193m 40s (- 272m 15s) (13252 41%) 1.3797\n",
      "195m 8s (- 270m 47s) (13352 41%) 1.3732\n",
      "196m 35s (- 269m 19s) (13452 42%) 1.3942\n",
      "198m 3s (- 267m 51s) (13552 42%) 1.3865\n",
      "199m 31s (- 266m 23s) (13652 42%) 1.4068\n",
      "200m 58s (- 264m 55s) (13752 43%) 1.3919\n",
      "202m 26s (- 263m 27s) (13852 43%) 1.3973\n",
      "203m 53s (- 261m 59s) (13952 43%) 1.3684\n",
      "205m 21s (- 260m 32s) (14052 44%) 1.4043\n",
      "206m 48s (- 259m 4s) (14152 44%) 1.3800\n",
      "208m 16s (- 257m 36s) (14252 44%) 1.4115\n",
      "209m 43s (- 256m 8s) (14352 45%) 1.3970\n",
      "211m 11s (- 254m 40s) (14452 45%) 1.3783\n",
      "212m 38s (- 253m 12s) (14552 45%) 1.3683\n",
      "214m 6s (- 251m 44s) (14652 45%) 1.3786\n",
      "215m 33s (- 250m 16s) (14752 46%) 1.4036\n",
      "217m 1s (- 248m 48s) (14852 46%) 1.3922\n",
      "218m 28s (- 247m 21s) (14952 46%) 1.3715\n",
      "219m 56s (- 245m 53s) (15052 47%) 1.3925\n",
      "221m 23s (- 244m 25s) (15152 47%) 1.3885\n",
      "222m 51s (- 242m 57s) (15252 47%) 1.3980\n",
      "224m 18s (- 241m 29s) (15352 48%) 1.3757\n",
      "225m 46s (- 240m 2s) (15452 48%) 1.3934\n",
      "227m 13s (- 238m 34s) (15552 48%) 1.3772\n",
      "228m 41s (- 237m 6s) (15652 49%) 1.3885\n",
      "230m 9s (- 235m 38s) (15752 49%) 1.3990\n",
      "231m 36s (- 234m 10s) (15852 49%) 1.3924\n",
      "233m 3s (- 232m 42s) (15952 50%) 1.3730\n",
      "234m 31s (- 231m 14s) (16052 50%) 1.3935\n",
      "235m 58s (- 229m 46s) (16152 50%) 1.3495\n",
      "237m 25s (- 228m 18s) (16252 50%) 1.3694\n",
      "238m 53s (- 226m 50s) (16352 51%) 1.3961\n",
      "240m 20s (- 225m 23s) (16452 51%) 1.3828\n",
      "241m 48s (- 223m 55s) (16552 51%) 1.3589\n",
      "243m 15s (- 222m 27s) (16652 52%) 1.3637\n",
      "244m 42s (- 220m 59s) (16752 52%) 1.3703\n",
      "246m 10s (- 219m 31s) (16852 52%) 1.3877\n",
      "247m 37s (- 218m 3s) (16952 53%) 1.3962\n",
      "249m 5s (- 216m 35s) (17052 53%) 1.3678\n",
      "250m 32s (- 215m 7s) (17152 53%) 1.3895\n",
      "251m 59s (- 213m 40s) (17252 54%) 1.3514\n",
      "253m 27s (- 212m 12s) (17352 54%) 1.3896\n",
      "254m 54s (- 210m 44s) (17452 54%) 1.3564\n",
      "256m 22s (- 209m 16s) (17552 55%) 1.3701\n",
      "257m 49s (- 207m 48s) (17652 55%) 1.3624\n",
      "259m 16s (- 206m 20s) (17752 55%) 1.3625\n",
      "260m 43s (- 204m 52s) (17852 55%) 1.3822\n",
      "262m 11s (- 203m 24s) (17952 56%) 1.3414\n",
      "263m 38s (- 201m 57s) (18052 56%) 1.3388\n",
      "265m 5s (- 200m 29s) (18152 56%) 1.3199\n",
      "266m 32s (- 199m 1s) (18252 57%) 1.3811\n",
      "268m 0s (- 197m 33s) (18352 57%) 1.3338\n",
      "269m 27s (- 196m 5s) (18452 57%) 1.3359\n",
      "270m 54s (- 194m 37s) (18552 58%) 1.3430\n",
      "272m 22s (- 193m 9s) (18652 58%) 1.3376\n",
      "273m 49s (- 191m 42s) (18752 58%) 1.3470\n",
      "275m 17s (- 190m 14s) (18852 59%) 1.3280\n",
      "276m 44s (- 188m 46s) (18952 59%) 1.3803\n",
      "278m 11s (- 187m 18s) (19052 59%) 1.3510\n",
      "280m 46s (- 184m 44s) (19228 60%) 2.2712\n",
      "282m 13s (- 183m 17s) (19328 60%) 1.2679\n",
      "283m 41s (- 181m 49s) (19428 60%) 1.2691\n",
      "285m 8s (- 180m 21s) (19528 61%) 1.2772\n",
      "286m 35s (- 178m 53s) (19628 61%) 1.2811\n",
      "288m 3s (- 177m 26s) (19728 61%) 1.2523\n",
      "289m 30s (- 175m 58s) (19828 62%) 1.2788\n",
      "290m 57s (- 174m 30s) (19928 62%) 1.2634\n",
      "292m 25s (- 173m 2s) (20028 62%) 1.2666\n",
      "293m 52s (- 171m 34s) (20128 63%) 1.2610\n",
      "295m 19s (- 170m 7s) (20228 63%) 1.2862\n",
      "296m 47s (- 168m 39s) (20328 63%) 1.2731\n",
      "298m 14s (- 167m 11s) (20428 64%) 1.2523\n",
      "299m 41s (- 165m 43s) (20528 64%) 1.2675\n",
      "301m 9s (- 164m 16s) (20628 64%) 1.2609\n",
      "302m 36s (- 162m 48s) (20728 65%) 1.2582\n",
      "304m 3s (- 161m 20s) (20828 65%) 1.2818\n",
      "305m 31s (- 159m 53s) (20928 65%) 1.2989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306m 58s (- 158m 25s) (21028 65%) 1.2631\n",
      "308m 25s (- 156m 57s) (21128 66%) 1.2652\n",
      "309m 53s (- 155m 29s) (21228 66%) 1.2448\n",
      "311m 20s (- 154m 2s) (21328 66%) 1.2984\n",
      "312m 47s (- 152m 34s) (21428 67%) 1.2660\n",
      "314m 15s (- 151m 6s) (21528 67%) 1.2545\n",
      "315m 42s (- 149m 38s) (21628 67%) 1.2759\n",
      "317m 9s (- 148m 11s) (21728 68%) 1.2602\n",
      "318m 36s (- 146m 43s) (21828 68%) 1.2611\n",
      "320m 4s (- 145m 15s) (21928 68%) 1.2777\n",
      "321m 31s (- 143m 47s) (22028 69%) 1.2887\n",
      "322m 58s (- 142m 20s) (22128 69%) 1.2840\n",
      "324m 25s (- 140m 52s) (22228 69%) 1.2848\n",
      "325m 53s (- 139m 24s) (22328 70%) 1.2797\n",
      "327m 20s (- 137m 57s) (22428 70%) 1.2612\n",
      "328m 47s (- 136m 29s) (22528 70%) 1.2930\n",
      "330m 15s (- 135m 1s) (22628 70%) 1.2810\n",
      "331m 42s (- 133m 34s) (22728 71%) 1.2785\n",
      "333m 9s (- 132m 6s) (22828 71%) 1.2826\n",
      "334m 37s (- 130m 38s) (22928 71%) 1.2603\n",
      "336m 4s (- 129m 11s) (23028 72%) 1.2713\n",
      "337m 32s (- 127m 43s) (23128 72%) 1.2539\n",
      "338m 59s (- 126m 16s) (23228 72%) 1.2907\n",
      "340m 27s (- 124m 48s) (23328 73%) 1.2881\n",
      "341m 54s (- 123m 20s) (23428 73%) 1.2817\n",
      "343m 22s (- 121m 53s) (23528 73%) 1.2442\n",
      "344m 49s (- 120m 25s) (23628 74%) 1.2849\n",
      "346m 16s (- 118m 58s) (23728 74%) 1.2888\n",
      "347m 43s (- 117m 30s) (23828 74%) 1.2782\n",
      "349m 11s (- 116m 2s) (23928 75%) 1.2900\n",
      "350m 38s (- 114m 35s) (24028 75%) 1.2750\n",
      "352m 5s (- 113m 7s) (24128 75%) 1.2824\n",
      "353m 33s (- 111m 39s) (24228 75%) 1.2475\n",
      "355m 0s (- 110m 12s) (24328 76%) 1.2860\n",
      "356m 27s (- 108m 44s) (24428 76%) 1.2641\n",
      "357m 55s (- 107m 16s) (24528 76%) 1.2814\n",
      "359m 22s (- 105m 49s) (24628 77%) 1.2586\n",
      "360m 50s (- 104m 21s) (24728 77%) 1.2669\n",
      "362m 17s (- 102m 54s) (24828 77%) 1.2653\n",
      "363m 44s (- 101m 26s) (24928 78%) 1.2862\n",
      "365m 12s (- 99m 58s) (25028 78%) 1.2755\n",
      "366m 39s (- 98m 31s) (25128 78%) 1.2747\n",
      "368m 7s (- 97m 3s) (25228 79%) 1.2591\n",
      "369m 34s (- 95m 36s) (25328 79%) 1.2513\n",
      "371m 1s (- 94m 8s) (25428 79%) 1.2541\n",
      "373m 36s (- 91m 34s) (25604 80%) 2.0835\n",
      "375m 4s (- 90m 7s) (25704 80%) 1.1812\n",
      "376m 31s (- 88m 39s) (25804 80%) 1.1884\n",
      "377m 58s (- 87m 11s) (25904 81%) 1.1821\n",
      "379m 25s (- 85m 44s) (26004 81%) 1.1821\n",
      "380m 52s (- 84m 16s) (26104 81%) 1.1712\n",
      "382m 19s (- 82m 48s) (26204 82%) 1.2206\n",
      "383m 47s (- 81m 21s) (26304 82%) 1.1765\n",
      "385m 14s (- 79m 53s) (26404 82%) 1.2000\n",
      "386m 41s (- 78m 26s) (26504 83%) 1.1832\n",
      "388m 8s (- 76m 58s) (26604 83%) 1.1831\n",
      "389m 36s (- 75m 30s) (26704 83%) 1.1718\n",
      "391m 3s (- 74m 3s) (26804 84%) 1.1971\n",
      "392m 30s (- 72m 35s) (26904 84%) 1.1779\n",
      "393m 57s (- 71m 8s) (27004 84%) 1.1824\n",
      "395m 25s (- 69m 40s) (27104 85%) 1.2005\n",
      "396m 52s (- 68m 13s) (27204 85%) 1.1937\n",
      "398m 19s (- 66m 45s) (27304 85%) 1.1828\n",
      "399m 46s (- 65m 17s) (27404 85%) 1.2310\n",
      "401m 13s (- 63m 50s) (27504 86%) 1.2256\n",
      "402m 41s (- 62m 22s) (27604 86%) 1.1996\n",
      "404m 8s (- 60m 55s) (27704 86%) 1.2321\n",
      "405m 35s (- 59m 27s) (27804 87%) 1.1999\n",
      "407m 2s (- 57m 59s) (27904 87%) 1.1978\n",
      "408m 29s (- 56m 32s) (28004 87%) 1.1916\n",
      "409m 56s (- 55m 4s) (28104 88%) 1.2009\n",
      "411m 24s (- 53m 37s) (28204 88%) 1.1895\n",
      "412m 51s (- 52m 9s) (28304 88%) 1.2021\n",
      "414m 18s (- 50m 42s) (28404 89%) 1.1982\n",
      "415m 46s (- 49m 14s) (28504 89%) 1.1916\n",
      "417m 13s (- 47m 47s) (28604 89%) 1.1909\n",
      "418m 40s (- 46m 19s) (28704 90%) 1.1870\n",
      "420m 7s (- 44m 51s) (28804 90%) 1.1969\n",
      "421m 35s (- 43m 24s) (28904 90%) 1.1999\n",
      "423m 2s (- 41m 56s) (29004 90%) 1.2142\n",
      "424m 29s (- 40m 29s) (29104 91%) 1.2116\n",
      "425m 55s (- 39m 1s) (29204 91%) 1.1877\n",
      "427m 22s (- 37m 34s) (29304 91%) 1.1992\n",
      "428m 49s (- 36m 6s) (29404 92%) 1.2003\n",
      "430m 15s (- 34m 38s) (29504 92%) 1.2005\n",
      "431m 42s (- 33m 11s) (29604 92%) 1.1843\n",
      "433m 8s (- 31m 43s) (29704 93%) 1.1952\n",
      "434m 35s (- 30m 16s) (29804 93%) 1.2154\n",
      "436m 2s (- 28m 48s) (29904 93%) 1.2140\n",
      "437m 28s (- 27m 21s) (30004 94%) 1.1988\n",
      "438m 54s (- 25m 53s) (30104 94%) 1.1917\n",
      "440m 21s (- 24m 26s) (30204 94%) 1.2036\n",
      "441m 48s (- 22m 58s) (30304 95%) 1.2081\n",
      "443m 14s (- 21m 31s) (30404 95%) 1.2097\n",
      "444m 41s (- 20m 3s) (30504 95%) 1.1644\n",
      "446m 7s (- 18m 36s) (30604 95%) 1.2274\n",
      "447m 34s (- 17m 8s) (30704 96%) 1.2153\n",
      "449m 1s (- 15m 41s) (30804 96%) 1.1956\n",
      "450m 27s (- 14m 13s) (30904 96%) 1.1850\n",
      "451m 54s (- 12m 46s) (31004 97%) 1.2253\n",
      "453m 21s (- 11m 18s) (31104 97%) 1.2021\n",
      "454m 47s (- 9m 51s) (31204 97%) 1.2241\n",
      "456m 14s (- 8m 23s) (31304 98%) 1.1983\n",
      "457m 41s (- 6m 56s) (31404 98%) 1.2015\n",
      "459m 7s (- 5m 28s) (31504 98%) 1.2121\n",
      "460m 34s (- 4m 1s) (31604 99%) 1.2271\n",
      "462m 0s (- 2m 33s) (31704 99%) 1.1966\n",
      "463m 27s (- 1m 6s) (31804 99%) 1.1780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1725371810913083,\n",
       " 2.3256503189086914,\n",
       " 2.212146612548828,\n",
       " 2.2090936004638673,\n",
       " 2.1874858612060546,\n",
       " 2.159147244262696,\n",
       " 2.1636260925292965,\n",
       " 2.133938365173341,\n",
       " 2.124004521179198,\n",
       " 2.156092054748535,\n",
       " 2.101654627990722,\n",
       " 2.1222064315795897,\n",
       " 2.0965353240966804,\n",
       " 2.062804957580566,\n",
       " 2.0862856689453126,\n",
       " 2.0441677261352544,\n",
       " 2.064520614624024,\n",
       " 2.091127497863769,\n",
       " 2.0668927261352548,\n",
       " 2.0242532516479494,\n",
       " 2.0238956451416006,\n",
       " 1.9896821533203126,\n",
       " 1.9821562805175779,\n",
       " 1.9993349761962897,\n",
       " 1.942914813232422,\n",
       " 1.9494950424194335,\n",
       " 1.9083024917602538,\n",
       " 1.9160343627929683,\n",
       " 1.9021659393310555,\n",
       " 1.8842550643920908,\n",
       " 1.8808449478149418,\n",
       " 1.8285670288085936,\n",
       " 1.8319193077087403,\n",
       " 1.8013206436157234,\n",
       " 1.8193561012268074,\n",
       " 1.8104171951293955,\n",
       " 1.834755006408692,\n",
       " 1.78335489654541,\n",
       " 1.8019796287536618,\n",
       " 1.769428837585448,\n",
       " 1.8144602035522461,\n",
       " 1.7608200782775876,\n",
       " 1.760891564941406,\n",
       " 1.7422825447082528,\n",
       " 1.7696539825439448,\n",
       " 1.72764622039795,\n",
       " 1.7546535720825194,\n",
       " 1.7181234146118172,\n",
       " 1.7602615890502933,\n",
       " 1.6894864273071295,\n",
       " 1.6684387695312504,\n",
       " 1.6739707344055177,\n",
       " 1.6879993515014642,\n",
       " 1.6997193893432614,\n",
       " 1.7424080780029303,\n",
       " 1.6495905067443852,\n",
       " 1.6641140373229986,\n",
       " 1.6604368667602538,\n",
       " 1.6767407196044917,\n",
       " 1.6719459716796872,\n",
       " 1.6434426368713373,\n",
       " 1.6578258850097651,\n",
       " 1.6452275177001956,\n",
       " 2.8587381156921383,\n",
       " 1.5911074729919432,\n",
       " 1.603341493988037,\n",
       " 1.572883917236328,\n",
       " 1.6145101982116703,\n",
       " 1.6041366348266601,\n",
       " 1.6129707176208496,\n",
       " 1.5692039360046388,\n",
       " 1.5733868247985834,\n",
       " 1.5901564132690431,\n",
       " 1.5732055885314944,\n",
       " 1.5418992446899418,\n",
       " 1.5887782791137692,\n",
       " 1.5528803855895992,\n",
       " 1.5865528732299807,\n",
       " 1.5864180519104008,\n",
       " 1.570273889923095,\n",
       " 1.5676704887390143,\n",
       " 1.5582889259338384,\n",
       " 1.5335631561279308,\n",
       " 1.5567082977294928,\n",
       " 1.546236693572998,\n",
       " 1.5454705345153819,\n",
       " 1.5139579750061034,\n",
       " 1.5415787338256837,\n",
       " 1.5517150314331054,\n",
       " 1.5750915336608877,\n",
       " 1.5495308174133302,\n",
       " 1.5409370674133296,\n",
       " 1.5490931236267094,\n",
       " 1.5210656066894535,\n",
       " 1.5296085716247558,\n",
       " 1.5431892700195318,\n",
       " 1.5228387725830081,\n",
       " 1.5164005767822257,\n",
       " 1.5343008224487305,\n",
       " 1.5400389709472655,\n",
       " 1.5118967712402347,\n",
       " 1.4912151832580562,\n",
       " 1.5243664382934567,\n",
       " 1.525456433105469,\n",
       " 1.4646635124206546,\n",
       " 1.5036087287902837,\n",
       " 1.495633653259277,\n",
       " 1.496725247192383,\n",
       " 1.5055413383483889,\n",
       " 1.4842902275085457,\n",
       " 1.512767092895509,\n",
       " 1.4891042373657235,\n",
       " 1.4975874458312988,\n",
       " 1.5113891120910645,\n",
       " 1.4933175392150886,\n",
       " 1.473260203552246,\n",
       " 1.5008063430786132,\n",
       " 1.4604413841247554,\n",
       " 1.4895577079772948,\n",
       " 1.4896628005981447,\n",
       " 1.4722286857604987,\n",
       " 1.484729698181153,\n",
       " 1.4835251144409178,\n",
       " 1.4832739921569813,\n",
       " 1.4728094970703125,\n",
       " 1.4678652168273927,\n",
       " 2.481117617034913,\n",
       " 1.3989081787109379,\n",
       " 1.4089875709533686,\n",
       " 1.3863076629638673,\n",
       " 1.379669127655029,\n",
       " 1.373170649719238,\n",
       " 1.394230368041992,\n",
       " 1.3864712890624995,\n",
       " 1.406812170410156,\n",
       " 1.3918890625,\n",
       " 1.3972589599609375,\n",
       " 1.368359394073486,\n",
       " 1.4043129783630368,\n",
       " 1.3800011993408203,\n",
       " 1.4115221046447761,\n",
       " 1.397006687164306,\n",
       " 1.3783365638732912,\n",
       " 1.3682518699645991,\n",
       " 1.378631703186035,\n",
       " 1.4036156600952154,\n",
       " 1.3921714294433598,\n",
       " 1.3714501441955564,\n",
       " 1.3924537536621096,\n",
       " 1.3885345695495603,\n",
       " 1.3979954078674313,\n",
       " 1.3756951232910157,\n",
       " 1.3934340103149412,\n",
       " 1.377229768371582,\n",
       " 1.3884844848632818,\n",
       " 1.3989675025939936,\n",
       " 1.392416675567626,\n",
       " 1.3729985214233402,\n",
       " 1.3934530120849609,\n",
       " 1.34949659576416,\n",
       " 1.3693638809204103,\n",
       " 1.3961431556701662,\n",
       " 1.3828352684021,\n",
       " 1.3588619171142584,\n",
       " 1.36369451675415,\n",
       " 1.3702874191284187,\n",
       " 1.3877139808654781,\n",
       " 1.396211023712158,\n",
       " 1.3677875946044926,\n",
       " 1.3895215286254878,\n",
       " 1.351418985748291,\n",
       " 1.3895515518188482,\n",
       " 1.3564287574768072,\n",
       " 1.3700776603698728,\n",
       " 1.3623648124694825,\n",
       " 1.3624972740173344,\n",
       " 1.3822387359619142,\n",
       " 1.3413733116149908,\n",
       " 1.3387582168579097,\n",
       " 1.3199296936035159,\n",
       " 1.381141337585449,\n",
       " 1.3338350738525393,\n",
       " 1.3358808029174805,\n",
       " 1.343030087280274,\n",
       " 1.3375726211547851,\n",
       " 1.3469915443420408,\n",
       " 1.328000148010254,\n",
       " 1.3803162605285644,\n",
       " 1.3510324607849113,\n",
       " 2.2711942672729504,\n",
       " 1.2678752670288085,\n",
       " 1.2690729629516602,\n",
       " 1.2771686187744138,\n",
       " 1.2810727157592776,\n",
       " 1.2522743583679197,\n",
       " 1.2788480529785153,\n",
       " 1.2634359901428223,\n",
       " 1.2665760276794429,\n",
       " 1.261009272766113,\n",
       " 1.2861528228759764,\n",
       " 1.2731389770507817,\n",
       " 1.252326343536377,\n",
       " 1.2674647033691402,\n",
       " 1.2609222396850586,\n",
       " 1.2582161865234374,\n",
       " 1.2818315223693846,\n",
       " 1.2989363273620607,\n",
       " 1.2630799522399903,\n",
       " 1.265200973510742,\n",
       " 1.2447672546386719,\n",
       " 1.298392528533936,\n",
       " 1.2660222908020018,\n",
       " 1.2545010726928711,\n",
       " 1.2759342338562012,\n",
       " 1.2602394691467282,\n",
       " 1.2610574554443357,\n",
       " 1.2777394165039064,\n",
       " 1.288667808532715,\n",
       " 1.2839733474731443,\n",
       " 1.284760543823242,\n",
       " 1.2797024253845215,\n",
       " 1.2611744354248045,\n",
       " 1.2930214164733882,\n",
       " 1.2809659744262691,\n",
       " 1.278524412536621,\n",
       " 1.282561540985108,\n",
       " 1.260262957763672,\n",
       " 1.271319525146484,\n",
       " 1.2539123840332025,\n",
       " 1.2907460754394533,\n",
       " 1.2880676818847656,\n",
       " 1.2816502212524414,\n",
       " 1.2442051132202145,\n",
       " 1.284868099975586,\n",
       " 1.2888238121032713,\n",
       " 1.2781865310668943,\n",
       " 1.289983785247803,\n",
       " 1.2750442924499512,\n",
       " 1.2823630706787108,\n",
       " 1.2475035362243647,\n",
       " 1.2859732192993163,\n",
       " 1.264112219238282,\n",
       " 1.281405227661133,\n",
       " 1.258596824645996,\n",
       " 1.2668677803039552,\n",
       " 1.2652965896606447,\n",
       " 1.28624411239624,\n",
       " 1.275490909576416,\n",
       " 1.274732036590576,\n",
       " 1.2590665679931645,\n",
       " 1.2512826911926267,\n",
       " 1.2540538299560546,\n",
       " 2.0835361213684087,\n",
       " 1.1812294685363776,\n",
       " 1.188382374572754,\n",
       " 1.1820522834777833,\n",
       " 1.1820668563842773,\n",
       " 1.1711736503601073,\n",
       " 1.220556172180176,\n",
       " 1.176476860046387,\n",
       " 1.1999684326171873,\n",
       " 1.1832411605834958,\n",
       " 1.1831254219055185,\n",
       " 1.1717660690307616,\n",
       " 1.197099583435059,\n",
       " 1.1778556312561033,\n",
       " 1.1823633827209477,\n",
       " 1.200469075775146,\n",
       " 1.1936896728515625,\n",
       " 1.1828041542053223,\n",
       " 1.2309647132873538,\n",
       " 1.2255745697021483,\n",
       " 1.1996223564147945,\n",
       " 1.2321037872314449,\n",
       " 1.1999306068420406,\n",
       " 1.1978201606750487,\n",
       " 1.191574096679687,\n",
       " 1.2008797935485833,\n",
       " 1.1895115776062013,\n",
       " 1.2021353004455573,\n",
       " 1.1981518142700192,\n",
       " 1.191574807739258,\n",
       " 1.190896530151367,\n",
       " 1.1869946243286131,\n",
       " 1.1969074134826656,\n",
       " 1.1999454307556152,\n",
       " 1.21420147857666,\n",
       " 1.2115572944641118,\n",
       " 1.187678902435303,\n",
       " 1.1991542213439943,\n",
       " 1.2003012275695804,\n",
       " 1.2005382797241213,\n",
       " 1.184258499908447,\n",
       " 1.1951727851867673,\n",
       " 1.2154413398742676,\n",
       " 1.2140181427001953,\n",
       " 1.198845664215088,\n",
       " 1.1917147293090822,\n",
       " 1.2035849212646486,\n",
       " 1.2081338912963866,\n",
       " 1.2096867408752443,\n",
       " 1.164442501831055,\n",
       " 1.2273627616882323,\n",
       " 1.2152751846313479,\n",
       " 1.195639514160156,\n",
       " 1.1850292762756343,\n",
       " 1.2253086433410645,\n",
       " 1.2020692787170408,\n",
       " 1.224109420776367,\n",
       " 1.1983307090759279,\n",
       " 1.2014704505920413,\n",
       " 1.2120938720703123,\n",
       " 1.2271141708374023,\n",
       " 1.1966455192565915,\n",
       " 1.1780480590820313]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "self_encoder = make_model(loaded_zh_embeddings, len(zh_ordered_words)).to(device)\n",
    "attn_decoder = PreBatchAttnDecoderRNN(loaded_en_embeddings, emb_size,hidden_size, len(en_ordered_words)).to(device)\n",
    "\n",
    "AttnTrainIters(train_zh_loader, self_encoder, attn_decoder, 5, print_every=100, plot_every=100, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Reference lab8 1-nmt\n",
    "# def greedy_selfattn_evaluate(val_loader, encoder, decoder, en_id2words):\n",
    "#     #Will generate sentences 1 by 1. \n",
    "#     # process input sentence\n",
    "#     decoded_words_all = []\n",
    "#     decoder_attentions_all = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         encoder.eval()\n",
    "#         decoder.eval()\n",
    "        \n",
    "#         for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "#             #if i == 5:\n",
    "#             #    break\n",
    "#             #batch_size, max_len = output_list.size()\n",
    "#             #print(input_list.size())\n",
    "            \n",
    "#             # encode the source lanugage\n",
    "#             encoder_outputs = encoder(input_list)\n",
    "\n",
    "#             decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS\n",
    "#             decoder_hidden = torch.zeros(attn_decoder.n_layers, curr_batch, attn_decoder.hidden_size)\n",
    "\n",
    "#             # decode the context vector\n",
    "            \n",
    "#             # output of this function\n",
    "#             decoded_words = []\n",
    "#             decoder_attentions = torch.zeros(100, 100)\n",
    "\n",
    "#             for di in range(MAX_LENGTH):\n",
    "#                 # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "#                 decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                     decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "#                 top_score, topi = decoder_output.data.topk(1)\n",
    "#                 decoder_attentions[di, :decoder_attention.size(-1)] = decoder_attention\n",
    "#                 decoded_words.append(en_id2words[topi.item()])\n",
    "#                 if topi.item() == EOS_TOKEN:\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     decoder_input = topi.squeeze().detach()\n",
    "                    \n",
    "#             decoded_words_all.append(decoded_words)\n",
    "#             decoder_attentions_all.append(decoder_attentions[:di+1])\n",
    "\n",
    "#         return decoded_words_all, decoder_attentions_all\n",
    "\n",
    "#Reference lab8 1-nmt\n",
    "def greedy_attn_evaluate(val_loader, encoder, decoder, en_id2words ):\n",
    "    #Will generate sentences 1 by 1. \n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    decoded_words_all = []\n",
    "    decoder_attentions_all = []\n",
    "    \n",
    "#     batch_size, max_input_length = input_list.size()\n",
    "#     max_output_length = output_list.size(1)\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     encoder_outputs = self_encoder(input_list)\n",
    "\n",
    "#     #Initialize for decoding process\n",
    "# #     curr_batch = input_list.size(0)#Take the current batch size\n",
    "#     decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "#     #decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "#     decoder_hidden = torch.zeros(1, batch_size, attn_decoder.hidden_size).to(device)\n",
    "#     decoder_outputs = torch.zeros(max_output_length, batch_size, attn_decoder.output_size).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "            if i %100 == 0:\n",
    "                print(\"%d/%d\"%(i,len(val_loader)))\n",
    "                \n",
    "            batch_size, max_input_length = input_list.size()\n",
    "            max_output_length = output_list.size(1)\n",
    "            \n",
    "            #    break\n",
    "            #batch_size, max_len = output_list.size()\n",
    "#             print(input_list.size())\n",
    "            \n",
    "            encoder_outputs = self_encoder(input_list)\n",
    "\n",
    "            decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "#             decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS\n",
    "            # decode the context vector\n",
    "            decoder_hidden = torch.zeros(1, batch_size, attn_decoder.hidden_size).to(device)\n",
    "#             decoder_hidden = encoder_hidden[:decoder.n_layers] # decoder starts from the last encoding sentence\n",
    "            # output of this function\n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input.reshape(1, batch_size), decoder_hidden, encoder_outputs)\n",
    "\n",
    "                top_score, topi = decoder_output.data.topk(1)\n",
    "                decoder_attentions[di, :decoder_attention.size(-1)] = decoder_attention\n",
    "                decoded_words.append(en_id2words[topi.item()])\n",
    "                if topi.item() == EOS_TOKEN:\n",
    "                    break\n",
    "                else:\n",
    "                    decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "                    \n",
    "            decoded_words_all.append(decoded_words)\n",
    "            decoder_attentions_all.append(decoder_attentions[:di+1])\n",
    "\n",
    "        return decoded_words_all, decoder_attentions_all\n",
    "\n",
    "def post_process(decoded_words_all):\n",
    "    cleaned_decoded_words_all = []\n",
    "    \n",
    "    for sentence in decoded_words_all:\n",
    "        cleaned_sentence = []\n",
    "        for word in sentence:\n",
    "            if word == '<PAD>':\n",
    "                continue\n",
    "            else:\n",
    "                cleaned_sentence.append(word)\n",
    "        if cleaned_sentence[-1] != '<EOS>':\n",
    "            cleaned_sentence.append(' <EOS>')\n",
    "            \n",
    "        cleaned_decoded_words_all.append(cleaned_sentence)\n",
    "        \n",
    "    return cleaned_decoded_words_all\n",
    "\n",
    "#Translate the test and val lists back to english\n",
    "def en_translate(index_list, en_id2words):\n",
    "    translated_sentence_list = []\n",
    "    for sentence in index_list:\n",
    "        translated_sentence = []\n",
    "        for index in sentence:\n",
    "            translated_sentence.append(en_id2words[index])\n",
    "        #translated_sentence.append('<EOS>')\n",
    "        translated_sentence_list.append(translated_sentence)\n",
    "    return translated_sentence_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'waking',\n",
       " 'up',\n",
       " 'one',\n",
       " 'morning',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'of',\n",
       " 'joy',\n",
       " 'in',\n",
       " 'my',\n",
       " 'house',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_en_val_list = [pair[1] for pair in zh_val_pairs_cleaned]\n",
    "translated_sentence_list = en_translate(zh_en_val_list, en_id2words)\n",
    "translated_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1200\n",
      "100/1200\n",
      "200/1200\n",
      "300/1200\n",
      "400/1200\n",
      "500/1200\n",
      "600/1200\n",
      "700/1200\n",
      "800/1200\n",
      "900/1200\n",
      "1000/1200\n",
      "1100/1200\n"
     ]
    }
   ],
   "source": [
    "decoded_val, decoder_attentions = greedy_attn_evaluate(val_zh_loader, self_encoder, attn_decoder, en_id2words)\n",
    "decoded_clean = post_process(decoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu,raw_corpus_bleu\n",
    "\n",
    "#Translate the test and val lists back to english\n",
    "def en_translate(index_list, en_id2words):\n",
    "    translated_sentence_list = []\n",
    "    for sentence in index_list:\n",
    "        translated_sentence = []\n",
    "        for index in sentence:\n",
    "            translated_sentence.append(en_id2words[index])\n",
    "        #translated_sentence.append('<EOS>')\n",
    "        translated_sentence_list.append(translated_sentence)\n",
    "    return translated_sentence_list\n",
    "\n",
    "def concatenate_tokens(token_lists):\n",
    "    sentence_list = []\n",
    "    for token_list in token_lists:\n",
    "        sentence = ''\n",
    "        for token in token_list:\n",
    "            sentence = sentence+' '+token\n",
    "        sentence_list.append(sentence)\n",
    "    return sentence_list\n",
    "        \n",
    "from sacrebleu import corpus_bleu, raw_corpus_bleu\n",
    "def bleu_score(pred_list, target_list):\n",
    "    pred_sentence_list = concatenate_tokens(pred_list)\n",
    "    target_sentence_list = concatenate_tokens(target_list)\n",
    "    print('bleu score for dataset:', corpus_bleu(pred_sentence_list, [target_sentence_list]).score)\n",
    "    print('bleu score for dataset [raw]:', raw_corpus_bleu(pred_sentence_list, [target_sentence_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1371\n",
      "100/1371\n",
      "200/1371\n",
      "300/1371\n",
      "400/1371\n",
      "500/1371\n",
      "600/1371\n",
      "700/1371\n",
      "800/1371\n",
      "900/1371\n",
      "1000/1371\n",
      "1100/1371\n",
      "1200/1371\n",
      "1300/1371\n"
     ]
    }
   ],
   "source": [
    "zh_en_test_list = [pair[1] for pair in zh_test_pairs_cleaned]\n",
    "translated_sentence_list = en_translate(zh_en_test_list, en_id2words)\n",
    "translated_sentence_list[0]\n",
    "\n",
    "decoded_test, decoder_attentions = greedy_attn_evaluate(test_zh_loader, self_encoder, attn_decoder, en_id2words)\n",
    "decoded_clean = post_process(decoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
