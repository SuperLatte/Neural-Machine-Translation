{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from queue import PriorityQueue\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'PAD', 1: 'SOS', 2:'EOS', 3:'UNK'}#Dict\n",
    "        self.n_words = 4  # Count SOS and EOS +(batch: pad and unk)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #This line is commented out since it will not properly deal with Chinese Letters\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# def readLangs(lang1, lang2, reverse=False):\n",
    "#     print(\"Reading lines...\")\n",
    "\n",
    "#     # Read the file and split into lines\n",
    "#     lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "#         read().strip().split('\\n')\n",
    "\n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "#     # Reverse pairs, make Lang instances\n",
    "#     if reverse:\n",
    "#         pairs = [list(reversed(p)) for p in pairs]\n",
    "#         input_lang = Lang(lang2)\n",
    "#         output_lang = Lang(lang1)\n",
    "#     else:\n",
    "#         input_lang = Lang(lang1)\n",
    "#         output_lang = Lang(lang2)\n",
    "\n",
    "#     return input_lang, output_lang, pairs\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess for Project Data\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, category, reverse = False):#category = ['train', 'dev','test]\n",
    "    print('Reading lines:')\n",
    "    lines1 = open('data/iwslt-' + lang1 +'-en/' + category +'.tok.'+ lang1, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data1 = [normalizeString(l) for l in lines1]\n",
    "    \n",
    "    lines2 = open('data/iwslt-' + lang1 +'-en/' + category + '.tok.' + lang2, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data2 = [normalizeString(l) for l in lines2]\n",
    "    #Given that data2 is english hence we further normalize\n",
    "    data2 = [re.sub(r\"[^a-zA-Z.!?]+\", r\" \", data) for data in data2]\n",
    "    print('Generating pairs')\n",
    "    pairs = [[data1[i], data2[i]] for i in range(len(data1))]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 213376 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 88426\n",
      "en 50970\n",
      "['盖亚 的 整个 假说 就是   生命 为 其 本身 使 世界 更 美好     周五 五下 下午 在 洛杉矶 高速 高速公路 公路 路上 开过 车 的 任何 任何人 何人   都 相信 盖亚 假说 吗   不是', 'this whole gaia idea that life makes the world better for itself anybody been on a freeway on a friday afternoon in los angeles believing in the gaia theory ? no .']\n"
     ]
    }
   ],
   "source": [
    "#Data Preparation for CHN to ENG\n",
    "def prepareData(lang1, lang2, category, reverse = False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, category, reverse)\n",
    "    print('Read %s sentence pairs' % len(pairs))\n",
    "    #Build the vocabulary\n",
    "    print('Counting words')\n",
    "    #max_length = 0\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        #Get the maximum legnth \n",
    "        #pair_max = max(len(list(filter(None, pair[0].split(' ')))),\n",
    "          #             len(list(filter(None, pair[1].split(' ')))))\n",
    "        #max_length = max(pair_max, max_length)\n",
    "        \n",
    "    #Test for basic info about the data\n",
    "    print('Counted Words')\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, train_pairs = prepareData('zh', 'en', 'train')\n",
    "print(random.choice(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 213376 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 88426\n",
      "en 50970\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 1261 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 6133\n",
      "en 3671\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 1397 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 5214\n",
      "en 3220\n"
     ]
    }
   ],
   "source": [
    "input_zh, output_zh_en, train_zh_pairs = prepareData('zh', 'en', 'train')\n",
    "_,_, val_zh_pairs = prepareData('zh', 'en', 'dev')\n",
    "_,_, test_zh_pairs = prepareData('zh','en','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 133317 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 30768\n",
      "en 41272\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 1268 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 3050\n",
      "en 3572\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "Read 1553 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 2899\n",
      "en 3408\n"
     ]
    }
   ],
   "source": [
    "input_vi, output_vi_en, train_vi_pairs = prepareData('vi', 'en', 'train')\n",
    "_,_, val_vi_pairs = prepareData('vi', 'en', 'dev')\n",
    "_,_, test_vi_pairs = prepareData('vi','en','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vi_vay chung_ta nen dung ngay su u_me nay lai , dung su tho_o , dung su ky_thi che_nhao , va dung su im_lang nay , va pha_bo nhung đieu cam ky , nhin thang vao su_that , va bat_đau tro_chuyen , boi_vi cach duy_nhat đe đanh_bai mot van_đe ma ca_nhan moi nguoi phai tu minh chien_đau đo la cung manh_me vung_vang đung_lai gan nhau , cung manh_me vung_vang đung_lai gan nhau  .',\n",
       " 'so we need to stop the ignorance stop the intolerance stop the stigma and stop the silence and we need to take away the taboos take a look at the truth and start talking because the only way we apos re going to beat a problem that people are battling alone is by standing strong together by standing strong together .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vi_pairs[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['作为 领导 领导人   我们 不可 可能 总是 对 的', 'we apos re not always right as leaders .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_zh_pairs[419]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data Preparation for Eng to FRA\n",
    "# def prepareData(lang1, lang2, reverse=False):\n",
    "#     input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "#     print(\"Read %s sentence pairs\" % len(pairs))\n",
    "#     pairs = filterPairs(pairs)\n",
    "#     print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "#     print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         input_lang.addSentence(pair[0])\n",
    "#         output_lang.addSentence(pair[1])\n",
    "#     print(\"Counted words:\")\n",
    "#     print(input_lang.name, input_lang.n_words)\n",
    "#     print(output_lang.name, output_lang.n_words)\n",
    "#     return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "# input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "# print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs, input_lang, output_lang):#Needs the index pairs\n",
    "        self.pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.input_seqs = [pairs[i][0] for i in range(len(self.pairs))]\n",
    "        self.output_seqs = [pairs[i][1] for i in range(len(self.pairs))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)#Returning number of pairs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.input_seqs[index]\n",
    "        output_seq = self.output_seqs[index]\n",
    "        return [input_seq, len(input_seq), output_seq, len(output_seq)]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    #Reference: lab8_3_mri\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "        padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    batch_input_seqs = [datum[0] for datum in batch]\n",
    "    batch_output_seqs = [datum[2] for datum in batch]\n",
    "    #batch_input_length = [datum[1] for datum in batch]\n",
    "    #batch_output_length = [datum[3] for datum in batch]\n",
    "\n",
    "    sorted_pairs = sorted(zip(batch_input_seqs, batch_output_seqs), key=lambda x: len(x[0]), reverse = True)\n",
    "    in_seq_sorted, out_seq_sorted = zip(*sorted_pairs)\n",
    "    \n",
    "    padded_input,input_lens = _pad_sequences(in_seq_sorted)\n",
    "    padded_output,output_lens = _pad_sequences(out_seq_sorted)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(padded_input)).to(device),\n",
    "            torch.LongTensor(input_lens).to(device),\n",
    "            torch.from_numpy(np.array(padded_output)).to(device),\n",
    "            torch.LongTensor(output_lens).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_zh_pairs = [[indexesFromSentence(input_zh, train_zh_pairs[i][0]),\n",
    "                indexesFromSentence(output_zh_en, train_zh_pairs[i][1])] for i in range(len(train_zh_pairs))]\n",
    "#index_pairs = indexesFromPair(pair)\n",
    "\n",
    "'''\n",
    "NMTDataset needs index pairs, need to call indexesFromPairs functions beforehand\n",
    "The dataLoader is sorted according to length of the input_length, and padded to\n",
    "max length of input and output list repectively\n",
    "TODO: output_list is not sorted, hence need to sort (maybe) in the rnn sequence.\n",
    "'''\n",
    "train_dataset = NMTDataset(index_zh_pairs, input_lang, output_lang)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "#Input_batch in size Batch x maxLen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for batch sizes\n",
    "for i, (input_list, input_length, output_list, output_length) in enumerate(train_loader):\n",
    "    if i== 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27, 27, 10], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder w/ Attn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 256\n",
    "max_length = 10\n",
    "BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, n_layers=1, dropout=0.1):\n",
    "        super(BatchEncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional = True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        '''\n",
    "        input_seqs in size B x L sorted in decreasing order -> will transpose to fit in embedding dimension\n",
    "        '''\n",
    "        embedded = self.embedding(input_seqs.transpose(0,1))#input_seqs B x L -> transpose to L x B\n",
    "        #Input length sorted by loader\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        #Outputs in shape L x B x 2H, hidden as the last state of the GRU\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        #hidden = hidden[:self.n_layers, :, :] + hidden[self.n_layers:,:,:]#Sum bidrectional information\n",
    "        #outputs L x B x H\n",
    "        #hidden size (2*n_layers) x B x H\n",
    "\n",
    "        #outputs: seq_len x Batch x H\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers *2, self.batch_size, self.hidden_size,device = device)#hidden size 2lays *B*H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256]), torch.Size([27, 3, 256]), torch.Size([4, 3, 256]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_encoder = BatchEncoderRNN(input_lang.n_words, hidden_size, BATCH_SIZE, n_layers = 2, dropout = 0.1).to(device)\n",
    "init_hidden = batch_encoder.initHidden()\n",
    "encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, init_hidden)\n",
    "#inithidden bidirectional, encoder_hidden, summing up both directions\n",
    "init_hidden.size(), encoder_outputs.size(), encoder_hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Decoder w/o attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN decoder with no attention used, batch implemented\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, batch_size, n_layers=1, dropout_p = 0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(output_size, emb_size,padding_idx = PAD_TOKEN)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, self.batch_size, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output = F.relu(embedded)\n",
    "        output, hidden = self.gru(output, hidden)#output 1 x B x E, hidden n_layers x B x H\n",
    "        out = self.out(output[0])\n",
    "        out = self.softmax(out)\n",
    "        #out size batch x output_lang vocab size\n",
    "        #hidden n_layers x B x H\n",
    "        return out, hidden\n",
    "    \n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: lab8 3_mri, Luong model\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            #self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Create variable to store attention energies\n",
    "        # For each batch of encoder outputs Calculate energy for each encoder output\n",
    "        beta = self.score(hidden, encoder_outputs)#Len x Batch\n",
    "        #print('The size for beta is {}'.format(beta.size()))\n",
    "        alpha = F.softmax(beta.transpose(0,1)).unsqueeze(1)#B x 1 x L\n",
    "        #encoder_ouputs L x B x H originally -> transpose to B x L x H\n",
    "        #alpha bmm encoder_outputs: B x 1 x L bmm B x L x H -> B x 1 x N\n",
    "        context = alpha.bmm(encoder_outputs.transpose(0,1))#B X 1 X N        \n",
    "        \n",
    "        # Return context vectors and attnetion weights for visualization\n",
    "        return context, alpha\n",
    "    \n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            ## TODO implement\n",
    "            energy = torch.sum(hidden*encoder_outputs, 2)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_outputs)\n",
    "            ## TODO implement \n",
    "            energy = torch.sum(hidden*energy,2)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            cat = torch.cat((hidden.repeat(encoder_outputs.size(0), 1,1),encoder_outputs),dim=2)\n",
    "            energy = F.tanh(self.attn(cat))\n",
    "            ## TODO implement \n",
    "            energy = torch.sum(self.v * energy, 2)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference Lab8 3-mri, Luong model\n",
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x H\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context, alpha = self.attn(rnn_output, encoder_outputs)\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "        #Softmax to get the distribution\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([2, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "attn_model = 'dot'\n",
    "decoder_test = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers=2, dropout=0.1).to(device)\n",
    "decoder_input = torch.tensor([[SOS_TOKEN]], device=device).repeat(TEST_BATCH_SIZE,1)\n",
    "decoder_hidden = encoder_hidden[:decoder_test.n_layers]\n",
    "#decoder_hidden = encoder_hidden\n",
    "input_seq = decoder_input\n",
    "last_hidden = decoder_hidden\n",
    "# attn_model = 'dot'\n",
    "print(decoder_input.size(), decoder_hidden.size())\n",
    "# decoder_test = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers=2, dropout=0.1).to(device)\n",
    "decoder_output, decoder_hidden, attn_weights = decoder_test(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "#Test rounds\n",
    "all_decoder_outputs = torch.zeros(output_length.max().item(), TEST_BATCH_SIZE, decoder_test.output_size)\n",
    "for t in range(output_length.max().item()):\n",
    "    decoder_output, decoder_hidden, decoder_attn = decoder_test(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    all_decoder_outputs[t] = decoder_output\n",
    "    decoder_input = output_list[:,t].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Encoder and Decoder session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_forcing_ratio = 0.5\n",
    "# MAX_LENGTH = 10\n",
    "# def batch_train(input_list, input_length, output_list,output_length, batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "#     batch_encoder.train()\n",
    "#     batch_decoder.train()\n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "#     max_output_length = output_length.max().item()\n",
    "\n",
    "\n",
    "# #     input_length = input_tensor.size(0)\n",
    "# #     target_length = target_tensor.size(0)\n",
    "\n",
    "# #     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "#     loss = 0\n",
    "\n",
    "#     #batch_encoder = BatchEncoderRNN(input_lang.n_words, hidden_size, BATCH_SIZE, n_layers = 2, dropout = 0.1)\n",
    "#     init_hidden = batch_encoder.initHidden()\n",
    "#     encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, init_hidden)\n",
    "\n",
    "#     #Initialize for decoding process\n",
    "#     curr_batch = input_list.size(0)#Take the current batch size\n",
    "#     decoder_input = torch.tensor([[SOS_TOKEN]], device=device).repeat(curr_batch,1)\n",
    "#     decoder_hidden = encoder_hidden#Bidirectional summoned\n",
    "#     #encoder_outputs : L x B x H\n",
    "#     decoder_outputs = torch.zeros(max_output_length, curr_batch, batch_decoder.output_size)\n",
    "\n",
    "#     #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "#     use_teacher_forcing = True\n",
    "#     if use_teacher_forcing:\n",
    "#         # Teacher forcing: Feed the target as the next input\n",
    "#         for di in range(max_output_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_outputs[di] = decoder_output\n",
    "#             loss += criterion(decoder_output, output_list[:,di])\n",
    "#             decoder_input = output_list[:,di].unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "#     else:\n",
    "#         # Without teacher forcing: use its own predictions as the next input\n",
    "#         for di in range(max_output_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.detach()# detach from history as input: size batch x 1 \n",
    "\n",
    "#             loss += criterion(decoder_output, output_list[:, di])\n",
    "#             if ((decoder_output == EOS_TOKEN).sum().item()) == decoder_output.size(0):#If all are EOS tokens\n",
    "#                 break;\n",
    "            \n",
    "\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     encoder_optimizer.step()\n",
    "#     decoder_optimizer.step()\n",
    "\n",
    "#     return loss.item() / max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1.0\n",
    "MAX_LENGTH = 10\n",
    "def batch_train(input_list, input_length, output_list,output_length, batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    batch_encoder.train()\n",
    "    batch_decoder.train()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    max_output_length = output_length.max().item()\n",
    "    \n",
    "\n",
    "\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    #batch_encoder = BatchEncoderRNN(input_lang.n_words, hidden_size, BATCH_SIZE, n_layers = 2, dropout = 0.1)\n",
    "    init_hidden = batch_encoder.initHidden()\n",
    "    encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, init_hidden)\n",
    "\n",
    "    #Initialize for decoding process\n",
    "    curr_batch = input_list.size(0)#Take the current batch size\n",
    "    decoder_input = torch.tensor([[SOS_TOKEN]], device=device).repeat(curr_batch,1)\n",
    "    decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "    #encoder_outputs : L x B x H\n",
    "    decoder_outputs = torch.zeros(max_output_length, curr_batch, batch_decoder.output_size)\n",
    "\n",
    "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    use_teacher_forcing = True\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs[di] = decoder_output\n",
    "            loss += criterion(decoder_output, output_list[:,di])\n",
    "            decoder_input = output_list[:,di].unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs[di] = decoder_input\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach()# detach from history as input: size batch x 1 \n",
    "\n",
    "            loss += criterion(decoder_output, output_list[:, di])\n",
    "            if ((decoder_output == EOS_TOKEN).sum().item()) == decoder_output.size(0):#If all are EOS tokens\n",
    "                break;\n",
    "            \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_encoder_test = BatchEncoderRNN(input_lang.n_words, hidden_size, TEST_BATCH_SIZE, n_layers = 2, dropout = 0.1).to(device)\n",
    "init_hidden = batch_encoder.initHidden()\n",
    "# encoder_outputs, encoder_hidden = batch_encoder_test(input_list, input_length, init_hidden)\n",
    "attn_model = 'dot'\n",
    "batch_decoder_test = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers=2, dropout=0.1).to(device)\n",
    "\n",
    "learning_rate = 0.1\n",
    "encoder_optimizer = optim.SGD(batch_encoder_test.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(batch_decoder_test.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.838990529378256\n"
     ]
    }
   ],
   "source": [
    "loss= batch_train(input_list, input_length, output_list, output_length, \n",
    "            batch_encoder_test, batch_decoder_test, encoder_optimizer, \n",
    "           decoder_optimizer, criterion)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.83899180094401\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.83899180094401\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.838990529378256\n",
      "10.83899180094401\n",
      "10.838990529378256\n",
      "10.83899180094401\n",
      "10.838990529378256\n",
      "10.838990529378256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-04a5155f51eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     loss = batch_train(input_list, input_length, output_list, output_length, \n\u001b[1;32m     24\u001b[0m             \u001b[0mbatch_encoder_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_decoder_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m            decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-3fe222c0c66b>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(input_list, input_length, output_list, output_length, batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Breif test on tiny batch to see whether it is working.\n",
    "#Teaching forcing  always\n",
    "batch_encoder_test = BatchEncoderRNN(input_lang.n_words, hidden_size, TEST_BATCH_SIZE, n_layers = 2, dropout = 0.1).to(device)\n",
    "init_hidden = batch_encoder.initHidden()\n",
    "# encoder_outputs, encoder_hidden = batch_encoder_test(input_list, input_length, init_hidden)\n",
    "attn_model = 'general'\n",
    "batch_decoder_test = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers=2, dropout=0.1).to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(batch_encoder_test.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(batch_decoder_test.parameters(), lr=learning_rate)\n",
    "#criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "# for i, (input_list, input_length, output_list, output_length) in enumerate(train_loader):\n",
    "#     loss = batch_train(input_list, input_length, output_list, output_length, \n",
    "#             batch_encoder_test, batch_decoder_test, encoder_optimizer, \n",
    "#            decoder_optimizer, criterion)\n",
    "#     train_loss_list.append(loss)\n",
    "#     print(loss)\n",
    "for i in range(1000):\n",
    "    loss = batch_train(input_list, input_length, output_list, output_length, \n",
    "            batch_encoder_test, batch_decoder_test, encoder_optimizer, \n",
    "           decoder_optimizer, criterion)\n",
    "    train_loss_list.append(loss)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ab2b0ca01d0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,10))\n",
    "ax.plot(train_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.getcwd()\n",
    "\n",
    "#Reference: lab8 3_mri\n",
    "def save_checkpoint(encoder, decoder, checkpoint_dir):\n",
    "    enc_filename = \"{}/enc-{}.pth\".format(checkpoint_dir, time.strftime(\"%d%m%y-%H%M%S\"))\n",
    "    dec_filename = \"{}/dec-{}.pth\".format(checkpoint_dir, time.strftime(\"%d%m%y-%H%M%S\"))\n",
    "    torch.save(encoder.state_dict(), enc_filename)\n",
    "    torch.save(decoder.state_dict(), dec_filename)\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "def train(train_dataset, batch_size, n_epochs, batch_encoder, batch_decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, \n",
    "          checkpoint_dir=None, save_every=500):\n",
    "    '''\n",
    "    Call batch_train  inside trian\n",
    "    '''\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              collate_fn = vocab_collate_func,\n",
    "                              shuffle = True)\n",
    "    for i in range(n_epochs):\n",
    "        tick = time.clock()\n",
    "        print(\"Epoch {}/{}\".format(i+1, n_epochs))\n",
    "        losses = []\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_list, input_length, output_list, output_length = batch\n",
    "            loss = batch_train(input_list, input_length, output_list, output_length, \n",
    "                                batch_encoder, batch_decoder, encoder_optimizer, \n",
    "                              decoder_optimizer, criterion)\n",
    "            losses.append(loss)          \n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\"batch: {}, loss: {}\".format(batch_idx, loss))\n",
    "            if checkpoint_dir:\n",
    "                if batch_idx % save_every == 0:\n",
    "                    save_checkpoint(batch_encoder, batch_decoder, checkpoint_dir)\n",
    "        \n",
    "        tock = time.clock()\n",
    "        print(\"Time: {} Avg loss: {}\".format(tock-tick, np.mean(losses)))\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        save_checkpoint(batch_encoder, batch_decoder, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceFromTensor(tensor, lang):\n",
    "    seq_idx = tensor.data.cpu().numpy()#A single sentence \n",
    "    seq_word = [lang.index2word[seq_idx[i]] for i in range(len(seq_idx))]\n",
    "    return seq_word\n",
    "    \n",
    "def batch_translate(decoded_batch, lang):\n",
    "    translated_batch = []\n",
    "    for i in range(decoded_batch.shape[0]):\n",
    "        translated_batch.append(sentenceFromTensor(decoded_batch[i], lang))\n",
    "    return translated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_evaluate(input_list, input_length, encoder, decoder, lang, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         input_tensor = tensorFromSentence(input_lang, input_list)\n",
    "#         input_length = input_tensor.size()[0]\n",
    "        # encode the source lanugage\n",
    "        max_input_length = input_length.max().item()\n",
    "        encoder_outputs = torch.zeros(batch_size, max_length, max_input_length, device=device)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_list, input_length, None)\n",
    "        \n",
    "        decoder_input = torch.tensor([SOS_TOKEN]*batch_size, device=device)  # SOS\n",
    "        # decode the context vector\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers] # decoder starts from the last encoding sentence\n",
    "        # output of this function\n",
    "        decoded_batch = torch.zeros((batch_size, max_length))#batch x sentence length\n",
    "        decoder_attentions = torch.zeros(max_length, batch_size, max_input_length)#in length, each position has attention\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #decoder_attention: B x L\n",
    "            decoder_attentions[di] = decoder_attention.squeeze(1)\n",
    "            topv, topi = decoder_output.data.topk(1)  # get candidates\n",
    "            topi = topi.view(-1)\n",
    "            decoded_batch[:, di] = topi#Still indexes\n",
    "\n",
    "            decoder_input = topi.detach()\n",
    "\n",
    "        #return decoded_words, decoder_attentions[:di + 1]\n",
    "        decoded_words = batch_translate(decoded_batch, lang)\n",
    "        return decoded_words, decoded_batch, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_score(val_loader, encoder, decoder, max_length, search_method = 'greedy', target_lang='zh'):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    predict_file = 'predict_temp'\n",
    "    predict_lines = open(predict_file, 'w')\n",
    "    for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "        loss = batch_train(\n",
    "            input_list, input_length, output_list, output_length,\n",
    "            encoder, decoder,\n",
    "            encoder_optimizer, decoder_optimizer, criterion\n",
    "        )\n",
    "        if search_method == 'greedy':\n",
    "            decoded_words, decoded_list, decoder_attention = greedy_evaluate(input_list, input_length,\n",
    "                                                                            encoder, decoder, target_lang,\n",
    "                                                                            max_length = max_length)\n",
    "        else:\n",
    "            decoded_words, decoded_list, decoder_attention = beam_evaluate(input_list)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total += 1\n",
    "        predict_lines.write(''.join(decoded_words) + '\\n')\n",
    "    predict_lines.close()\n",
    "        \n",
    "    if target_lang == 'zh':\n",
    "        target_file = 'iwslt-zh-en-processed/test.tok.en'\n",
    "    else:\n",
    "        target_file = 'iwslt-vi-en-processed/test.tok.en'\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file,target_file),shell=True,stdout=subprocess.PIPE)\n",
    "    score = get_blue_score(str(result))\n",
    "        \n",
    "    return (total_loss / total), score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SearchNode(object):\n",
    "    def __init__(self, word_idx, hidden, prev, curr_score, length):\n",
    "        self.hidden = hidden\n",
    "        self.word_idx = word_idx\n",
    "        self.prev = prev\n",
    "        \n",
    "        if self.prev == None:\n",
    "            self.score = curr_score\n",
    "        else:\n",
    "            self.score = self.prev.score + curr_score\n",
    "        self.length = length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beam_search(decoder_hiddens, encoder_outputs, batch_decoder, batch_size, beam_width, attn_flag, k):\n",
    "    '''\n",
    "    beam_width: number of best nodes kept at each iterations\n",
    "    k: number of sentences we want to keep\n",
    "    Returns the translated batch\n",
    "    '''\n",
    "    decoded_batch = []\n",
    "    #decoder_attentions = torch.zeros(max_length, batch_size, max_input_length)#in length, each position has attention\n",
    "\n",
    "    \n",
    "    for idx in range(batch_size):\n",
    "        #Take the certain batch hidden layer\n",
    "        decoder_hidden = decoder_hiddens[:, idx, :].unsqueeze(1)#1 x layers x H\n",
    "        encoder_output = encoder_outputs[:, idx, :].unsqueeze(1)#L x 1 x H\n",
    "        \n",
    "        decoder_input = torch.LongTensor([SOS_TOKEN], device = device)\n",
    "        start_node = SearchNode(decoder_input, decoder_hidden, prev = None, curr_score = 0, length = 1)\n",
    "        nodes = PriorityQueue()\n",
    "        nodes.put(( -(start_node.score), start_node))\n",
    "        end_nodes = []\n",
    "        \n",
    "        while(len(end_nodes) < k):\n",
    "            curr_score, curr_node = nodes.get()\n",
    "\n",
    "            if (curr_node.word_idx == EOS_TOKEN) and (curr_node.prev != None):\n",
    "                endnodes.append((-(curr_node.score),curr_node))\n",
    "                if len(end_nodes) >=k:\n",
    "                    break;\n",
    "                else: \n",
    "                    continue;\n",
    "                    \n",
    "            if nodes.qsize() >= 2000:#if too long will force to stop\n",
    "                #Create an EOS dummy node to trace back the entire sentence\n",
    "                EOS_node = SearchNode(torch.tensor([[EOS_TOKEN]], device = device), curr_node.hidden, \n",
    "                                      curr_node, curr_node.score, (curr_node.length)+1)\n",
    "                end_nodes.append((-(EOS_node.score), EOS_node))\n",
    "                if len(end_nodes) >= k:\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "                    \n",
    "            decoder_input = curr_node.word_idx\n",
    "            decoder_hidden = curr_node.hidden\n",
    "            \n",
    "            if attn_flag :\n",
    "                decoder_output, decoder_hidden, decoder_attention = batch_decoder(decoder_input, decoder_hidden, \n",
    "                                                                            encoder_output)\n",
    "            else:\n",
    "                decoder_output, decoder_hidden = batch_decoder(deocder_input, decoder_hidden,encoder_output)\n",
    "                \n",
    "            scores, indexes = torch.topk(decoder_output, beam_width)\n",
    "            candidate_nodes = []\n",
    "            for i in range(beam_width):\n",
    "                candidate_idx = indexes[0][i].view(1, -1)\n",
    "                candidate_score = scores[0][i].item()\n",
    "                \n",
    "                candidate_node = SearchNode(candidate_idx, decoder_hidden, \n",
    "                                               curr_node, candidate_score, curr_node.length + 1)\n",
    "                candidate_nodes.append((-(candidate_node.score), candidate_node))\n",
    "            \n",
    "            for j in range(beam_width):\n",
    "                to_push_score, to_push_node = candidate_nodes[j]\n",
    "                #print(to_push_score)\n",
    "                nodes.put((-(to_push_score), to_push_node))\n",
    "            #End of the while loop\n",
    "            \n",
    "        sentences = translate_end_nodes(end_nodes)\n",
    "        decoded_batch.append(sentences)\n",
    "        #End of the batch loop\n",
    "        \n",
    "    return decoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Translate the sentences.\n",
    "def translate_end_nodes(end_nodes):\n",
    "    sentences = []\n",
    "    end_nodes = sorted(end_nodes, key = operator.itemgetter(0),reverse = True)\n",
    "    for _, end_node in end_nodes:\n",
    "        sentence = []\n",
    "        while end_node.prev != None:\n",
    "            sentence.append(output_lang.index2word[end_node.word_idx.item()])\n",
    "            end_node = end_node.prev\n",
    "        sentence = sentence[::-1]#Reverse the sentence to get the sentence\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256]) torch.Size([48, 3, 256]) torch.Size([4, 3, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['rounds', 'possession', 'fraudsters', 'conditionally', 'EOS']],\n",
       " [['strangled', 'ramona', 'knowing', 'cow', 'EOS']],\n",
       " [['unscrew', 'whispers', 'causes', 'cavendish', 'EOS']]]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_encoder = BatchEncoderRNN(input_lang.n_words, hidden_size, BATCH_SIZE, n_layers = 2, dropout = 0.1)\n",
    "init_hidden = batch_encoder.initHidden()\n",
    "encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, init_hidden)\n",
    "#inithidden bidirectional, encoder_hidden, summing up both directions\n",
    "print(init_hidden.size(), encoder_outputs.size(), encoder_hidden.size())\n",
    "\n",
    "decoded_batches = beam_search(decoder_hidden, encoder_outputs, batch_decoder_test, TEST_BATCH_SIZE, 5, True, 1)\n",
    "decoded_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
