{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from queue import PriorityQueue\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from sacrebleu import raw_corpus_bleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define constants here\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 3\n",
    "words_to_load = 50000\n",
    "emb_size = 300\n",
    "wiki_size = 300\n",
    "CUDA = True\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = os.getcwd()\n",
    "datadir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained word embeddings:\n",
    "Reference: https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "\n",
    "@article{bojanowski2017enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={Transactions of the Association for Computational Linguistics},\n",
    "  volume={5},\n",
    "  year={2017},\n",
    "  issn={2307-387X},\n",
    "  pages={135--146}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "datadir = os.getcwd()\n",
    "words_to_load = 5000\n",
    "with open(datadir + '/data/wiki-news-300d-1M.vec') as f:\n",
    "    loaded_en_embeddings = np.zeros(((words_to_load+4), wiki_size))\n",
    "    en_word2id = {}\n",
    "    en_id2words = {}\n",
    "    \n",
    "    en_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    en_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    en_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    en_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    en_word2id['<PAD>'] = PAD_TOKEN\n",
    "    en_word2id['<SOS>'] = SOS_TOKEN\n",
    "    en_word2id['<EOS>'] = EOS_TOKEN\n",
    "    en_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    en_ordered_words= []\n",
    "    en_ordered_words.append('<PAD>')\n",
    "    en_ordered_words.append('<SOS>')\n",
    "    en_ordered_words.append('<EOS>')\n",
    "    en_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        if i ==0:#Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        #print(len(s))\n",
    "        loaded_en_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        en_word2id[s[0]] = i+4 #for extra pad and unk eos and unk\n",
    "        en_id2words[i+4] = s[0]\n",
    "        en_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "#Over 200000 loaded words, only 10 has wrong dimensions, simply discarded\n",
    "words_to_load = 200000\n",
    "datadir = os.getcwd()\n",
    "with open(datadir + '/data/wiki.vi.vec') as f:\n",
    "    loaded_zh_embeddings = np.zeros(((words_to_load+4),wiki_size))\n",
    "    zh_word2id = {}\n",
    "    zh_id2words = {}\n",
    "    \n",
    "    zh_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    zh_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    zh_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    zh_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    zh_word2id['<PAD>'] = PAD_TOKEN\n",
    "    zh_word2id['<SOS>'] = SOS_TOKEN\n",
    "    zh_word2id['<EOS>'] = EOS_TOKEN\n",
    "    zh_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    zh_ordered_words= []\n",
    "    zh_ordered_words.append('<PAD>')\n",
    "    zh_ordered_words.append('<SOS>')\n",
    "    zh_ordered_words.append('<EOS>')\n",
    "    zh_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        #print(i)\n",
    "        if i >= words_to_load:\n",
    "            break;\n",
    "        if i == 0: #Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        if len(s) != 301:\n",
    "            print('Wrong dimension, skip')\n",
    "            continue;\n",
    "        loaded_zh_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        zh_word2id[s[0]] = i+4 #for extra pad and unk \n",
    "        zh_id2words[i+4] = s[0]\n",
    "        zh_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "#Over 200000 loaded words, only 10 has wrong dimensions, simply discarded\n",
    "words_to_load = 200000\n",
    "datadir = os.getcwd()\n",
    "with open(datadir + '/data/wiki.zh.vec') as f:\n",
    "    loaded_zh_embeddings = np.zeros(((words_to_load+4),wiki_size))\n",
    "    zh_word2id = {}\n",
    "    zh_id2words = {}\n",
    "    \n",
    "    zh_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    zh_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    zh_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    zh_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    zh_word2id['<PAD>'] = PAD_TOKEN\n",
    "    zh_word2id['<SOS>'] = SOS_TOKEN\n",
    "    zh_word2id['<EOS>'] = EOS_TOKEN\n",
    "    zh_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    zh_ordered_words= []\n",
    "    zh_ordered_words.append('<PAD>')\n",
    "    zh_ordered_words.append('<SOS>')\n",
    "    zh_ordered_words.append('<EOS>')\n",
    "    zh_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        #print(i)\n",
    "        if i >= words_to_load:\n",
    "            break;\n",
    "        if i == 0: #Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        if len(s) != 301:\n",
    "            print('Wrong dimension, skip')\n",
    "            continue;\n",
    "        loaded_zh_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        zh_word2id[s[0]] = i+4 #for extra pad and unk \n",
    "        zh_id2words[i+4] = s[0]\n",
    "        zh_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 3\n",
    "CUDA = False\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_word2id, emb_id2word, emb_ordered_words):\n",
    "        self.name = name\n",
    "        self.word2index = emb_word2id\n",
    "        self.word2count = {}\n",
    "        self.index2word = emb_id2word #Dict\n",
    "        self.n_words = 4  # Count SOS and EOS +(batch: pad and unk)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2count:\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #This line is commented out since it will not properly deal with Chinese Letters\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "#reference: LAB4 hw2\n",
    "def indexesFromSentences(lang1, lang2, pairs):\n",
    "    id_list1 = []\n",
    "    id_list2 = []\n",
    "    for i in range(len(pairs)):\n",
    "        sentence1 = pairs[i][0]\n",
    "        sentence2 = pairs[i][1]\n",
    "        \n",
    "        sentence1 = sentence1.replace('quot','')\n",
    "        sentence1 = sentence1.replace('apos', '')\n",
    "        sentence2 = sentence2.replace('quot','')\n",
    "        sentence2 = sentence2.replace('apos', '')\n",
    "        #If either sentence is empty, then remove the pair\n",
    "        if sentence1 == '' or sentence2 == '':\n",
    "            continue;\n",
    "        \n",
    "        id_sentence1 = [lang1.word2index[word] if word in lang1.word2index else UNK_TOKEN \n",
    "                        for word in sentence1.split()] + [EOS_TOKEN]\n",
    "        id_list1.append(id_sentence1)\n",
    "        id_sentence2 = [lang2.word2index[word] if word in lang2.word2index else UNK_TOKEN \n",
    "                        for word in sentence2.split()] + [EOS_TOKEN]\n",
    "        id_list2.append(id_sentence2)\n",
    "        \n",
    "   \n",
    "        \n",
    "    return id_list1,id_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(loaded_zh_embeddings, open('./data/zh_embeddings.p', 'wb'))\n",
    "pkl.dump(loaded_en_embeddings, open('./data/en_embeddings.p', 'wb'))\n",
    "pkl.dump(loaded_vi_embeddings, open('./data/vi_embeddings.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, category, reverse = False):#category = ['train', 'dev','test]\n",
    "    print('Reading lines:')\n",
    "    lines1 = open('data/iwslt-' + lang1.name +'-en/' + category +'.tok.'+ lang1.name, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data1 = [normalizeString(l) for l in lines1]\n",
    "    #data1 = list(filter(None, data1)) # fastest\n",
    "\n",
    "    lines2 = open('data/iwslt-' + lang1.name +'-en/' + category + '.tok.' + lang2.name, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data2 = [normalizeString(l) for l in lines2]\n",
    "    #Given that data2 is english hence we further normalize\n",
    "    data2 = [re.sub(r\"[^a-zA-Z.!?]+\", r\" \", data) for data in data2]\n",
    "    #data2 = list(filter(None, data2)) # fastest\n",
    "\n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Preparation for CHN to ENG\n",
    "def prepareData(lang1, lang2, category, reverse = False):\n",
    "    data1, data2 = readLangs(lang1, lang2, category, reverse)#Read data returns list of sentences\n",
    "    pairs = [[data1[i], data2[i]] for i in range(len(data1))]\n",
    "    print('Read %s sentence pairs' % len(pairs))\n",
    "    #Count the words\n",
    "    print('Counting words')\n",
    "    for i in range(len(pairs)):\n",
    "        lang1.addSentence(data1[i])\n",
    "        lang2.addSentence(data2[i])\n",
    "\n",
    "    print('Counted Words')\n",
    "    print(lang1.name, lang1.n_words)\n",
    "    print(lang2.name, lang2.n_words)\n",
    "\n",
    "    return pairs, data1, data2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create language object\n",
    "input_zh = Lang('zh', zh_word2id, zh_id2words, zh_ordered_words)\n",
    "output_zh_en = Lang('en', en_word2id, en_id2words, en_ordered_words)\n",
    "input_vi = Lang('vi', vi_word2id, vi_id2words, vi_ordered_words)\n",
    "output_vi_en = Lang('en', en_word2id, en_id2words, en_ordered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_zh_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the string pairs and the string lists\n",
    "train_zh_pairs, zh_train, zh_en_train = prepareData(input_zh, output_zh_en, 'train')\n",
    "val_zh_pairs, zh_val, zh_en_val = prepareData(input_zh, output_zh_en, 'dev')\n",
    "test_zh_pairs, zh_test, zh_en_test = prepareData(input_zh, output_zh_en, 'test')\n",
    "\n",
    "train_vi_pairs, vi_train, vi_en_train = prepareData(input_vi, output_vi_en, 'train')\n",
    "val_vi_pairs, vi_val, vi_en_val = prepareData(input_vi, output_vi_en, 'dev')\n",
    "test_vi_pairs, vi_test, vi_en_test = prepareData(input_vi, output_vi_en, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_idx_train, zh_en_idx_train = indexesFromSentences(input_zh, output_zh_en, train_zh_pairs)\n",
    "zh_idx_val, zh_en_idx_val = indexesFromSentences(input_zh, output_zh_en, val_zh_pairs)\n",
    "zh_idx_test, zh_en_idx_test = indexesFromSentences(input_zh, output_zh_en, test_zh_pairs)\n",
    "\n",
    "vi_idx_train, vi_en_idx_train = indexesFromSentences(input_vi, output_vi_en, train_vi_pairs)\n",
    "vi_idx_val, vi_en_idx_val = indexesFromSentences(input_vi, output_vi_en, val_vi_pairs)\n",
    "vi_idx_test, vi_en_idx_test = indexesFromSentences(input_vi, output_vi_en, test_vi_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zh_train_pairs = [[zh_idx_train[i], zh_en_idx_train[i]] for i in range(len(zh_idx_train))]\n",
    "zh_val_pairs = [[zh_idx_val[i], zh_en_idx_val[i]] for i in range(len(zh_idx_val))]\n",
    "zh_test_pairs= [[zh_idx_test[i], zh_en_idx_test[i]] for i in range(len(zh_idx_test))]\n",
    "vi_train_pairs = [[vi_idx_train[i], vi_en_idx_train[i]] for i in range(len(vi_idx_train))]\n",
    "vi_val_pairs = [[vi_idx_val[i], vi_en_idx_val[i]] for i in range(len(vi_idx_val))]\n",
    "vi_test_pairs = [[vi_idx_test[i], vi_en_idx_test[i]] for i in range(len(vi_idx_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(zh_train_pairs, open('./data/zh_train_pairs.p', 'wb'))\n",
    "pkl.dump(zh_val_pairs, open('./data/zh_val_pairs.p', 'wb'))\n",
    "pkl.dump(zh_test_pairs, open('./data/zh_test_pairs.p', 'wb'))\n",
    "\n",
    "pkl.dump(vi_train_pairs, open('./data/vi_train_pairs.p', 'wb'))\n",
    "pkl.dump(vi_val_pairs, open('./data/vi_val_pairs.p', 'wb'))\n",
    "pkl.dump(vi_test_pairs, open('./data/vi_test_pairs.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_val_len = 0\n",
    "second_len = 0\n",
    "for pair in zh_val_pairs_cleaned:\n",
    "    if max_val_len < len(pair[0]):\n",
    "        second_len = max_val_len\n",
    "        max_val_len = len(pair[0])\n",
    "\n",
    "print(max_val_len, second_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For training data, we have max-len as 100: train: 212727/212922, val: 1260/1261, test: 1397/1397\n",
    "zh_train_pairs_cleaned= []\n",
    "zh_val_pairs_cleaned = []\n",
    "zh_test_pairs_cleaned = []\n",
    "\n",
    "for zh_list in zh_train_pairs:\n",
    "    if len(zh_list[0])<=100 or len(zh_list[1]) <= 100:\n",
    "        zh_train_pairs_cleaned.append(zh_list)\n",
    "        \n",
    "for zh_list in zh_val_pairs:\n",
    "    if len(zh_list[0])<=100 or len(zh_list[1]) <= 100:\n",
    "        zh_val_pairs_cleaned.append(zh_list)\n",
    "\n",
    "for zh_list in zh_test_pairs:\n",
    "    if len(zh_list[0])<=100 or len(zh_list[1]) <= 100:\n",
    "        zh_test_pairs_cleaned.append(zh_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For training data, we have max-len as 100: train: 133038/133316, val: 1268/1268, test: 1553/1553\n",
    "vi_train_pairs_cleaned= []\n",
    "vi_val_pairs_cleaned = []\n",
    "vi_test_pairs_cleaned = []\n",
    "\n",
    "for vi_list in vi_train_pairs:\n",
    "    if len(vi_list[0])<=100 or len(vi_list[1]) <= 100:\n",
    "        vi_train_pairs_cleaned.append(vi_list)\n",
    "        \n",
    "for vi_list in vi_val_pairs:\n",
    "    if len(vi_list[0])<=100 or len(vi_list[1]) <= 100:\n",
    "        vi_val_pairs_cleaned.append(vi_list)\n",
    "\n",
    "for vi_list in vi_test_pairs:\n",
    "    if len(vi_list[0])<=100 or len(vi_list[1]) <= 100:\n",
    "        vi_test_pairs_cleaned.append(vi_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(zh_train_pairs_cleaned, open('./data/zh_train_pairs_cleaned.p', 'wb'))\n",
    "pkl.dump(zh_val_pairs_cleaned, open('./data/zh_val_pairs_cleaned.p', 'wb'))\n",
    "pkl.dump(zh_test_pairs_cleaned, open('./data/zh_test_pairs_cleaned.p', 'wb'))\n",
    "\n",
    "pkl.dump(vi_train_pairs_cleaned, open('./data/vi_train_pairs_cleaned.p', 'wb'))\n",
    "pkl.dump(vi_val_pairs_cleaned, open('./data/vi_val_pairs_cleaned.p', 'wb'))\n",
    "pkl.dump(vi_test_pairs_cleaned, open('./data/vi_test_pairs_cleaned.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs):#Needs the index pairs\n",
    "        self.pairs = pairs\n",
    "#         self.input_lang = input_lang\n",
    "#         self.output_lang = output_lang\n",
    "        self.input_seqs = [pairs[i][0] for i in range(len(self.pairs))]\n",
    "        self.output_seqs = [pairs[i][1] for i in range(len(self.pairs))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)#Returning number of pairs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.input_seqs[index]\n",
    "        output_seq = self.output_seqs[index]\n",
    "        return [input_seq, len(input_seq), output_seq, len(output_seq)]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    #Reference: lab8_3_mri\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "        padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    batch_input_seqs = [datum[0] for datum in batch]\n",
    "    batch_output_seqs = [datum[2] for datum in batch]\n",
    "    #batch_input_length = [datum[1] for datum in batch]\n",
    "    #batch_output_length = [datum[3] for datum in batch]\n",
    "\n",
    "    sorted_pairs = sorted(zip(batch_input_seqs, batch_output_seqs), key=lambda x: len(x[0]), reverse = True)\n",
    "    in_seq_sorted, out_seq_sorted = zip(*sorted_pairs)\n",
    "    \n",
    "    padded_input,input_lens = _pad_sequences(in_seq_sorted)\n",
    "    padded_output,output_lens = _pad_sequences(out_seq_sorted)\n",
    "    \n",
    "    input_list = torch.from_numpy(np.array(padded_input))\n",
    "    input_length = torch.LongTensor(input_lens)\n",
    "    output_list = torch.from_numpy(np.array(padded_output))\n",
    "    output_length = torch.LongTensor(output_lens)\n",
    "    \n",
    "    if CUDA:\n",
    "        input_list = input_list.cuda()\n",
    "        output_list = output_list.cuda()\n",
    "        input_length = input_length.cuda()\n",
    "        output_length = out_length.cuda()\n",
    "            \n",
    "    return [input_list, input_length, output_list, output_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "'''\n",
    "NMTDataset needs index pairs, need to call indexesFromPairs functions beforehand\n",
    "The dataLoader is sorted according to length of the input_length, and padded to\n",
    "max length of input and output list repectively\n",
    "TODO: output_list is not sorted, hence need to sort (maybe) in the rnn sequence.\n",
    "'''\n",
    "# train_zh_dataset = NMTDataset(zh_train_pairs_cleaned, input_zh, output_zh_en)\n",
    "# train_vi_dataset = NMTDataset(vi_train_pairs_cleaned, input_vi, output_vi_en)\n",
    "# val_zh_dataset = NMTDataset(zh_val_pairs_cleaned, input_zh, output_zh_en)\n",
    "# val_vi_dataset = NMTDataset(vi_val_pairs_cleaned, input_vi, output_vi_en)\n",
    "# test_zh_dataset = NMTDataset(zh_test_pairs_cleaned, input_zh, output_zh_en)\n",
    "# test_vi_dataset = NMTDataset(vi_test_pairs_cleaned, input_vi, output_vi_en)\n",
    "\n",
    "train_zh_dataset = NMTDataset(zh_train_pairs_cleaned)\n",
    "train_vi_dataset = NMTDataset(vi_train_pairs_cleaned)\n",
    "val_zh_dataset = NMTDataset(zh_val_pairs_cleaned)\n",
    "val_vi_dataset = NMTDataset(vi_val_pairs_cleaned)\n",
    "test_zh_dataset = NMTDataset(zh_test_pairs_cleaned)\n",
    "test_vi_dataset = NMTDataset(vi_test_pairs_cleaned)\n",
    "\n",
    "\n",
    "train_zh_loader = torch.utils.data.DataLoader(dataset = train_zh_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "\n",
    "train_vi_loader = torch.utils.data.DataLoader(dataset = train_vi_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "\n",
    "#Will use batch size 1 for validation and test since the sentence will be translated one by one\n",
    "val_zh_loader = torch.utils.data.DataLoader(dataset = val_zh_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "val_vi_loader = torch.utils.data.DataLoader(dataset = val_vi_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "test_zh_loader = torch.utils.data.DataLoader(dataset = test_zh_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "test_vi_loader = torch.utils.data.DataLoader(dataset = test_vi_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "#Input_batch in size Batch x maxLen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 256\n",
    "max_length = 10\n",
    "BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n",
    "CLIP = 50\n",
    "TEACHER_RATIO = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "loaded_zh_embeddings = torch.from_numpy(loaded_zh_embeddings).float()\n",
    "loaded_vi_embeddings = torch.from_numpy(loaded_vi_embeddings).float()\n",
    "loaded_en_embeddings = torch.from_numpy(loaded_en_embeddings).float()\n",
    "\n",
    "if CUDA:\n",
    "    loaded_zh_embeddings = loaded_zh_embeddings.cuda()\n",
    "    loaded_vi_embeddings = loaded_vi_embeddings.cuda()\n",
    "    loaded_en_embeddings = loaded_en_embeddings.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_mask(length_list):\n",
    "    max_length = length_list.max().item()\n",
    "    masked_sentences = []\n",
    "    longest_sentence = [1]*max_length\n",
    "    for i in range(BATCH_SIZE):\n",
    "        curr_length = length_list[i].item()\n",
    "        masked_sentence = [1]*max_length\n",
    "        masked_sentence[curr_length:] = [0] * (max_length - curr_length)\n",
    "        masked_sentences.append(masked_sentence)\n",
    "    if CUDA:\n",
    "        masked_sentences = torch.from_numpy(np.array(masked_sentences)).cuda()\n",
    "    else:\n",
    "        masked_sentences = torch.from_numpy(np.array(masked_sentences))\n",
    "    return masked_sentences\n",
    "        \n",
    "def rnn_mask_loss(decoder_outputs, output_list, output_length):\n",
    "    '''\n",
    "    decoder_outputs: 3d matrix containing all decoder output(B x output_lang vocab size)\n",
    "                    while decoder_output is in size(max_len x vocab_size)\n",
    "    output_list: Batch x max_len\n",
    "    output_length: batch\n",
    "    '''\n",
    "    batch_size, max_len = output_list.size()\n",
    "    decoder_outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))#(bxmax_len) x V\n",
    "    output_list = output_list.view(-1, 1)# (b x max_len) x 1 \n",
    "    neg_loss = -torch.gather(decoder_outputs, 1, output_list)#(b x max_len) x 1\n",
    "    neg_loss = neg_loss.view(batch_size, -1)# restore to b x max_len\n",
    "    \n",
    "    mask = rnn_mask(output_length)#b x max_len\n",
    "    mask_loss = neg_loss * mask.float()\n",
    "    \n",
    "    loss = mask_loss.sum() / output_length.float().sum()\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PreBatchEncoderRNN(nn.Module):\n",
    "    def __init__(self, emb, emb_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(PreBatchEncoderRNN, self).__init__()\n",
    "        \n",
    "        #self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        #self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(self.emb, False, False)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout=self.dropout, bidirectional = True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        '''\n",
    "        input_seqs in size B x L sorted in decreasing order -> will transpose to fit in embedding dimension\n",
    "        '''\n",
    "        self.batch_size = input_seqs.size(0)\n",
    "        #embedded size: max_len x B x H\n",
    "        embedded = self.embedding(input_seqs.transpose(0,1))#input_seqs B x L -> transpose to L x B\n",
    "        \n",
    "        #Input length sorted by loader\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        #Outputs in shape L x B x 2H, hidden as the last state of the GRU\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        #outputs L x B x H\n",
    "        #hidden size (2*n_layers) x B x H\n",
    "\n",
    "        #outputs: seq_len x Batch x H\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #Due to bidrectional will have self.n_layers * 2\n",
    "        return torch.zeros(self.n_layers *2, self.batch_size, self.hidden_size,device = device)#hidden size 2lays *B*H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN decoder with no attention used, batch implemented\n",
    "# RNN decoder take one token at a time\n",
    "class PreDecoderRNN(nn.Module):\n",
    "    def __init__(self, emb, emb_size, hidden_size, output_size, n_layers=1, dropout_p = 0.1):\n",
    "        super(PreDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "        #self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb, False, False)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        self.batch_size = input.size(0)\n",
    "        embedded = self.embedding(input).view(1, self.batch_size, -1)# 1 x B x E\n",
    "        embedded = self.dropout(embedded)\n",
    "        output = F.relu(embedded)\n",
    "        output, hidden = self.gru(output, hidden)#output 1 x B x E, hidden n_layers x B x H\n",
    "        out = self.out(output[0])\n",
    "        out = self.softmax(out)\n",
    "        #out size batch x output_lang_vocab_size\n",
    "        #hidden n_layers x B x H\n",
    "        return out, hidden\n",
    "    \n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference and modified from 3-nmt\n",
    "def no_attn_batch_train(input_list, input_length, output_list,output_length, \n",
    "                batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    '''\n",
    "    param: @attention is a Boolean variable indicating whether using attention\n",
    "    '''\n",
    "    batch_encoder.train()\n",
    "    batch_decoder.train()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    max_output_length = output_length.max().item()\n",
    "\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length)\n",
    "\n",
    "    #Initialize for decoding process\n",
    "    curr_batch = input_list.size(0)#Take the current batch size\n",
    "    decoder_input = torch.tensor([[SOS_TOKEN]]*curr_batch, device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "    decoder_outputs = torch.zeros(max_output_length, curr_batch, batch_decoder.output_size)\n",
    "\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "    if CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_outputs = decoder_outputs.cuda()\n",
    "    \n",
    "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden = batch_decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            decoder_outputs[di] = decoder_output\n",
    "            decoder_input = output_list[:,di] # Teacher forcing\n",
    "            loss += criterion(decoder_output, output_list[:,di])\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden = batch_decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            decoder_outputs[di] = decoder_input\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()# detach from history as input: size batch x 1 \n",
    "            if ((decoder_output == EOS_TOKEN).sum().item()) == decoder_output.size(0):#If all are EOS tokens\n",
    "                break;\n",
    "            #loss += criterion(decoder_output, output_list[:,di])\n",
    "            \n",
    "    loss += rnn_mask_loss(decoder_outputs.transpose(0,1).contiguous(), output_list.contiguous(), output_length)\n",
    "    #loss = criterion()      \n",
    "\n",
    "    loss.backward()\n",
    "    ec = torch.nn.utils.clip_grad_norm(batch_encoder.parameters(), CLIP)\n",
    "    dc = torch.nn.utils.clip_grad_norm(batch_decoder.parameters(), CLIP)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(pre_decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE)\n",
    "pre_decoder = PreDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE)\n",
    "encoder_outputs, encoder_hidden = pre_encoder(input_list, input_length)\n",
    "decoder_input = torch.tensor([[SOS_TOKEN]]*BATCH_SIZE)\n",
    "decoder_hidden = encoder_hidden[:no_attn_decoder.n_layers]\n",
    "max_output_length = output_length.max().item()\n",
    "decoder_outputs = torch.zeros(max_output_length, curr_batch, pre_decoder.output_size)\n",
    "loss = 0\n",
    "for di in range(max_output_length):\n",
    "    #print(di)\n",
    "    decoder_output, decoder_hidden = pre_decoder(\n",
    "        decoder_input, decoder_hidden)\n",
    "    decoder_outputs[di] = decoder_output\n",
    "    decoder_input = output_list[:,di] # Teacher forcing\n",
    "    loss += criterion(decoder_output, output_list[:,di])\n",
    "print(loss.item()/max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE).to(device)\n",
    "pre_decoder = PreDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(pre_decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = no_attn_batch_train(input_list, input_length, output_list, output_length, \n",
    "                       pre_encoder, pre_decoder, encoder_optimizer, decoder_optimizer, \n",
    "                       criterion)\n",
    "\n",
    "print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference lab8 1-nmt\n",
    "def greedy_no_attn_evaluate(val_loader, encoder, decoder, en_id2words ):\n",
    "    #Will generate sentences 1 by 1. \n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    decoded_words_all = []\n",
    "    decoder_attentions_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "            #if i == 5:\n",
    "            #    break\n",
    "            #batch_size, max_len = output_list.size()\n",
    "            print(input_list.size())\n",
    "            \n",
    "            # encode the source lanugage\n",
    "            encoder_outputs, encoder_hidden = encoder(input_list, input_length)\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS\n",
    "            # decode the context vector\n",
    "            decoder_hidden = encoder_hidden[:decoder.n_layers] # decoder starts from the last encoding sentence\n",
    "            # output of this function\n",
    "            decoded_words = []\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "\n",
    "                top_score, topi = decoder_output.data.topk(1)\n",
    "                decoder_attentions[di, :decoder_attention.size(-1)] = decoder_attention\n",
    "                decoded_words.append(en_id2words[topi.item()])\n",
    "                if topi.item() == EOS_TOKEN:\n",
    "                    break\n",
    "                else:\n",
    "                    decoder_input = topi.squeeze().detach()\n",
    "                    \n",
    "            decoded_words_all.append(decoded_words)\n",
    "            decoder_attentions_all.append(decoder_attentions[:di+1])\n",
    "\n",
    "        return decoded_words_all, decoder_attentions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Translate the test and val lists back to english\n",
    "def en_translate(index_list, en_id2words):\n",
    "    translated_sentence_list = []\n",
    "    for sentence in index_list:\n",
    "        translated_sentence = []\n",
    "        for index in sentence:\n",
    "            translated_sentence.append(en_id2words[index])\n",
    "        #translated_sentence.append('<EOS>')\n",
    "        translated_sentence_list.append(translated_sentence)\n",
    "    return translated_sentence_list\n",
    "def post_process(decoded_words_all):\n",
    "    cleaned_decoded_words_all = []\n",
    "    \n",
    "    for sentence in decoded_words_all:\n",
    "        cleaned_sentence = []\n",
    "        for word in sentence:\n",
    "            if word == '<PAD>':\n",
    "                continue\n",
    "            else:\n",
    "                cleaned_sentence.append(word)\n",
    "        if cleaned_sentence[-1] != '<EOS>':\n",
    "            cleaned_sentence.append(' <EOS>')\n",
    "            \n",
    "        cleaned_decoded_words_all.append(cleaned_sentence)\n",
    "        \n",
    "    return cleaned_decoded_words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference LAB8 1-nmt\n",
    "def AttnTrainIters(train_loader, encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.01,\n",
    "                  val_translated_list):\n",
    "    start = time.time()\n",
    "    learning_curve_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    for i in range(n_iters):\n",
    "        losses = []\n",
    "        for i, (input_list,input_length,output_list, output_length) in enumerate(train_loader):\n",
    "            loss = attn_batch_train(input_list, input_length, output_list, output_length, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            learning_curve_losses.append(loss)\n",
    "            \n",
    "            if i > 0 and i % 500 == 0:\n",
    "                decoded_val, decoder_attentions = greedy_attn_evaluate(val_loader, encoder, decoder, en_id2words)\n",
    "                decoded_clean = post_process(decoded_val)\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    torch.save(encoder.state_dict(), model_path + \"encoder_rnn_atten.pth\")\n",
    "    torch.save(decoder.state_dict(), model_path + \"decoder_rnn_atten.pth\")\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
