{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4489\n",
      "eng 2925\n",
      "['vous etes maigrichon .', 'you re skinny .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nous sommes seules .', 'we re alone .']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[419]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above is the data preparation stage(for single batch). No code should be changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First test out with no batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Encoder with no batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 256\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "max_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1) # B x 1 x H\n",
    "        #print('The embeddded size is {}'.format(embedded.size()))\n",
    "        #When feeded in batch sizes, the size will be B x 1 x H, one token at a time\n",
    "        output = embedded##Need changes if doing the batch size\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 1 batch size, prepare the test data\n",
    "source_list = [pairs[i][0] for i in range(10)]\n",
    "output_list = [pairs[i][1] for i in range(10)]\n",
    "\n",
    "#Here they are strings:\n",
    "input_tensor = source_list[0]\n",
    "output_tensor = output_list[0]\n",
    "\n",
    "#Transfer to tensor\n",
    "input_tensor = tensorFromSentence(input_lang, input_tensor)\n",
    "output_tensor = tensorFromSentence(output_lang, output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "if device == torch.device('cuda'):\n",
    "    encoder.cuda()\n",
    "else:\n",
    "    encoder.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = input_tensor.size(0)\n",
    "output_length = output_tensor.size(0)\n",
    "\n",
    "encoder_outputs = torch.zeros(MAX_SENTENCE_LENGTH, \n",
    "                              encoder.hidden_size, \n",
    "                              device = device)#10x256\n",
    "encoder_hidden = encoder.initHidden()#1x1x256\n",
    "\n",
    "#Encode the sentence one index at a time\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Decoder with Attention\n",
    "The decoder has already been batchified. Therefore \n",
    "if feeding 1 sentence at a time make sure unsqueeze the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers = 1, dropout_p = 0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size#vocab size of the output lang\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        word_embedded = self.embedding(word_input).view(1,1,-1)\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "\n",
    "        #last hidden = B x 1 x 256\n",
    "        #encoder outputs = B x 10 x 256\n",
    "\n",
    "        #Scoring function\n",
    "        last_hidden_repeated = last_hidden.repeat(1,encoder_outputs.size(1),1)\n",
    "        attn_weights = F.tanh(torch.add(self.attn1(last_hidden_repeated),self.attn2(encoder_outputs)))\n",
    "        v = self.v.unsqueeze(0).repeat(batch_size,1).unsqueeze(1)\n",
    "        beta = v.bmm(attn_weights.transpose(1,2)) #batch x 1 x len\n",
    "        alpha = F.softmax(beta.squeeze(1)).unsqueeze(1)#batch x 1 x len\n",
    "        #End of scoring function\n",
    "        \n",
    "        context = alpha.bmm(encoder_outputs)#1 x 1 x 10 1 x 10 x 256 -> 1x 1 x 256\n",
    "        rnn_input = torch.cat((word_embedded, context),2)#batch x 1 x 2hidden\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)#Hidden layer: layer x Bathc x Hidd\n",
    "\n",
    "        output = output.squeeze(0)\n",
    "        output = F.log_softmax(self.out(output))\n",
    "\n",
    "        return output, hidden, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For single interations only, encoer_outputs already unsqueezed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size()\n",
    "# encoder_outputs = encoder_outputs.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words)\n",
    "if device == torch.device('cuda'):\n",
    "    attn_decoder.cuda()\n",
    "else:\n",
    "    attn_decoder.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
    "decoder_hidden = encoder_hidden#1x1x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yz4499/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yz4499/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for di in range(output_length):\n",
    "    decoder_output, decoder_hidden, decoder_attntion = attn_decoder(decoder_input, \n",
    "                                                     decoder_hidden, encoder_outputs)\n",
    "    decoder_input = output_tensor[di]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder outputs remained unchanged\n",
    "## End of the decoder and encoder session\n",
    "code shall not be changed. \n",
    "two sentence has already been encoded and now we\n",
    "aim to generate best translation\n",
    "\n",
    "## During the train, will be the inference state\n",
    "two methods: greedy and beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First greedy search: training and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-1053716553c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Teacher forcing: Feed the target as the next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0m\u001b[1;32m     11\u001b[0m             decoder_input, decoder_hidden, encoder_outputs)\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "#Greedy search\n",
    "#DOnt execute these\n",
    "#Training process:\n",
    "teacher_forcing_ratio = 0.6\n",
    "use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "if use_teacher_forcing:\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(output_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(output_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        #topv is the score, topi is the index of the highest score\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the decoder_hidden here\n",
    "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
    "decoder_hidden = encoder_hidden#1x1x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yz4499/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yz4499/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "#Evaluations:\n",
    "decoded_words = []\n",
    "decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "for di in range(max_length):\n",
    "    decoder_output, decoder_hidden, decoder_attention = attn_decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    topv, topi = decoder_output.topk(1)#Take the best one\n",
    "    #teacher forcing or not\n",
    "    #decoder_input = topi.squeeze().detach()\n",
    "    decoder_input = output_tensor[di]#For next round\n",
    "    decoder_attentions[di] = decoder_attention[0,0]\n",
    "    decoded_words.append(output_lang.index2word[topi.item()])#append the best token right now\n",
    "    \n",
    "    if decoder_input.item() == EOS_token:\n",
    "\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5106, 0.1436, 0.0858, 0.0185, 0.0340, 0.0415, 0.0415, 0.0415, 0.0415,\n",
       "         0.0415],\n",
       "        [0.5122, 0.1391, 0.0874, 0.0178, 0.0318, 0.0423, 0.0423, 0.0423, 0.0423,\n",
       "         0.0423],\n",
       "        [0.4883, 0.1467, 0.0946, 0.0178, 0.0328, 0.0440, 0.0440, 0.0440, 0.0440,\n",
       "         0.0440],\n",
       "        [0.4913, 0.1440, 0.0894, 0.0180, 0.0327, 0.0449, 0.0449, 0.0449, 0.0449,\n",
       "         0.0449],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Beam Search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchNode(object):\n",
    "    def __init__(self, word_idx, hidden, curr_score, prev, curr_length):\n",
    "        self.word_idx = word_idx        \n",
    "        self.hidden = hidden\n",
    "        self.prev = prev\n",
    "        if self.prev == None:\n",
    "            self.score = curr_score\n",
    "        else:\n",
    "            self.score = self.prev.score + curr_score#The score summed up so far\n",
    "        \n",
    "        self.curr_length = curr_length #the nth node in the sentence\n",
    "        self.next = None\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_beam(decoder_hidden, encoder_outputs, decoder, k):\n",
    "    decoder_input = torch.tensor([[SOS_token]], device = device)\n",
    "    start_node = SearchNode(SOS_token,decoder_hidden, 0,None,1 )\n",
    "    \n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    curr_nodes = []\n",
    "    scores, indexes = torch.topk(decoder_output, k)##Watch out for dimension, topk on the vocab direction\n",
    "    for i in range(k):\n",
    "        curr_idx = indexes[0][i].view(1,-1)\n",
    "            \n",
    "        curr_score = scores[0][i].item()\n",
    "        curr_node = SearchNode(curr_idx, decoder_hidden, curr_score, start_node, 2)\n",
    "\n",
    "        curr_nodes.append((curr_node, curr_score))\n",
    "        \n",
    "    return curr_nodes, decoder_hidden\n",
    "\n",
    "def beam_search(decoder_hidden,encoder_outputs,decoder,k):\n",
    "    curr_nodes, decoder_hidden = init_beam(decoder_hidden, encoder_outputs, decoder, k)\n",
    "\n",
    "    end_nodes = []#List including the ending nodes to trace back\n",
    "    while len(curr_nodes) != 0 and len(end_nodes) < k:\n",
    "        next_nodes = []   \n",
    "        for curr_node, curr_score in curr_nodes:\n",
    "            #First we decide whether to terminate the search or not\n",
    "            #print(curr_node.word_idx.item())\n",
    "            \n",
    "            if (curr_node.word_idx == EOS_token):#If already at the end of the sentence, then skip the calculation\n",
    "                end_nodes.append((curr_node, curr_node.score))\n",
    "                if len(end_nodes) >= k:\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "                    \n",
    "            if curr_node.curr_length == max_length:\n",
    "                end_nodes.append((curr_node, curr_node.score))\n",
    "                if len(end_nodes) >= k:\n",
    "                    break;\n",
    "                else:\n",
    "                    continue;\n",
    "            \n",
    "            decoder_input = curr_node.word_idx\n",
    "            decoder_hidden = curr_node.hidden\n",
    "            decoder_output, decoder_hidden,decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            scores, indexes = torch.topk(decoder_output, k)##Watch out for dimension, topk on the vocab direction\n",
    "            for i in range(k):\n",
    "                next_idx = indexes[0][i].view(1,-1)\n",
    "                next_score = scores[0][i].item()\n",
    "                next_node = SearchNode(next_idx, decoder_hidden, next_score, curr_node, curr_node.curr_length+1)\n",
    "\n",
    "                next_nodes.append((next_node, next_node.score))\n",
    "        \n",
    "        curr_nodes = sorted(next_nodes, key = operator.itemgetter(1))\n",
    "        curr_nodes = curr_nodes[:k]\n",
    "        #j+=1\n",
    "\n",
    "    return end_nodes        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disintegration of beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the decoder_hidden here\n",
    "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
    "decoder_hidden = encoder_hidden#1x1x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "end_nodes = beam_search(decoder_hidden, encoder_outputs, decoder = attn_decoder, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<__main__.SearchNode at 0x122549a90>, -66.93753623962402),\n",
       " (<__main__.SearchNode at 0x122549588>, -66.88386154174805)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Genereating the sentences.\n",
    "sentence = []\n",
    "end_nodes = sorted(end_nodes, key = operator.itemgetter(1),reverse = True)\n",
    "(end_node, end_score) = end_nodes[0]\n",
    "while end_node.word_idx != SOS_token:\n",
    "    sentence.append(output_lang.index2word[end_node.word_idx.item()])\n",
    "    end_node = end_node.prev\n",
    "sentence = sentence[::-1]#Reverse the sentence to get the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleared',\n",
       " 'snakes',\n",
       " 'singapore',\n",
       " 'graceful',\n",
       " 'working',\n",
       " 'deluding',\n",
       " 'wild',\n",
       " 'ashore',\n",
       " 'dragging']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of disintegration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Genereating the sentences.\n",
    "sentence = []\n",
    "end_nodes = sorted(end_nodes, key = operator.itemgetter(1),reverse = True)\n",
    "end_node = end_nodes[0]\n",
    "while end_node.word_idx != EOS_token:\n",
    "    sentence.append(end_node.word_idx)\n",
    "    end_node = end_node.prev\n",
    "sentence = sentence[::-1]#Reverse the sentence to get the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize the decoder_hidden here\n",
    "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
    "decoder_hidden = encoder_hidden#1x1x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        #topv is the score, topi is the index of the highest score\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'word_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-4dfb2f503967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Evaluations: beam_width = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mend_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-223-8745fddc648a>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(decoder_hidden, encoder_outputs, decoder, k)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnext_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurr_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurr_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEOS_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#If already at the end of the sentence, then skip the calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mend_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'word_idx'"
     ]
    }
   ],
   "source": [
    "#Evaluations: beam_width = 2\n",
    "end_nodes = beam_search(decoder_hidden, encoder_outputs,decoder = attn_decoder, k=2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training:\n",
    "for di in range(max_length):\n",
    "    decoder_output,decoder-hid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
