{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaned_Version_12_2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IpdH9W7VSkuE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import operator\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPnBUrB2Skut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "            \n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "LTj_tfp8Sku4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d43d7d3b-c9e4-43f8-9c29-55e34d6991ab"
      },
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10853 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4489\n",
            "eng 2925\n",
            "['je ne baisse pas facilement les bras .', 'i m no quitter .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "7TBaXKjaSkvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a816803b-4a76-4382-a2a4-5288eb5cdba6"
      },
      "cell_type": "code",
      "source": [
        "pairs[419]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nous sommes seules .', 'we re alone .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "GEApaeEESkvN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The above is the data preparation stage(for single batch). No code shall be changed. "
      ]
    },
    {
      "metadata": {
        "id": "Ce0_0xzXSkvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. First test out with no batch size"
      ]
    },
    {
      "metadata": {
        "id": "Hupv89HHSkvR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Encoder with no batch"
      ]
    },
    {
      "metadata": {
        "id": "Ra2HD52iSkvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here for the constant definition\n",
        "MAX_SENTENCE_LENGTH = 10\n",
        "hidden_size = 256\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "max_length = 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_78Ucr73SkvZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1,1,-1) # B x 1 x H\n",
        "        #print('The embeddded size is {}'.format(embedded.size()))\n",
        "        #When feeded in batch sizes, the size will be B x 1 x H, one token at a time\n",
        "        output = embedded##Need changes if doing the batch size\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1,1,self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RbdhCOEjSkve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For 1 batch size, prepare the test data\n",
        "source_list = [pairs[i][0] for i in range(10)]\n",
        "output_list = [pairs[i][1] for i in range(10)]\n",
        "\n",
        "#Here they are strings:\n",
        "input_tensor = source_list[0]\n",
        "output_tensor = output_list[0]\n",
        "\n",
        "#Transfer to tensor\n",
        "input_tensor = tensorFromSentence(input_lang, input_tensor)\n",
        "output_tensor = tensorFromSentence(output_lang, output_tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxWUax4eSkvl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
        "if device == torch.device('cuda'):\n",
        "    encoder.cuda()\n",
        "else:\n",
        "    encoder.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Wjh5TSxSkvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_length = input_tensor.size(0)\n",
        "output_length = output_tensor.size(0)\n",
        "\n",
        "encoder_outputs = torch.zeros(MAX_SENTENCE_LENGTH, \n",
        "                              encoder.hidden_size, \n",
        "                              device = device)#10x256\n",
        "encoder_hidden = encoder.initHidden()#1x1x256\n",
        "\n",
        "#Encode the sentence one index at a time\n",
        "for ei in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "    encoder_outputs[ei] = encoder_output[0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ahPrqXLxSkvw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 Decoder with Attention\n",
        "The decoder has already been batchified. Therefore \n",
        "if feeding 1 sentence at a time make sure unsqueeze the batch dimension"
      ]
    },
    {
      "metadata": {
        "id": "Uj2OwtMUSkvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers = 1, dropout_p = 0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size#vocab size of the output lang\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.attn1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.attn2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers, dropout = dropout_p)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        word_embedded = self.embedding(word_input).view(1,1,-1)\n",
        "        word_embedded = self.dropout(word_embedded)\n",
        "\n",
        "        #last hidden = B x 1 x 256\n",
        "        #encoder outputs = B x 10 x 256\n",
        "\n",
        "        #Scoring function\n",
        "        last_hidden_repeated = last_hidden.repeat(1,encoder_outputs.size(1),1)\n",
        "        attn_weights = F.tanh(torch.add(self.attn1(last_hidden_repeated),self.attn2(encoder_outputs)))\n",
        "        v = self.v.unsqueeze(0).repeat(batch_size,1).unsqueeze(1)\n",
        "        beta = v.bmm(attn_weights.transpose(1,2)) #batch x 1 x len\n",
        "        alpha = F.softmax(beta.squeeze(1)).unsqueeze(1)#batch x 1 x len\n",
        "        #End of scoring function\n",
        "        \n",
        "        context = alpha.bmm(encoder_outputs)#1 x 1 x 10 1 x 10 x 256 -> 1x 1 x 256\n",
        "        rnn_input = torch.cat((word_embedded, context),2)#batch x 1 x 2hidden\n",
        "        output, hidden = self.gru(rnn_input, last_hidden)#Hidden layer: layer x Bathc x Hidd\n",
        "\n",
        "        output = output.squeeze(0)\n",
        "        output = F.log_softmax(self.out(output))\n",
        "\n",
        "        return output, hidden, alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYy5zx5ZSkv3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# For single interations only, encoer_outputs already unsqueezed "
      ]
    },
    {
      "metadata": {
        "id": "H2qBZl5VSkv5",
        "colab_type": "code",
        "colab": {},
        "outputId": "da67951f-8edf-4f57-ee29-9f2467c9c427"
      },
      "cell_type": "code",
      "source": [
        "#encoder_outputs = encoder_outputs.unsqueeze(0)\n",
        "encoder_outputs.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "lSnh4xs-SkwA",
        "colab_type": "code",
        "colab": {},
        "outputId": "c371b007-c85d-4288-8b56-dbeafae1b285"
      },
      "cell_type": "code",
      "source": [
        "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words)\n",
        "if device == torch.device('cuda'):\n",
        "    attn_decoder.cuda()\n",
        "else:\n",
        "    attn_decoder.cpu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "euJHWt_rSkwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "decoder_hidden = encoder_hidden#1x1x256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VZoj_s0pSkwN",
        "colab_type": "code",
        "colab": {},
        "outputId": "9b04e99f-2e30-41c1-c6e3-bc9c4e177cd4"
      },
      "cell_type": "code",
      "source": [
        "for di in range(output_length):\n",
        "    decoder_output, decoder_hidden, decoder_attntion = attn_decoder(decoder_input, \n",
        "                                                     decoder_hidden, encoder_outputs)\n",
        "    decoder_input = output_tensor[di]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jv225GF3SkwV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Note: Encoder outputs remained unchanged -> can be used later for testing\n",
        "## End of the decoder and encoder session\n",
        "code shall not be changed. \n",
        "two sentence has already been encoded and now we\n",
        "aim to generate best translation\n",
        "\n",
        "## During the train, will be the inference state\n",
        "## During training, no beam needed -> only during the evaluations state -> reference: Chow's note\n",
        "two methods: greedy and beam search"
      ]
    },
    {
      "metadata": {
        "id": "abK1EDGTSkwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# First greedy search: training and evaluations"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "NHVjUJxcSkwb",
        "colab_type": "code",
        "colab": {},
        "outputId": "8e790e82-d2ff-42d6-c6e3-b0f3a8ca66e2"
      },
      "cell_type": "code",
      "source": [
        "#Greedy search\n",
        "#DOnt execute these\n",
        "#Training process:\n",
        "teacher_forcing_ratio = 0.6\n",
        "use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "if use_teacher_forcing:\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for di in range(output_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "else:\n",
        "    # Without teacher forcing: use its own predictions as the next input\n",
        "    for di in range(output_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "        #topv is the score, topi is the index of the highest score\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "\n",
        "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        if decoder_input.item() == EOS_token:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'decoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-1053716553c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Teacher forcing: Feed the target as the next input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0m\u001b[1;32m     11\u001b[0m             decoder_input, decoder_hidden, encoder_outputs)\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'decoder' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dAGwnm_uSkwj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize the decoder_hidden here\n",
        "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "decoder_hidden = encoder_hidden#1x1x256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yzwfHFwSSkwo",
        "colab_type": "code",
        "colab": {},
        "outputId": "2ec1b84f-33bb-4c85-aebe-91d12cb36fce"
      },
      "cell_type": "code",
      "source": [
        "#Evaluations:\n",
        "decoded_words = []\n",
        "decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "for di in range(max_length):\n",
        "    decoder_output, decoder_hidden, decoder_attention = attn_decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "    topv, topi = decoder_output.topk(1)#Take the best one\n",
        "    #teacher forcing or not\n",
        "    #decoder_input = topi.squeeze().detach()\n",
        "    decoder_input = output_tensor[di]#For next round\n",
        "    decoder_attentions[di] = decoder_attention[0,0]\n",
        "    decoded_words.append(output_lang.index2word[topi.item()])#append the best token right now\n",
        "    \n",
        "    if decoder_input.item() == EOS_token:\n",
        "\n",
        "        break;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/yz4499/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/home/yz4499/miniconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "73nc7F7LSkwv",
        "colab_type": "code",
        "colab": {},
        "outputId": "6a0f83a7-121b-4240-fca9-e3e7ed0f1f2f"
      },
      "cell_type": "code",
      "source": [
        "decoder_attentions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5106, 0.1436, 0.0858, 0.0185, 0.0340, 0.0415, 0.0415, 0.0415, 0.0415,\n",
              "         0.0415],\n",
              "        [0.5122, 0.1391, 0.0874, 0.0178, 0.0318, 0.0423, 0.0423, 0.0423, 0.0423,\n",
              "         0.0423],\n",
              "        [0.4883, 0.1467, 0.0946, 0.0178, 0.0328, 0.0440, 0.0440, 0.0440, 0.0440,\n",
              "         0.0440],\n",
              "        [0.4913, 0.1440, 0.0894, 0.0180, 0.0327, 0.0449, 0.0449, 0.0449, 0.0449,\n",
              "         0.0449],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]], grad_fn=<CopySlices>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "sZwsMdOKSkw3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Second Beam Search: "
      ]
    },
    {
      "metadata": {
        "id": "gZWJ9dwmSkw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SearchNode(object):\n",
        "    def __init__(self, word_idx, hidden, curr_score, prev, curr_length):\n",
        "        self.word_idx = word_idx#Tensor        \n",
        "        self.hidden = hidden\n",
        "        self.prev = prev\n",
        "        if self.prev == None:\n",
        "            self.score = curr_score\n",
        "        else:\n",
        "            self.score = self.prev.score + curr_score#The score summed up so far\n",
        "        \n",
        "        self.curr_length = curr_length #the nth node in the sentence\n",
        "        #self.next = None\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-xDRO4nSkw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def init_beam(decoder_hidden, encoder_outputs, decoder, beam_width):\n",
        "    decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "    start_node = SearchNode(decoder_input,decoder_hidden, 0,None,1 )\n",
        "    \n",
        "    decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "    curr_nodes = []\n",
        "    scores, indexes = torch.topk(decoder_output, beam_width)##Watch out for dimension, topk on the vocab direction\n",
        "    for i in range(beam_width):\n",
        "        curr_idx = indexes[0][i].view(1,-1)\n",
        "            \n",
        "        curr_score = scores[0][i].item()\n",
        "        curr_node = SearchNode(curr_idx, decoder_hidden, curr_score, start_node, 2)\n",
        "        curr_nodes.append((curr_node, curr_score))\n",
        "        \n",
        "    return curr_nodes, decoder_hidden\n",
        "\n",
        "def beam_search(decoder_hidden,encoder_outputs,decoder,beam_width, k):\n",
        "    '''\n",
        "    beam_width: number of best nodes kept at each iterations\n",
        "    k: number of sentences we want to keep\n",
        "    '''\n",
        "    curr_nodes, decoder_hidden = init_beam(decoder_hidden, encoder_outputs, decoder, beam_width)\n",
        "\n",
        "    end_nodes = []#List including the ending nodes to trace back i.e. the EOS's\n",
        "    #n_needed = min((k+1), k-len(end_nodes))\n",
        "    \n",
        "    while len(end_nodes) < k:\n",
        "        candidate_nodes = []   \n",
        "        for curr_node, curr_score in curr_nodes:\n",
        "            \n",
        "            #First we decide whether to terminate the search or not            \n",
        "            if (curr_node.word_idx == EOS_token):#If already at the end of the sentence, then skip the calculation\n",
        "                end_nodes.append((curr_node, curr_node.score))\n",
        "                if len(end_nodes) >= k: #if already has the desired number of sentences generated\n",
        "                    break;\n",
        "                else:\n",
        "                    continue;\n",
        "                          \n",
        "            #If the inference takes too long, also termiantes\n",
        "            if curr_node.curr_length == 2000: #if too long will force to stop\n",
        "                #Create an EOS dummy node to trace back the entire sentence\n",
        "                \n",
        "                EOS_node = SearchNode(torch.tensor([[EOS_token]], device = device), curr_node.hidden, \n",
        "                                      curr_node.score, curr_node, (curr_node.curr_length)+1)\n",
        "                end_nodes.append((EOS_node, EOS_node.score))\n",
        "                if len(end_nodes) >= k:\n",
        "                    break;\n",
        "                else:\n",
        "                    continue;\n",
        "            \n",
        "            #If not, continue to inference the current time step choices, given the current node\n",
        "            decoder_input = curr_node.word_idx\n",
        "            decoder_hidden = curr_node.hidden\n",
        "            decoder_output, decoder_hidden,decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            #Pick out k best for the current node\n",
        "            scores, indexes = torch.topk(decoder_output, beam_width)##Watch out for dimension, topk on the vocab direction\n",
        "            for i in range(beam_width):\n",
        "                candidate_idx = indexes[0][i].view(1,-1)\n",
        "                candidate_score = scores[0][i].item()\n",
        "                candidate_node = SearchNode(candidate_idx, decoder_hidden, candidate_score, curr_node, (curr_node.curr_length)+1)\n",
        "\n",
        "                candidate_nodes.append((candidate_node, candidate_node.score))\n",
        "        \n",
        "        #Will now have candidate candidate_nodes with size beam_width * beam_width, only need the top beam_width one.\n",
        "        curr_nodes = sorted(candidate_nodes, key = operator.itemgetter(1))\n",
        "        curr_nodes = curr_nodes[:beam_width]\n",
        "\n",
        "    return end_nodes              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "VEC9kHCJSkxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Translate the sentences.\n",
        "def translate_end_nodes(end_nodes):\n",
        "    sentences = []\n",
        "    end_nodes = sorted(end_nodes, key = operator.itemgetter(1),reverse = True)\n",
        "    for end_node, _ in end_nodes:\n",
        "        sentence = []\n",
        "        while end_node.prev != None:\n",
        "            sentence.append(output_lang.index2word[end_node.word_idx.item()])\n",
        "            end_node = end_node.prev\n",
        "        sentence = sentence[::-1]#Reverse the sentence to get the sentence\n",
        "        sentences.append(sentence)\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RNhtm07-SkxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# beam search test"
      ]
    },
    {
      "metadata": {
        "id": "AcjfFDJRSkxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize the decoder_hidden here\n",
        "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "decoder_hidden = encoder_hidden#1x1x256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j5IKeW0zSkxN",
        "colab_type": "code",
        "colab": {},
        "outputId": "4e70d5b5-89f0-4db1-f967-17d502e945c6"
      },
      "cell_type": "code",
      "source": [
        "#Test if the batch is unsqueezed; only doing so when batch not implemented and hidden size = 256 max_length = 10\n",
        "list(encoder_outputs.size()) == [1, 10, 256]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "1fW4HnIASkxS",
        "colab_type": "code",
        "colab": {},
        "outputId": "05ecc714-ff04-4ba2-a3d3-96bd636b7240"
      },
      "cell_type": "code",
      "source": [
        "end_nodes = beam_search(decoder_hidden, encoder_outputs, decoder = attn_decoder, beam_width = 10, k = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "OJoD_MlMSkxY",
        "colab_type": "code",
        "colab": {},
        "outputId": "7028a8c2-a422-4ac2-c6d7-50cf750d914c"
      },
      "cell_type": "code",
      "source": [
        "end_nodes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<__main__.SearchNode at 0x122323b00>, -10116.319769859314)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "w2oSe-3tSkxe",
        "colab_type": "code",
        "colab": {},
        "outputId": "1eba8a2e-5ff5-4c0e-a0b4-09c4e78cca5e"
      },
      "cell_type": "code",
      "source": [
        "result = translate_end_nodes(end_nodes)\n",
        "print(len(result[0]))\n",
        "result[0][:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['immediately',\n",
              " 'television',\n",
              " 'those',\n",
              " 'endowed',\n",
              " 'chances',\n",
              " 'foggy',\n",
              " 'actress',\n",
              " 'grateful',\n",
              " 'discussing',\n",
              " 'eggs',\n",
              " 'about',\n",
              " 'caught',\n",
              " 'look',\n",
              " 'familiar',\n",
              " 'typist',\n",
              " 'nice',\n",
              " 'power',\n",
              " 'cousin',\n",
              " 'choices',\n",
              " 'hook']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "metadata": {
        "id": "rjtsLxizSkxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# End of testing beam search"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "WSMFUnYbSkxn",
        "colab_type": "code",
        "colab": {},
        "outputId": "055dfb24-6d27-47f1-ca99-682b0ac86b9a"
      },
      "cell_type": "code",
      "source": [
        "#Evaluations: beam_width = 2\n",
        "end_nodes = beam_search(decoder_hidden, encoder_outputs,decoder = attn_decoder, k=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'word_idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-4dfb2f503967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Evaluations: beam_width = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mend_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-223-8745fddc648a>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(decoder_hidden, encoder_outputs, decoder, k)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnext_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurr_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurr_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEOS_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#If already at the end of the sentence, then skip the calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mend_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'word_idx'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "scBk72klSkx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train the lab data, if workable, then implement the mini-batch"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "b_--b6wrSkx3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train function works#"
      ]
    },
    {
      "metadata": {
        "id": "wEoljqowSkx4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_hidden = encoder1.initHidden()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hQL73hxaSkx9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_length = input_tensor.size(0)\n",
        "output_length = output_tensor.size(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3dBQn-BSkyF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_outputs = torch.zeros(max_length, encoder1.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rqyaMNbgSkyI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_output, encoder_hidden = encoder1(input_tensor[0], encoder_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1L8WCnISkyN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder1(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rsy237l8SkyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## End of train function"
      ]
    },
    {
      "metadata": {
        "id": "E5zGyXCRSkyd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "MAX_LENGTH = 10\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    \n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "        \n",
        "    #Added specifically for this test\n",
        "    encoder_outputs = encoder_outputs.unsqueeze(0)\n",
        "    #Will be deleted\n",
        "    \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Okuo-cDgSkym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJx5RwI2Skyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w99CrHUySkyz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1f0cbfc8-c044-42e3-dcab-5f973bcc0bfa"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, n_layers=1, dropout_p=0.1).to(device)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "g4WoF3vdSky4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "e2d84b73-ab35-4c5d-f131-6e2dca0af39d"
      },
      "cell_type": "code",
      "source": [
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4m 50s (- 67m 45s) (5000 6%) 2.8977\n",
            "9m 38s (- 62m 41s) (10000 13%) 2.2920\n",
            "14m 26s (- 57m 44s) (15000 20%) 1.9737\n",
            "19m 17s (- 53m 3s) (20000 26%) 1.7344\n",
            "24m 9s (- 48m 18s) (25000 33%) 1.5166\n",
            "29m 1s (- 43m 31s) (30000 40%) 1.3491\n",
            "33m 52s (- 38m 42s) (35000 46%) 1.2389\n",
            "38m 46s (- 33m 55s) (40000 53%) 1.0919\n",
            "43m 39s (- 29m 6s) (45000 60%) 0.9597\n",
            "48m 33s (- 24m 16s) (50000 66%) 0.8673\n",
            "53m 29s (- 19m 27s) (55000 73%) 0.7623\n",
            "58m 23s (- 14m 35s) (60000 80%) 0.7051\n",
            "63m 17s (- 9m 44s) (65000 86%) 0.6131\n",
            "68m 11s (- 4m 52s) (70000 93%) 0.5485\n",
            "73m 5s (- 0m 0s) (75000 100%) 0.5004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbb62590710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbb62590278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fsh2OmCzSky9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(75000)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uj1HSP79SkzB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=0.01)\n",
        "decoder_optimizer = optim.SGD(attn_decoder1.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFlMcSwESkzE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJLfEHepSkzI",
        "colab_type": "code",
        "colab": {},
        "outputId": "48fccbd6-b325-47f2-b44b-d875d873c9c6"
      },
      "cell_type": "code",
      "source": [
        "loss = train(input_tensor, output_tensor, encoder1,\n",
        "                     attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5UT77nmcSkzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}