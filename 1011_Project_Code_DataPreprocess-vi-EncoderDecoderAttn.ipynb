{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from queue import PriorityQueue\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from sacrebleu import raw_corpus_bleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define constants here\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 3\n",
    "words_to_load = 80000\n",
    "emb_size = 300\n",
    "wiki_size = 300\n",
    "CUDA = True\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.getcwd()\n",
    "datadir\n",
    "ftdir = '/scratch/yz4499/fasttext/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained word embeddings:\n",
    "Reference: https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "\n",
    "@article{bojanowski2017enriching,\n",
    "  title={Enriching Word Vectors with Subword Information},\n",
    "  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n",
    "  journal={Transactions of the Association for Computational Linguistics},\n",
    "  volume={5},\n",
    "  year={2017},\n",
    "  issn={2307-387X},\n",
    "  pages={135--146}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference Lab4 HW2\n",
    "# datadir = os.getcwd()\n",
    "words_to_load = 50000\n",
    "# with open(datadir + '/data/wiki-news-300d-1M.vec') as f:\n",
    "with open(ftdir + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_en_embeddings = np.zeros(((words_to_load+4), wiki_size))\n",
    "    en_word2id = {}\n",
    "    en_id2words = {}\n",
    "    \n",
    "    en_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    en_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    en_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    en_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    en_word2id['<PAD>'] = PAD_TOKEN\n",
    "    en_word2id['<SOS>'] = SOS_TOKEN\n",
    "    en_word2id['<EOS>'] = EOS_TOKEN\n",
    "    en_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    en_ordered_words= []\n",
    "    en_ordered_words.append('<PAD>')\n",
    "    en_ordered_words.append('<SOS>')\n",
    "    en_ordered_words.append('<EOS>')\n",
    "    en_ordered_words.append('<UNK>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        if i ==0:#Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        #print(len(s))\n",
    "        loaded_en_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        en_word2id[s[0]] = i+4 #for extra pad and unk eos and unk\n",
    "        en_id2words[i+4] = s[0]\n",
    "        en_ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 0 has wrong dimension, hence skipped\n"
     ]
    }
   ],
   "source": [
    "#Reference Lab4 HW2\n",
    "#Over 200000 loaded words, 58 has wrong dimensions\n",
    "words_to_load = 50000\n",
    "# datadir = os.getcwd()\n",
    "with open(ftdir + 'cc.vi.300.vec') as f:\n",
    "    loaded_vi_embeddings = np.zeros(((words_to_load+4),wiki_size))\n",
    "    vi_word2id = {}\n",
    "    vi_id2words = {}\n",
    "    \n",
    "    vi_id2words[PAD_TOKEN] = '<PAD>'\n",
    "    vi_id2words[SOS_TOKEN] = '<SOS>'\n",
    "    vi_id2words[EOS_TOKEN] = '<EOS>'\n",
    "    vi_id2words[UNK_TOKEN] = '<UNK>'\n",
    "    \n",
    "    vi_word2id['<PAD>'] = PAD_TOKEN\n",
    "    vi_word2id['<SOS>'] = SOS_TOKEN\n",
    "    vi_word2id['<EOS>'] = EOS_TOKEN\n",
    "    vi_word2id['<UNK>'] = UNK_TOKEN\n",
    "    \n",
    "    vi_ordered_words= []\n",
    "    vi_ordered_words.append('<PAD>')\n",
    "    vi_ordered_words.append('<SOS>')\n",
    "    vi_ordered_words.append('<EOS>')\n",
    "    vi_ordered_words.append('<UNK>')\n",
    "    wrong_dim = 0;\n",
    "    for i, line in enumerate(f):\n",
    "        #print(line)\n",
    "        if i >= words_to_load:\n",
    "            break;\n",
    "        if i == 0: #Ignore the first line\n",
    "            continue;\n",
    "        s = line.split()\n",
    "        if len(s) != 301:\n",
    "            wrong_dim += 1#Skip the wrong dimension one\n",
    "            continue;\n",
    "        loaded_vi_embeddings[i+4,:] = np.asarray(s[1:])\n",
    "        vi_word2id[s[0]] = i+4 #for extra pad and unk \n",
    "        vi_id2words[i+4] = s[0]\n",
    "        vi_ordered_words.append(s[0])\n",
    "    print('In total {} has wrong dimension, hence skipped'.format(wrong_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# pkl.dump(loaded_zh_embeddings, open(ftdir+'zh_embeddings.p', 'wb'))\n",
    "# pkl.dump(loaded_en_embeddings, open(ftdir+'en_embeddings.p', 'wb'))\n",
    "# pkl.dump(loaded_vi_embeddings, open(ftdir+'vi_embeddings.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, emb_word2id, emb_id2word, emb_ordered_words):\n",
    "        self.name = name\n",
    "        self.word2index = emb_word2id\n",
    "        self.word2count = {}\n",
    "        self.index2word = emb_id2word #Dict\n",
    "        self.n_words = 4  # Count SOS and EOS +(batch: pad and unk)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2count:\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s, lang):\n",
    "    if lang == \"en\":\n",
    "        s = s.replace(\"&apos;\", \"\").replace(\"&quot;\", \"\")\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #This line is commented out since it will not properly deal with Chinese Letters\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "#reference: LAB4 hw2\n",
    "def indexesFromSentences(lang1, lang2, pairs):\n",
    "    id_list1 = []\n",
    "    id_list2 = []\n",
    "    for i in range(len(pairs)):\n",
    "        sentence1 = pairs[i][0]\n",
    "        sentence2 = pairs[i][1]\n",
    "        \n",
    "        sentence1 = sentence1.replace('quot','')\n",
    "        sentence1 = sentence1.replace('apos', '')\n",
    "        sentence2 = sentence2.replace('quot','')\n",
    "        sentence2 = sentence2.replace('apos', '')\n",
    "        #If either sentence is empty, then remove the pair\n",
    "        if sentence1 == '' or sentence2 == '':\n",
    "            continue;\n",
    "        \n",
    "        id_sentence1 = [lang1.word2index[word] if word in lang1.word2index else UNK_TOKEN \n",
    "                        for word in sentence1.split()] + [EOS_TOKEN]\n",
    "        id_list1.append(id_sentence1)\n",
    "        id_sentence2 = [lang2.word2index[word] if word in lang2.word2index else UNK_TOKEN \n",
    "                        for word in sentence2.split()] + [EOS_TOKEN]\n",
    "        id_list2.append(id_sentence2)\n",
    "        \n",
    "   \n",
    "        \n",
    "    return id_list1,id_list2\n",
    "\n",
    "# def sentence2id(sentence_list):\n",
    "#     id_list = []\n",
    "#     for sentence in sentence_list:\n",
    "#         sentence_id_list = [word2id[word] if word in word2id else UNK_IDX for word in sentence]\n",
    "#         id_list.append(sentence_id_list)\n",
    "#     return id_list\n",
    "\n",
    "# def tensorFromSentence(lang, sentence):\n",
    "#     indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_TOKEN)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "# def tensorsFromPair(pair):\n",
    "#     input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "#     target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "#     return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "# def filterPair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "#         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "#         p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "# def filterPairs(pairs):\n",
    "#     return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, category, reverse = False):#category = ['train', 'dev','test]\n",
    "    print('Reading lines:')\n",
    "    lines1 = open('data/iwslt-' + lang1.name +'-en/' + category +'.tok.'+ lang1.name, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data1 = [normalizeString(l, lang1.name) for l in lines1]\n",
    "    #data1 = list(filter(None, data1)) # fastest\n",
    "\n",
    "    lines2 = open('data/iwslt-' + lang1.name +'-en/' + category + '.tok.' + lang2.name, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data2 = [normalizeString(l, lang2.name) for l in lines2]\n",
    "    #Given that data2 is english hence we further normalize\n",
    "    data2 = [re.sub(r\"[^a-zA-Z.!?]+\", r\" \", data) for data in data2]\n",
    "    #data2 = list(filter(None, data2)) # fastest\n",
    "\n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation for CHN to ENG\n",
    "def prepareData(lang1, lang2, category, reverse = False):\n",
    "    data1, data2 = readLangs(lang1, lang2, category, reverse)#Read data returns list of sentences\n",
    "    pairs = [[data1[i], data2[i]] for i in range(len(data1))]\n",
    "    print('Read %s sentence pairs' % len(pairs))\n",
    "    #Count the words\n",
    "    print('Counting words')\n",
    "    for i in range(len(pairs)):\n",
    "        lang1.addSentence(data1[i])\n",
    "        lang2.addSentence(data2[i])\n",
    "\n",
    "    print('Counted Words')\n",
    "    print(lang1.name, lang1.n_words)\n",
    "    print(lang2.name, lang2.n_words)\n",
    "\n",
    "    return pairs, data1, data2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create language object\n",
    "# input_zh = Lang('zh', zh_word2id, zh_id2words, zh_ordered_words)\n",
    "# output_zh_en = Lang('en', en_word2id, en_id2words, en_ordered_words)\n",
    "input_vi = Lang('vi', vi_word2id, vi_id2words, vi_ordered_words)\n",
    "output_vi_en = Lang('en', en_word2id, en_id2words, en_ordered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines:\n",
      "Read 133317 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 30768\n",
      "en 41271\n",
      "Reading lines:\n",
      "Read 1268 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 30916\n",
      "en 41434\n",
      "Reading lines:\n",
      "Read 1553 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 31057\n",
      "en 41598\n"
     ]
    }
   ],
   "source": [
    "#Create the string pairs and the string lists\n",
    "# train_zh_pairs, zh_train, zh_en_train = prepareData(input_zh, output_zh_en, 'train')\n",
    "# val_zh_pairs, zh_val, zh_en_val = prepareData(input_zh, output_zh_en, 'dev')\n",
    "# test_zh_pairs, zh_test, zh_en_test = prepareData(input_zh, output_zh_en, 'test')\n",
    "\n",
    "train_vi_pairs, vi_train, vi_en_train = prepareData(input_vi, output_vi_en, 'train')\n",
    "val_vi_pairs, vi_val, vi_en_val = prepareData(input_vi, output_vi_en, 'dev')\n",
    "test_vi_pairs, vi_test, vi_en_test = prepareData(input_vi, output_vi_en, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no ngoai tam kiem_soat cua em , va no that tuyet , nhung đo khong phai la mot con đuong su_nghiep .',\n",
       " 'it s out of your control and it s awesome and it s not a career path .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(val_vi_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zh_idx_train, zh_en_idx_train = indexesFromSentences(input_zh, output_zh_en, train_zh_pairs)\n",
    "# zh_idx_val, zh_en_idx_val = indexesFromSentences(input_zh, output_zh_en, val_zh_pairs)\n",
    "# zh_idx_test, zh_en_idx_test = indexesFromSentences(input_zh, output_zh_en, test_zh_pairs)\n",
    "\n",
    "vi_idx_train, vi_en_idx_train = indexesFromSentences(input_vi, output_vi_en, train_vi_pairs)\n",
    "vi_idx_val, vi_en_idx_val = indexesFromSentences(input_vi, output_vi_en, val_vi_pairs)\n",
    "vi_idx_test, vi_en_idx_test = indexesFromSentences(input_vi, output_vi_en, test_vi_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zh_train_pairs = [[zh_idx_train[i], zh_en_idx_train[i]] for i in range(len(zh_idx_train))]\n",
    "# zh_val_pairs = [[zh_idx_val[i], zh_en_idx_val[i]] for i in range(len(zh_idx_val))]\n",
    "# zh_test_pairs= [[zh_idx_test[i], zh_en_idx_test[i]] for i in range(len(zh_idx_test))]\n",
    "vi_train_pairs = [[vi_idx_train[i], vi_en_idx_train[i]] for i in range(len(vi_idx_train))]\n",
    "vi_val_pairs = [[vi_idx_val[i], vi_en_idx_val[i]] for i in range(len(vi_idx_val))]\n",
    "vi_test_pairs = [[vi_idx_test[i], vi_en_idx_test[i]] for i in range(len(vi_idx_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1268"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# pkl.dump(zh_train_pairs, open('./data/zh_train_pairs.p', 'wb'))\n",
    "# pkl.dump(zh_val_pairs, open('./data/zh_val_pairs.p', 'wb'))\n",
    "# pkl.dump(zh_test_pairs, open('./data/zh_test_pairs.p', 'wb'))\n",
    "\n",
    "# pkl.dump(vi_train_pairs, open('./data/vi_train_pairs.p', 'wb'))\n",
    "# pkl.dump(vi_val_pairs, open('./data/vi_val_pairs.p', 'wb'))\n",
    "# pkl.dump(vi_test_pairs, open('./data/vi_test_pairs.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 104\n"
     ]
    }
   ],
   "source": [
    "max_val_len = 0\n",
    "second_len = 0\n",
    "for pair in vi_val_pairs:\n",
    "    if max_val_len < len(pair[0]):\n",
    "        second_len = max_val_len\n",
    "        max_val_len = len(pair[0])\n",
    "\n",
    "print(max_val_len, second_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training data, we have max-len as 100: train: 133038/133316, val: 1268/1268, test: 1553/1553\n",
    "#For training data, we have max-len as 80: train: 132789/133316, val: 1267/1268, test: 1552/1553\n",
    "vi_train_pairs_cleaned= []\n",
    "vi_val_pairs_cleaned = []\n",
    "vi_test_pairs_cleaned = []\n",
    "MAX_LENGTH = 80\n",
    "for vi_list in vi_train_pairs:\n",
    "    if len(vi_list[0])<=MAX_LENGTH and len(vi_list[1]) <= MAX_LENGTH:\n",
    "        vi_train_pairs_cleaned.append(vi_list)\n",
    "        \n",
    "for vi_list in vi_val_pairs:\n",
    "    if len(vi_list[0])<=MAX_LENGTH and len(vi_list[1]) <= MAX_LENGTH:\n",
    "        vi_val_pairs_cleaned.append(vi_list)\n",
    "\n",
    "for vi_list in vi_test_pairs:\n",
    "    if len(vi_list[0])<=MAX_LENGTH and len(vi_list[1]) <= MAX_LENGTH:\n",
    "        vi_test_pairs_cleaned.append(vi_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133166"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_train_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132318"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_train_pairs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1268"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1262"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_val_pairs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1553"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vi_test_pairs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# # pkl.dump(zh_train_pairs_cleaned, open('./data/zh_train_pairs_cleaned.p', 'wb'))\n",
    "# # pkl.dump(zh_val_pairs_cleaned, open('./data/zh_val_pairs_cleaned.p', 'wb'))\n",
    "# # pkl.dump(zh_test_pairs_cleaned, open('./data/zh_test_pairs_cleaned.p', 'wb'))\n",
    "\n",
    "# pkl.dump(vi_train_pairs_cleaned, open('./data/vi_train_pairs_cleaned.p', 'wb'))\n",
    "# pkl.dump(vi_val_pairs_cleaned, open('./data/vi_val_pairs_cleaned.p', 'wb'))\n",
    "# pkl.dump(vi_test_pairs_cleaned, open('./data/vi_test_pairs_cleaned.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# #loading data\n",
    "# # zh_train_pairs_cleaned = pkl.load(open('./data/zh_train_pairs_cleaned.p', 'rb'))\n",
    "# # zh_val_pairs_cleaned = pkl.load(open('./data/zh_val_pairs_cleaned.p', 'rb'))\n",
    "# # zh_test_pairs_cleaned = pkl.load(open('./data/zh_test_pairs_cleaned.p', 'rb'))\n",
    "\n",
    "# vi_train_pairs_cleaned = pkl.load(open('./data/vi_train_pairs_cleaned.p', 'rb'))\n",
    "# vi_val_pairs_cleaned = pkl.load(open('./data/vi_val_pairs_cleaned.p', 'rb'))\n",
    "# vi_test_pairs_cleaned = pkl.load(open('./data/vi_test_pairs_cleaned.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs):#Needs the index pairs\n",
    "        self.pairs = pairs\n",
    "#         self.input_lang = input_lang\n",
    "#         self.output_lang = output_lang\n",
    "        self.input_seqs = [pairs[i][0] for i in range(len(self.pairs))]\n",
    "        self.output_seqs = [pairs[i][1] for i in range(len(self.pairs))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)#Returning number of pairs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.input_seqs[index]\n",
    "        output_seq = self.output_seqs[index]\n",
    "        return [input_seq, len(input_seq), output_seq, len(output_seq)]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    #Reference: lab8_3_mri\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "#         padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        padded_seqs = torch.zeros(len(seqs), MAX_LENGTH).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    batch_input_seqs = [datum[0] for datum in batch]\n",
    "    batch_output_seqs = [datum[2] for datum in batch]\n",
    "    #batch_input_length = [datum[1] for datum in batch]\n",
    "    #batch_output_length = [datum[3] for datum in batch]\n",
    "\n",
    "    sorted_pairs = sorted(zip(batch_input_seqs, batch_output_seqs), key=lambda x: len(x[0]), reverse = True)\n",
    "    in_seq_sorted, out_seq_sorted = zip(*sorted_pairs)\n",
    "    \n",
    "    padded_input,input_lens = _pad_sequences(in_seq_sorted)\n",
    "    padded_output,output_lens = _pad_sequences(out_seq_sorted)\n",
    "    \n",
    "    input_list = torch.from_numpy(np.array(padded_input))\n",
    "    input_length = torch.LongTensor(input_lens)\n",
    "    output_list = torch.from_numpy(np.array(padded_output))\n",
    "    output_length = torch.LongTensor(output_lens)\n",
    "    \n",
    "    if CUDA:\n",
    "        input_list = input_list.cuda()\n",
    "        output_list = output_list.cuda()\n",
    "        input_length = input_length.cuda()\n",
    "        output_length = output_length.cuda()\n",
    "            \n",
    "    return [input_list, input_length, output_list, output_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "'''\n",
    "NMTDataset needs index pairs, need to call indexesFromPairs functions beforehand\n",
    "The dataLoader is sorted according to length of the input_length, and padded to\n",
    "max length of input and output list repectively\n",
    "TODO: output_list is not sorted, hence need to sort (maybe) in the rnn sequence.\n",
    "'''\n",
    "# train_zh_dataset = NMTDataset(zh_train_pairs_cleaned, input_zh, output_zh_en)\n",
    "# train_vi_dataset = NMTDataset(vi_train_pairs_cleaned, input_vi, output_vi_en)\n",
    "# val_zh_dataset = NMTDataset(zh_val_pairs_cleaned, input_zh, output_zh_en)\n",
    "# val_vi_dataset = NMTDataset(vi_val_pairs_cleaned, input_vi, output_vi_en)\n",
    "# test_zh_dataset = NMTDataset(zh_test_pairs_cleaned, input_zh, output_zh_en)\n",
    "# test_vi_dataset = NMTDataset(vi_test_pairs_cleaned, input_vi, output_vi_en)\n",
    "\n",
    "# train_zh_dataset = NMTDataset(zh_train_pairs_cleaned)\n",
    "train_vi_dataset = NMTDataset(vi_train_pairs_cleaned)\n",
    "# val_zh_dataset = NMTDataset(zh_val_pairs_cleaned)\n",
    "val_vi_dataset = NMTDataset(vi_val_pairs_cleaned)\n",
    "# test_zh_dataset = NMTDataset(zh_test_pairs_cleaned)\n",
    "test_vi_dataset = NMTDataset(vi_test_pairs_cleaned)\n",
    "\n",
    "\n",
    "# train_zh_loader = torch.utils.data.DataLoader(dataset = train_zh_dataset, \n",
    "#                                           batch_size = BATCH_SIZE,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = True)\n",
    "\n",
    "train_vi_loader = torch.utils.data.DataLoader(dataset = train_vi_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "\n",
    "#Will use batch size 1 for validation and test since the sentence will be translated one by one\n",
    "# val_zh_loader = torch.utils.data.DataLoader(dataset = val_zh_dataset, \n",
    "#                                           batch_size = 1,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = False)\n",
    "val_vi_loader = torch.utils.data.DataLoader(dataset = val_vi_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "# test_zh_loader = torch.utils.data.DataLoader(dataset = test_zh_dataset, \n",
    "#                                           batch_size = 1,\n",
    "#                                           collate_fn = vocab_collate_func,\n",
    "#                                           shuffle = False)\n",
    "test_vi_loader = torch.utils.data.DataLoader(dataset = test_vi_dataset, \n",
    "                                          batch_size = 1,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = False)\n",
    "#Input_batch in size Batch x maxLen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (input_list, input_length, output_list, output_length) in enumerate(val_zh_loader):\n",
    "#     if i== 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "# MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 256\n",
    "max_length = 10\n",
    "BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n",
    "CLIP = 50\n",
    "TEACHER_RATIO = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "#loading data\n",
    "# loaded_zh_embeddings = pkl.load(open(ftdir+'zh_embeddings.p', 'rb'))\n",
    "loaded_vi_embeddings = pkl.load(open(ftdir+'vi_embeddings.p', 'rb'))\n",
    "loaded_en_embeddings = pkl.load(open(ftdir+'en_embeddings.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA = False\n",
    "# loaded_zh_embeddings = torch.from_numpy(loaded_zh_embeddings).float()\n",
    "loaded_vi_embeddings = torch.from_numpy(loaded_vi_embeddings).float()\n",
    "loaded_en_embeddings = torch.from_numpy(loaded_en_embeddings).float()\n",
    "\n",
    "if CUDA:\n",
    "#     loaded_zh_embeddings = loaded_zh_embeddings.cuda()\n",
    "    loaded_vi_embeddings = loaded_vi_embeddings.cuda()\n",
    "    loaded_en_embeddings = loaded_en_embeddings.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_mask(length_list):\n",
    "    max_length = length_list.max().item()\n",
    "    masked_sentences = []\n",
    "    longest_sentence = [1]*max_length\n",
    "    for i in range(len(length_list)):\n",
    "        curr_length = length_list[i].item()\n",
    "        masked_sentence = [1]*max_length\n",
    "        masked_sentence[curr_length:] = [0] * (max_length - curr_length)\n",
    "        masked_sentences.append(masked_sentence)\n",
    "    if CUDA:\n",
    "        masked_sentences = torch.from_numpy(np.array(masked_sentences)).cuda()\n",
    "    else:\n",
    "        masked_sentences = torch.from_numpy(np.array(masked_sentences))\n",
    "    return masked_sentences\n",
    "        \n",
    "def rnn_mask_loss(decoder_outputs, output_list, output_length):\n",
    "    '''\n",
    "    decoder_outputs: 3d matrix containing all decoder output(B x output_lang vocab size)\n",
    "                    while decoder_output is in size(max_len x vocab_size)\n",
    "    output_list: Batch x max_len\n",
    "    output_length: batch\n",
    "    '''\n",
    "    batch_size, max_len = output_list.size()\n",
    "    decoder_outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))#(bxmax_len) x V\n",
    "    output_list = output_list.view(-1, 1)# (b x max_len) x 1 \n",
    "    neg_loss = -torch.gather(decoder_outputs, 1, output_list)#(b x max_len) x 1\n",
    "    neg_loss = neg_loss.view(batch_size, -1)# restore to b x max_len\n",
    "    \n",
    "    mask = rnn_mask(output_length)#b x max_len\n",
    "    mask_loss = neg_loss * mask.float()\n",
    "    \n",
    "    loss = mask_loss.sum() / output_length.float().sum()\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreBatchEncoderRNN(nn.Module):\n",
    "    def __init__(self, emb, emb_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(PreBatchEncoderRNN, self).__init__()\n",
    "        \n",
    "        #self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        #self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(self.emb, False, False)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout=self.dropout, bidirectional = True, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        '''\n",
    "        input_seqs in size B x L sorted in decreasing order -> will transpose to fit in embedding dimension\n",
    "        '''\n",
    "        self.batch_size = input_seqs.size(0)\n",
    "        #embedded size: max_len x B x H\n",
    "        embedded = self.embedding(input_seqs.transpose(0,1))#input_seqs B x L -> transpose to L x B\n",
    "        \n",
    "        #Input length sorted by loader\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        #Outputs in shape L x B x 2H, hidden as the last state of the GRU\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        #outputs L x B x H\n",
    "        #hidden size (2*n_layers) x B x H\n",
    "\n",
    "        #outputs: seq_len x Batch x H\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        #Due to bidrectional will have self.n_layers * 2\n",
    "        return torch.zeros(self.n_layers *2, batch_size, self.hidden_size,device = device)#hidden size 2lays *B*H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Example of encoder:\n",
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size).to(device)\n",
    "# encoder_outputs, encoder_hidden = pre_encoder(input_list, input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deodcer w/o attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN decoder with no attention used, batch implemented\n",
    "# RNN decoder take one token at a time\n",
    "class PreDecoderRNN(nn.Module):\n",
    "    def __init__(self, emb, emb_size, hidden_size, output_size, n_layers=1, dropout_p = 0.1):\n",
    "        super(PreDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "        #self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb, False, False)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        self.batch_size = input.size(0)\n",
    "        embedded = self.embedding(input).view(1, self.batch_size, -1)# 1 x B x E\n",
    "        embedded = self.dropout(embedded)\n",
    "        output = F.relu(embedded)\n",
    "        output, hidden = self.gru(output, hidden)#output 1 x B x E, hidden n_layers x B x H\n",
    "        out = self.out(output[0])\n",
    "        out = self.softmax(out)\n",
    "        #out size batch x output_lang_vocab_size\n",
    "        #hidden n_layers x B x H\n",
    "        return out, hidden\n",
    "    \n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_attn_decoder = PreDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, output_zh_en.n_words).to(device)\n",
    "# decoder_input = torch.tensor([[SOS_TOKEN]]*encoder_hidden.size(1)).to(device)\n",
    "# decoder_hidden = encoder_hidden[:no_attn_decoder.n_layers]\n",
    "# decoder_output, decoder_hidden = no_attn_decoder(decoder_input, decoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reference: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "# class Attn(nn.Module):\n",
    "#     def __init__(self, method, hidden_size):\n",
    "#         super(Attn, self).__init__()\n",
    "        \n",
    "#         self.method = method\n",
    "#         self.hidden_size = hidden_size\n",
    "        \n",
    "#         if self.method == 'general':\n",
    "#             self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "#         elif self.method == 'concat':\n",
    "#             self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "#             self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         max_len = encoder_outputs.size(0)\n",
    "#         this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "#         # Create variable to store attention energies\n",
    "#         attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "#         if CUDA:\n",
    "#             attn_energies = attn_energies.cuda()\n",
    "\n",
    "#         # For each batch of encoder outputs\n",
    "#         for b in range(this_batch_size):\n",
    "#             # Calculate energy for each encoder output\n",
    "#             for i in range(max_len):\n",
    "#                 attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "#         # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "#         return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "#     def score(self, hidden, encoder_output):\n",
    "#         hidden = hidden.squeeze()\n",
    "#         encoder_output = encoder_output.squeeze()\n",
    "# #         print(hidden.size())\n",
    "# #         print(encoder_output.size())\n",
    "#         if self.method == 'dot':\n",
    "#             energy = hidden.dot(encoder_output)\n",
    "#             return energy\n",
    "        \n",
    "#         elif self.method == 'general':\n",
    "#             energy = self.attn(encoder_output)\n",
    "#             energy = hidden.dot(energy)\n",
    "#             return energy\n",
    "#         elif self.method == 'concat':\n",
    "#             energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "#             energy = self.v.dot(energy)\n",
    "#             return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n",
    "#Reference: lab8 1_nmt, lab8 3_mri\n",
    "class PreAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, emb, emb_size, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(PreAttnDecoderRNN, self).__init__()\n",
    "        self.emb = emb\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size#vocab size of the output lang\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.emb, False, False,)\n",
    "        \n",
    "        #self.attn = nn.Linear(hidden_size*, hidden_size)\n",
    "        #self.attn2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        self.concat = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        self.batch_size = encoder_outputs.size(1)\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        attn_energies = Variable(torch.zeros(self.batch_size, max_len))#B X max_len\n",
    "        attn_energies = attn_energies.cuda() if CUDA else attn_energies\n",
    "        \n",
    "        \n",
    "        embedded = self.embedding(word_input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = embedded.view(1, self.batch_size, -1) # S=1 x B x N\n",
    "        \n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        #rnn layer x batch x h\n",
    "        #encoder-outputs  max_len x batch x h\n",
    "        \n",
    "#         for b in range(self.batch_size):\n",
    "#             # Calculate energy for each encoder output\n",
    "#             for i in range(max_len):\n",
    "#                 attn_energies[b, i] = (rnn_output[:, b].squeeze()).dot(encoder_outputs[i, b])\n",
    "        \n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        #More efficient\n",
    "        attn_energies = ((rnn_output.transpose(0,1)).bmm(encoder_outputs.transpose(0,1).transpose(1,2))).squeeze(1)\n",
    "        attn_weights = F.softmax(attn_energies) # B x max_len\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "        \n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "        output = F.log_softmax(output)\n",
    "\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RNN decoder with no attention used, batch implemented\n",
    "# # RNN decoder take one token at a time\n",
    "# class PreDecoderRNN(nn.Module):\n",
    "#     def __init__(self, emb, emb_size, hidden_size, output_size, n_layers=1, dropout_p = 0.1):\n",
    "#         super(PreDecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.emb_size = emb_size\n",
    "#         #self.batch_size = batch_size\n",
    "#         self.output_size = output_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.dropout = nn.Dropout(dropout_p)\n",
    "#         self.embedding = nn.Embedding.from_pretrained(emb, False, False)\n",
    "#         self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "#     def forward(self, input, hidden):\n",
    "#         self.batch_size = input.size(0)\n",
    "#         embedded = self.embedding(input).view(1, self.batch_size, -1)# 1 x B x E\n",
    "#         embedded = self.dropout(embedded)\n",
    "#         output = F.relu(embedded)\n",
    "#         output, hidden = self.gru(output, hidden)#output 1 x B x E, hidden n_layers x B x H\n",
    "#         out = self.out(output[0])\n",
    "#         out = self.softmax(out)\n",
    "#         #out size batch x output_lang_vocab_size\n",
    "#         #hidden n_layers x B x H\n",
    "#         return out, hidden\n",
    "    \n",
    "# #     def initHidden(self):\n",
    "# #         return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device = device)\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size=emb_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(loaded_vi_embeddings, freeze=True)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "#         packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "#         output, hidden = self.gru(packed, hidden)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH, embed_size=emb_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(loaded_vi_embeddings, freeze=False)\n",
    "        self.attn = nn.Linear(hidden_size + embed_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded, hidden), 2)), dim=2)\n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "encoder = EncoderRNN(hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(vi_ordered_words)).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for i, (input_list,input_length,output_list, output_length) in enumerate(train_vi_loader):\n",
    "    batch_size, max_input_length = input_list.size()\n",
    "    max_output_length = output_list.size(1)\n",
    "            \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_output, encoder_hidden = encoder(input_list, encoder_hidden)\n",
    "#     encoder_output, encoder_hidden = batch_encoder(input_list, input_length, encoder_hidden)\n",
    "    \n",
    "    decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "#     decoder_input = torch.tensor([[SOS_TOKEN]]*batch_size, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "#     decoder_hidden = encoder_hidden[:batch_decoder.n_layers]\n",
    "\n",
    "    loss = 0\n",
    "    for di in range(max_output_length):\n",
    "\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output)\n",
    "\n",
    "\n",
    "        loss += criterion(decoder_output, output_list[:,di])\n",
    "        decoder_input = output_list[:,di].unsqueeze(0) \n",
    "    loss.backward()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced from lab8 1nmt and modified \n",
    "teacher_forcing_ratio = 0.5\n",
    "def attn_batch_train(input_list, input_length, output_list,output_length, \n",
    "                batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    '''\n",
    "    param: @attention is a Boolean variable indicating whether using attention\n",
    "    '''\n",
    "    batch_encoder.train()\n",
    "    batch_decoder.train()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    batch_size, max_input_length = input_list.size()\n",
    "    max_output_length = output_list.size(1)\n",
    "    \n",
    "    batch_size = input_list.size(0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    encoder_hidden = batch_encoder.initHidden(batch_size)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = batch_encoder(input_list, encoder_hidden)\n",
    "\n",
    "    #Initialize for decoding process\n",
    "    curr_batch = input_list.size(0)#Take the current batch size\n",
    "    decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "#     decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = output_list[:,di].unsqueeze(0)\n",
    "            loss += criterion(decoder_output, output_list[:,di])\n",
    "\n",
    "    else:\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "            loss += criterion(decoder_output, output_list[:,di])\n",
    "            \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "learning_rate = 0.01\n",
    "\n",
    "encoder = EncoderRNN(hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(vi_ordered_words)).to(device)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for i, (input_list,input_length,output_list, output_length) in enumerate(train_vi_loader):\n",
    "    loss = attn_batch_train(input_list, input_length, output_list, output_length, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Referenced from lab8 1nmt and modified \n",
    "# teacher_forcing_ratio = 0.5\n",
    "# def attn_batch_train(input_list, input_length, output_list,output_length, \n",
    "#                 batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "#     '''\n",
    "#     param: @attention is a Boolean variable indicating whether using attention\n",
    "#     '''\n",
    "#     batch_encoder.train()\n",
    "#     batch_decoder.train()\n",
    "    \n",
    "#     encoder_optimizer.zero_grad()\n",
    "#     decoder_optimizer.zero_grad()\n",
    "#     max_output_length = output_length.max().item()\n",
    "    \n",
    "#     batch_size = input_list.size(0)\n",
    "    \n",
    "#     loss = 0\n",
    "    \n",
    "#     encoder_hidden = batch_encoder.initHidden(batch_size)\n",
    "\n",
    "#     encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, encoder_hidden)\n",
    "\n",
    "#     #Initialize for decoding process\n",
    "#     curr_batch = input_list.size(0)#Take the current batch size\n",
    "#     decoder_input = torch.tensor([[SOS_TOKEN]]*curr_batch, device=device)\n",
    "    \n",
    "#     decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "# #     decoder_outputs = torch.zeros(max_output_length, curr_batch, batch_decoder.output_size)\n",
    "    \n",
    "#     # Move new Variables to CUDA\n",
    "#     if CUDA:\n",
    "#         decoder_input = decoder_input.cuda()\n",
    "# #         decoder_outputs = decoder_outputs.cuda()\n",
    "    \n",
    "#     use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "# #     use_teacher_forcing = True\n",
    "    \n",
    "#     if use_teacher_forcing:\n",
    "#     # Teacher forcing: Feed the target as the next input\n",
    "#         for di in range(max_output_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "# #             decoder_outputs[di] = decoder_output\n",
    "#             decoder_input = output_list[:,di] # Teacher forcing\n",
    "#             loss += criterion(decoder_output, output_list[:,di])\n",
    "\n",
    "#     else:\n",
    "#     # Without teacher forcing: use its own predictions as the next input\n",
    "#         for di in range(max_output_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "# #             decoder_outputs[di] = decoder_input\n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.squeeze().detach()# detach from history as input: size batch x 1 \n",
    "#             loss += criterion(decoder_output, output_list[:,di])\n",
    "            \n",
    "# #     loss += rnn_mask_loss(decoder_outputs.transpose(0,1).contiguous(), output_list.contiguous(), output_length)\n",
    "            \n",
    "#     loss.backward()\n",
    "#     ec = torch.nn.utils.clip_grad_norm(batch_encoder.parameters(), CLIP)\n",
    "#     dc = torch.nn.utils.clip_grad_norm(batch_decoder.parameters(), CLIP)\n",
    "\n",
    "#     encoder_optimizer.step()\n",
    "#     decoder_optimizer.step()\n",
    "\n",
    "#     return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.SGD(pre_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE)\n",
    "# pre_decoder = PreDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE)\n",
    "# encoder_outputs, encoder_hidden = pre_encoder(input_list, input_length)\n",
    "# decoder_input = torch.tensor([[SOS_TOKEN]]*BATCH_SIZE)\n",
    "# decoder_hidden = encoder_hidden[:no_attn_decoder.n_layers]\n",
    "# max_output_length = output_length.max().item()\n",
    "# decoder_outputs = torch.zeros(max_output_length, curr_batch, pre_decoder.output_size)\n",
    "# loss = 0\n",
    "# for di in range(max_output_length):\n",
    "#     #print(di)\n",
    "#     decoder_output, decoder_hidden = pre_decoder(\n",
    "#         decoder_input, decoder_hidden)\n",
    "#     decoder_outputs[di] = decoder_output\n",
    "#     decoder_input = output_list[:,di] # Teacher forcing\n",
    "#     loss += criterion(decoder_output, output_list[:,di])\n",
    "# print(loss.item()/max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE).to(device)\n",
    "# attn_decoder = PreAttnDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE)\n",
    "\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.SGD(attn_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# encoder_outputs, encoder_hidden = pre_encoder(input_list, input_length)\n",
    "# decoder_input = torch.tensor([[SOS_TOKEN]]*BATCH_SIZE)\n",
    "# decoder_hidden = encoder_hidden[:attn_decoder.n_layers]\n",
    "# max_output_length = output_length.max().item()\n",
    "# decoder_outputs = torch.zeros(max_output_length, curr_batch, attn_decoder.output_size)\n",
    "# loss = 0\n",
    "# for di in range(max_output_length):\n",
    "#     #print(di)\n",
    "#     decoder_output, decoder_hidden, attn_weights = attn_decoder(\n",
    "#         decoder_input, decoder_hidden, encoder_outputs)\n",
    "#     decoder_outputs[di] = decoder_output\n",
    "#     decoder_input = output_list[:,di] # Teacher forcing\n",
    "#     loss += criterion(decoder_output, output_list[:,di])\n",
    "# print(loss.item()/max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE).to(device)\n",
    "# pre_decoder = PreDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE)\n",
    "\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.SGD(pre_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss = no_attn_batch_train(input_list, input_length, output_list, output_length, \n",
    "#                        pre_encoder, pre_decoder, encoder_optimizer, decoder_optimizer, \n",
    "#                        criterion)\n",
    "\n",
    "# print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE).to(device)\n",
    "# attn_decoder = PreAttnDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE)\n",
    "\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.SGD(attn_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# loss = attn_batch_train(input_list, input_length, output_list, output_length, \n",
    "#                        pre_encoder, attn_decoder, encoder_optimizer, decoder_optimizer, \n",
    "#                        criterion)\n",
    "\n",
    "# print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE).to(device)\n",
    "# pre_decoder = PreDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE).to(device)\n",
    "\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.SGD(pre_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.SGD(pre_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train_loss = []\n",
    "# for i in range(2000):\n",
    "#     loss = no_attn_batch_train(input_list, input_length, output_list, output_length, \n",
    "#                        pre_encoder, pre_decoder, encoder_optimizer, decoder_optimizer, \n",
    "#                        criterion)\n",
    "#     train_loss.append(loss)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (12,10))\n",
    "# ax.plot(train_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_encoder = PreBatchEncoderRNN(loaded_zh_embeddings, emb_size, hidden_size, BATCH_SIZE).to(device)\n",
    "# attn_decoder = PreAttnDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), BATCH_SIZE).to(device)\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# encoder_optimizer = optim.Adam(pre_encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.Adam(attn_decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train_loss = []\n",
    "# for i in range(2000):\n",
    "#     loss = attn_batch_train(input_list, input_length, output_list, output_length, \n",
    "#                        pre_encoder, attn_decoder, encoder_optimizer, decoder_optimizer, \n",
    "#                        criterion)\n",
    "#     train_loss.append(loss)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (12,10))\n",
    "# ax.plot(train_loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference lab8 1-nmt\n",
    "def greedy_attn_evaluate(val_loader, encoder, decoder, en_id2words ):\n",
    "    #Will generate sentences 1 by 1. \n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    decoded_words_all = []\n",
    "    decoder_attentions_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        for i, (input_list, input_length, output_list, output_length) in enumerate(val_loader):\n",
    "            if i %100 == 0:\n",
    "                print(\"%d/%d\"%(i,len(val_loader)))\n",
    "                \n",
    "            batch_size, max_input_length = input_list.size()\n",
    "            max_output_length = output_list.size(1)\n",
    "            \n",
    "            #    break\n",
    "            #batch_size, max_len = output_list.size()\n",
    "#             print(input_list.size())\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(batch_size)\n",
    "            encoder_outputs, encoder_hidden = encoder(input_list, encoder_hidden)\n",
    "\n",
    "            decoder_input = torch.tensor(np.array([[SOS_TOKEN]] * batch_size), device=device)\n",
    "#             decoder_input = torch.tensor([[SOS_TOKEN]], device=device)  # SOS\n",
    "            # decode the context vector\n",
    "            decoder_hidden = encoder_hidden\n",
    "#             decoder_hidden = encoder_hidden[:decoder.n_layers] # decoder starts from the last encoding sentence\n",
    "            # output of this function\n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                # for each time step, the decoder network takes two inputs: previous outputs and the previous hidden states\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input.reshape(1, batch_size), decoder_hidden, encoder_outputs)\n",
    "\n",
    "                top_score, topi = decoder_output.data.topk(1)\n",
    "                decoder_attentions[di, :decoder_attention.size(-1)] = decoder_attention\n",
    "                decoded_words.append(en_id2words[topi.item()])\n",
    "                if topi.item() == EOS_TOKEN:\n",
    "                    break\n",
    "                else:\n",
    "                    decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "                    \n",
    "            decoded_words_all.append(decoded_words)\n",
    "            decoder_attentions_all.append(decoder_attentions[:di+1])\n",
    "\n",
    "        return decoded_words_all, decoder_attentions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_words_all, decoder_attention_all = greedy_attn_evaluate(val_zh_loader, pre_encoder, attn_decoder, en_id2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(decoded_words_all):\n",
    "    cleaned_decoded_words_all = []\n",
    "    \n",
    "    for sentence in decoded_words_all:\n",
    "        cleaned_sentence = []\n",
    "        for word in sentence:\n",
    "            if word == '<PAD>':\n",
    "                continue\n",
    "            else:\n",
    "                cleaned_sentence.append(word)\n",
    "        if cleaned_sentence[-1] != '<EOS>':\n",
    "            cleaned_sentence.append(' <EOS>')\n",
    "            \n",
    "        cleaned_decoded_words_all.append(cleaned_sentence)\n",
    "        \n",
    "    return cleaned_decoded_words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate the test and val lists back to english\n",
    "def en_translate(index_list, en_id2words):\n",
    "    translated_sentence_list = []\n",
    "    for sentence in index_list:\n",
    "        translated_sentence = []\n",
    "        for index in sentence:\n",
    "            translated_sentence.append(en_id2words[index])\n",
    "        #translated_sentence.append('<EOS>')\n",
    "        translated_sentence_list.append(translated_sentence)\n",
    "    return translated_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_words_list = post_process(decoded_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_words_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'little',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'my',\n",
       " 'country',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'on',\n",
       " 'the',\n",
       " 'planet',\n",
       " 'and',\n",
       " 'i',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'singing',\n",
       " 'a',\n",
       " 'song',\n",
       " 'called',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'envy',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_en_val_list = [pair[1] for pair in vi_val_pairs_cleaned]\n",
    "translated_sentence_list = en_translate(vi_en_val_list, en_id2words)\n",
    "translated_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference LAB8 1-nmt\n",
    "model_path = './model/'\n",
    "def AttnTrainIters(train_loader, val_loader, encoder, decoder, n_iters, val_translated_list,\n",
    "                   print_every=100, plot_every=100, eval_every=500, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    epoch = 0\n",
    "    epoch_total = n_iters*len(train_loader)\n",
    "    \n",
    "    for iter in range(n_iters):\n",
    "        #print(\"Epoch {}/{}\".format(i+1, n_epochs))\n",
    "        losses = []\n",
    "        for i, (input_list,input_length,output_list, output_length) in enumerate(train_loader):\n",
    "            loss = attn_batch_train(input_list, input_length, output_list, output_length, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            \n",
    "#             if i > 0 and i % eval_every == 0:\n",
    "#                 decoded_val, decoder_attentions = greedy_attn_evaluate(val_loader, encoder, decoder, en_id2words)\n",
    "#                 decoded_clean = post_process(decoded_val)\n",
    "#                 print('bleu score is {}'.format(raw_corpus_bleu(decoded_val, val_translated_list).score))\n",
    "\n",
    "            if i > 0 and i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / epoch_total),\n",
    "                                             epoch, epoch / epoch_total * 100, print_loss_avg))\n",
    "\n",
    "            if i > 0 and i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "            epoch += 1\n",
    "            \n",
    "        torch.save(encoder.state_dict(), model_path + \"encoder_rnn_atten_\"+str(start)+\".pth\")\n",
    "        torch.save(decoder.state_dict(), model_path + \"decoder_rnn_atten_\"+str(start)+\".pth\")\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 29s (- 184m 29s) (100 0%) 121.4117\n",
      "2m 59s (- 182m 41s) (200 1%) 118.3706\n",
      "4m 28s (- 180m 41s) (300 2%) 115.8974\n",
      "5m 58s (- 179m 5s) (400 3%) 117.1765\n",
      "7m 26s (- 177m 17s) (500 4%) 116.4000\n",
      "8m 55s (- 175m 36s) (600 4%) 111.7102\n",
      "10m 24s (- 174m 0s) (700 5%) 110.5280\n",
      "11m 53s (- 172m 27s) (800 6%) 113.1928\n",
      "13m 21s (- 170m 52s) (900 7%) 112.2294\n",
      "14m 51s (- 169m 23s) (1000 8%) 110.9180\n",
      "16m 19s (- 167m 51s) (1100 8%) 109.3502\n",
      "17m 49s (- 166m 22s) (1200 9%) 112.1029\n",
      "19m 18s (- 164m 57s) (1300 10%) 108.7259\n",
      "20m 47s (- 163m 26s) (1400 11%) 107.5408\n",
      "22m 16s (- 161m 57s) (1500 12%) 109.7117\n",
      "23m 45s (- 160m 27s) (1600 12%) 108.9423\n",
      "25m 14s (- 158m 57s) (1700 13%) 107.5366\n",
      "26m 43s (- 157m 29s) (1800 14%) 109.9944\n",
      "28m 12s (- 155m 59s) (1900 15%) 108.4127\n",
      "29m 41s (- 154m 29s) (2000 16%) 106.5030\n",
      "31m 10s (- 152m 59s) (2100 16%) 104.1261\n",
      "32m 40s (- 151m 31s) (2200 17%) 107.5423\n",
      "34m 9s (- 150m 3s) (2300 18%) 108.4920\n",
      "35m 38s (- 148m 34s) (2400 19%) 106.9207\n",
      "37m 7s (- 147m 6s) (2500 20%) 109.2629\n",
      "38m 37s (- 145m 38s) (2600 20%) 105.8167\n",
      "40m 6s (- 144m 11s) (2700 21%) 105.9077\n",
      "41m 36s (- 142m 43s) (2800 22%) 104.8668\n",
      "43m 6s (- 141m 16s) (2900 23%) 104.7941\n",
      "44m 35s (- 139m 48s) (3000 24%) 103.1591\n",
      "46m 5s (- 138m 19s) (3100 24%) 104.2716\n",
      "47m 35s (- 136m 53s) (3200 25%) 107.5117\n",
      "49m 4s (- 135m 25s) (3300 26%) 103.4061\n",
      "50m 34s (- 133m 56s) (3400 27%) 104.7002\n",
      "52m 3s (- 132m 27s) (3500 28%) 102.2516\n",
      "53m 33s (- 130m 59s) (3600 29%) 104.3644\n",
      "55m 2s (- 129m 30s) (3700 29%) 101.3861\n",
      "56m 32s (- 128m 2s) (3800 30%) 101.8435\n",
      "58m 2s (- 126m 34s) (3900 31%) 107.5506\n",
      "59m 32s (- 125m 6s) (4000 32%) 106.3907\n",
      "61m 2s (- 123m 38s) (4100 33%) 103.6542\n",
      "63m 4s (- 121m 40s) (4235 34%) 139.2334\n",
      "64m 34s (- 120m 12s) (4335 34%) 100.5607\n",
      "66m 3s (- 118m 43s) (4435 35%) 101.8447\n",
      "67m 33s (- 117m 13s) (4535 36%) 99.4795\n",
      "69m 2s (- 115m 44s) (4635 37%) 99.4727\n",
      "70m 32s (- 114m 15s) (4735 38%) 102.0409\n",
      "72m 2s (- 112m 46s) (4835 38%) 103.0853\n",
      "73m 31s (- 111m 17s) (4935 39%) 98.7217\n",
      "75m 1s (- 109m 48s) (5035 40%) 101.3696\n",
      "76m 31s (- 108m 20s) (5135 41%) 102.9216\n",
      "78m 0s (- 106m 51s) (5235 42%) 98.2637\n",
      "79m 30s (- 105m 21s) (5335 43%) 99.7883\n",
      "80m 59s (- 103m 52s) (5435 43%) 98.9926\n",
      "82m 29s (- 102m 23s) (5535 44%) 100.0747\n",
      "83m 58s (- 100m 53s) (5635 45%) 98.7156\n",
      "85m 28s (- 99m 24s) (5735 46%) 101.3034\n",
      "86m 58s (- 97m 55s) (5835 47%) 100.8125\n",
      "88m 28s (- 96m 26s) (5935 47%) 98.9914\n",
      "89m 57s (- 94m 57s) (6035 48%) 98.8336\n",
      "91m 27s (- 93m 28s) (6135 49%) 99.7473\n",
      "92m 56s (- 91m 58s) (6235 50%) 99.3088\n",
      "94m 27s (- 90m 29s) (6335 51%) 101.0301\n",
      "95m 56s (- 89m 0s) (6435 51%) 98.2360\n",
      "97m 26s (- 87m 31s) (6535 52%) 98.5250\n",
      "98m 55s (- 86m 1s) (6635 53%) 96.1984\n",
      "100m 25s (- 84m 32s) (6735 54%) 99.2606\n",
      "101m 55s (- 83m 3s) (6835 55%) 101.0827\n",
      "103m 24s (- 81m 34s) (6935 55%) 98.0329\n",
      "104m 54s (- 80m 4s) (7035 56%) 96.0753\n",
      "106m 24s (- 78m 35s) (7135 57%) 101.3972\n",
      "107m 53s (- 77m 5s) (7235 58%) 96.6711\n",
      "109m 22s (- 75m 35s) (7335 59%) 93.8431\n",
      "110m 51s (- 74m 6s) (7435 59%) 97.6259\n",
      "112m 21s (- 72m 37s) (7535 60%) 97.3919\n",
      "113m 50s (- 71m 7s) (7635 61%) 96.2470\n",
      "115m 20s (- 69m 38s) (7735 62%) 99.3429\n",
      "116m 49s (- 68m 8s) (7835 63%) 99.4191\n",
      "118m 19s (- 66m 39s) (7935 63%) 98.2364\n",
      "119m 49s (- 65m 9s) (8035 64%) 97.4480\n",
      "121m 19s (- 63m 40s) (8135 65%) 100.0501\n",
      "122m 48s (- 62m 11s) (8235 66%) 97.3428\n",
      "124m 50s (- 60m 11s) (8370 67%) 131.1312\n",
      "126m 20s (- 58m 41s) (8470 68%) 97.2911\n",
      "127m 50s (- 57m 12s) (8570 69%) 92.0832\n",
      "129m 19s (- 55m 42s) (8670 69%) 97.4592\n",
      "130m 49s (- 54m 13s) (8770 70%) 95.9218\n",
      "132m 19s (- 52m 44s) (8870 71%) 94.9475\n",
      "133m 48s (- 51m 14s) (8970 72%) 93.9958\n",
      "135m 18s (- 49m 44s) (9070 73%) 97.2127\n",
      "136m 48s (- 48m 15s) (9170 73%) 96.6960\n",
      "138m 17s (- 46m 46s) (9270 74%) 93.9506\n",
      "139m 47s (- 45m 16s) (9370 75%) 96.3119\n",
      "141m 16s (- 43m 47s) (9470 76%) 94.3724\n",
      "142m 46s (- 42m 17s) (9570 77%) 94.0768\n",
      "144m 15s (- 40m 48s) (9670 77%) 93.8253\n",
      "145m 44s (- 39m 18s) (9770 78%) 93.4615\n",
      "147m 14s (- 37m 49s) (9870 79%) 95.9012\n",
      "148m 43s (- 36m 19s) (9970 80%) 92.2967\n",
      "150m 13s (- 34m 50s) (10070 81%) 94.6852\n",
      "151m 42s (- 33m 20s) (10170 81%) 94.3627\n",
      "153m 12s (- 31m 50s) (10270 82%) 95.2107\n",
      "154m 42s (- 30m 21s) (10370 83%) 95.5012\n",
      "156m 12s (- 28m 52s) (10470 84%) 97.7981\n",
      "157m 41s (- 27m 22s) (10570 85%) 94.6445\n",
      "159m 11s (- 25m 53s) (10670 86%) 94.7231\n",
      "160m 40s (- 24m 23s) (10770 86%) 95.0984\n",
      "162m 10s (- 22m 54s) (10870 87%) 93.9921\n",
      "163m 39s (- 21m 24s) (10970 88%) 94.9525\n",
      "165m 9s (- 19m 55s) (11070 89%) 96.3791\n",
      "166m 39s (- 18m 25s) (11170 90%) 96.0700\n",
      "168m 9s (- 16m 56s) (11270 90%) 95.8105\n",
      "169m 38s (- 15m 26s) (11370 91%) 95.2051\n",
      "171m 8s (- 13m 57s) (11470 92%) 95.9038\n",
      "172m 38s (- 12m 27s) (11570 93%) 95.2205\n",
      "174m 7s (- 10m 58s) (11670 94%) 93.9923\n",
      "175m 37s (- 9m 28s) (11770 94%) 94.5432\n",
      "177m 6s (- 7m 58s) (11870 95%) 97.4582\n",
      "178m 36s (- 6m 29s) (11970 96%) 91.3579\n",
      "180m 5s (- 4m 59s) (12070 97%) 93.9335\n",
      "181m 34s (- 3m 30s) (12170 98%) 93.1335\n",
      "183m 3s (- 2m 0s) (12270 98%) 94.2886\n",
      "184m 32s (- 0m 31s) (12370 99%) 88.4815\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'showPlot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-6e27e8f881a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m AttnTrainIters(train_vi_loader, val_vi_loader, encoder, attn_decoder, 3,\n\u001b[0;32m---> 10\u001b[0;31m                None, print_every=100, learning_rate=learning_rate)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-d912018ac402>\u001b[0m in \u001b[0;36mAttnTrainIters\u001b[0;34m(train_loader, val_loader, encoder, decoder, n_iters, val_translated_list, print_every, plot_every, eval_every, learning_rate)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"decoder_rnn_atten_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mshowPlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'showPlot' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "hidden_size = 300\n",
    "\n",
    "encoder = EncoderRNN(hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, len(vi_ordered_words)).to(device)\n",
    "# pre_encoder = PreBatchEncoderRNN(loaded_vi_embeddings, emb_size, hidden_size, train_vi_loader.batch_size).to(device)\n",
    "# attn_decoder = PreAttnDecoderRNN(loaded_en_embeddings, emb_size, hidden_size, len(en_ordered_words), train_vi_loader.batch_size).to(device)\n",
    "\n",
    "AttnTrainIters(train_vi_loader, val_vi_loader, encoder, attn_decoder, 3,\n",
    "               None, print_every=100, learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1262\n",
      "100/1262\n",
      "200/1262\n",
      "300/1262\n",
      "400/1262\n",
      "500/1262\n",
      "600/1262\n",
      "700/1262\n",
      "800/1262\n",
      "900/1262\n",
      "1000/1262\n",
      "1100/1262\n",
      "1200/1262\n"
     ]
    }
   ],
   "source": [
    "decoded_val, decoder_attentions = greedy_attn_evaluate(val_vi_loader, encoder, attn_decoder, en_id2words)\n",
    "decoded_clean = post_process(decoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu,raw_corpus_bleu\n",
    "\n",
    "def bleu_score(predicted_list,translated_list):\n",
    "    predicted_list_nopad = []\n",
    "    for ii in range(len(predicted_list)):\n",
    "        line = ''\n",
    "        for jj in predicted_list[ii]:\n",
    "            if jj != '<pad>':\n",
    "                line = line + ' ' + jj\n",
    "        predicted_list_nopad.append(line)\n",
    "    labels = []\n",
    "    for ii in range(len(translated_list)):\n",
    "        line = ''\n",
    "        for jj in translated_list[ii]:\n",
    "            if jj != '<pad>':\n",
    "                line = line + ' ' + jj\n",
    "        labels.append(line)\n",
    "    #print(len(labels))\n",
    "    #print(len(predicted_list_nopad))\n",
    "    print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [labels]).score)\n",
    "    print('bleu score for test dataset [raw]:', raw_corpus_bleu(predicted_list_nopad, [labels]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 13.244067757436477\n",
      "bleu score for test dataset [raw]: 3.1149446308128703\n"
     ]
    }
   ],
   "source": [
    "bleu_score(decoded_clean,translated_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so', 'there', 's', 'this', '.', 'this', '.', '.', '<EOS>']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(decoded_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = timer()\n",
    "i = 0\n",
    "for batch in train_zh_loader:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-80f084b89653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshowPlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_losses' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
