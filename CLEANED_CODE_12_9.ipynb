{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from queue import PriorityQueue\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "BATCH_SIZE = 3\n",
    "CUDA = False\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'PAD', 1: 'SOS', 2:'EOS', 3:'UNK'}#Dict\n",
    "        self.n_words = 4  # Count SOS and EOS +(batch: pad and unk)\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "#Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #This line is commented out since it will not properly deal with Chinese Letters\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "# def readLangs(lang1, lang2, reverse=False):\n",
    "#     print(\"Reading lines...\")\n",
    "\n",
    "#     # Read the file and split into lines\n",
    "#     lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "#         read().strip().split('\\n')\n",
    "\n",
    "#     # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "#     # Reverse pairs, make Lang instances\n",
    "#     if reverse:\n",
    "#         pairs = [list(reversed(p)) for p in pairs]\n",
    "#         input_lang = Lang(lang2)\n",
    "#         output_lang = Lang(lang1)\n",
    "#     else:\n",
    "#         input_lang = Lang(lang1)\n",
    "#         output_lang = Lang(lang2)\n",
    "\n",
    "#     return input_lang, output_lang, pairs\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] + [EOS_TOKEN]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# eng_prefixes = (\n",
    "#     \"i am \", \"i m \",\n",
    "#     \"he is\", \"he s \",\n",
    "#     \"she is\", \"she s\",\n",
    "#     \"you are\", \"you re \",\n",
    "#     \"we are\", \"we re \",\n",
    "#     \"they are\", \"they re \"\n",
    "# )\n",
    "\n",
    "\n",
    "# def filterPair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "#         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "#         p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "# def filterPairs(pairs):\n",
    "#     return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data for project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, category, reverse = False):#category = ['train', 'dev','test]\n",
    "    print('Reading lines:')\n",
    "    lines1 = open('data/iwslt-' + lang1 +'-en/' + category +'.tok.'+ lang1, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data1 = [normalizeString(l) for l in lines1]\n",
    "    \n",
    "    lines2 = open('data/iwslt-' + lang1 +'-en/' + category + '.tok.' + lang2, encoding = 'utf-8').\\\n",
    "    read().strip().split('\\n')\n",
    "    data2 = [normalizeString(l) for l in lines2]\n",
    "    #Given that data2 is english hence we further normalize\n",
    "    data2 = [re.sub(r\"[^a-zA-Z.!?]+\", r\" \", data) for data in data2]\n",
    "    print('Generating pairs')\n",
    "    print(len(data1), len(data2))\n",
    "    pairs = [[data1[i], data2[i]] for i in range(len(data1))]\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Preparation for CHN to ENG\n",
    "def prepareData(lang1, lang2, category, reverse = False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, category, reverse)\n",
    "    print('Read %s sentence pairs' % len(pairs))\n",
    "    #Build the vocabulary\n",
    "    print('Counting words')\n",
    "    #max_length = 0\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        #Get the maximum legnth \n",
    "        #pair_max = max(len(list(filter(None, pair[0].split(' ')))),\n",
    "          #             len(list(filter(None, pair[1].split(' ')))))\n",
    "        #max_length = max(pair_max, max_length)\n",
    "        \n",
    "    #Test for basic info about the data\n",
    "    print('Counted Words')\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/Hengyu/Desktop/MasterWork/1014/Neural-Machine-Translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines:\n",
      "Generating pairs\n",
      "213376 213376\n",
      "Read 213376 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 88426\n",
      "en 50970\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "1261 1261\n",
      "Read 1261 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 6133\n",
      "en 3671\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "1397 1397\n",
      "Read 1397 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "zh 5214\n",
      "en 3220\n",
      "['就是 这么 一小 小点 基因   小 而 卑劣 的 基因', 'it apos s that little gene . it apos s small and it apos s mean .']\n"
     ]
    }
   ],
   "source": [
    "input_zh, output_zh_en, train_zh_pairs = prepareData('zh', 'en', 'train')\n",
    "_,_, val_zh_pairs = prepareData('zh', 'en', 'dev')\n",
    "_,_, test_zh_pairs = prepareData('zh','en','test')\n",
    "print(random.choice(train_zh_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines:\n",
      "Generating pairs\n",
      "133317 133317\n",
      "Read 133317 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 30768\n",
      "en 41272\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "1268 1268\n",
      "Read 1268 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 3050\n",
      "en 3572\n",
      "Reading lines:\n",
      "Generating pairs\n",
      "1553 1553\n",
      "Read 1553 sentence pairs\n",
      "Counting words\n",
      "Counted Words\n",
      "vi 2899\n",
      "en 3408\n",
      "['đung , co ay noi nhu the  . ok , đieu đo co nghia_la chung_ta moi đi đuoc nua cau_chuyen  .', 'yes she said something like that . ok that means we are at half of the story .']\n"
     ]
    }
   ],
   "source": [
    "input_vi, output_vi_en, train_vi_pairs = prepareData('vi', 'en', 'train')\n",
    "_,_, val_vi_pairs = prepareData('vi', 'en', 'dev')\n",
    "_,_, test_vi_pairs = prepareData('vi','en','test')\n",
    "print(random.choice(train_vi_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vi_vay chung_ta nen dung ngay su u_me nay lai , dung su tho_o , dung su ky_thi che_nhao , va dung su im_lang nay , va pha_bo nhung đieu cam ky , nhin thang vao su_that , va bat_đau tro_chuyen , boi_vi cach duy_nhat đe đanh_bai mot van_đe ma ca_nhan moi nguoi phai tu minh chien_đau đo la cung manh_me vung_vang đung_lai gan nhau , cung manh_me vung_vang đung_lai gan nhau  .',\n",
       " 'so we need to stop the ignorance stop the intolerance stop the stigma and stop the silence and we need to take away the taboos take a look at the truth and start talking because the only way we apos re going to beat a problem that people are battling alone is by standing strong together by standing strong together .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vi_pairs[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['作为 领导 领导人   我们 不可 可能 总是 对 的', 'we apos re not always right as leaders .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_zh_pairs[419]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, pairs, input_lang, output_lang):#Needs the index pairs\n",
    "        self.pairs = pairs\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.input_seqs = [pairs[i][0] for i in range(len(self.pairs))]\n",
    "        self.output_seqs = [pairs[i][1] for i in range(len(self.pairs))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)#Returning number of pairs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.input_seqs[index]\n",
    "        output_seq = self.output_seqs[index]\n",
    "        return [input_seq, len(input_seq), output_seq, len(output_seq)]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    #Reference: lab8_3_mri\n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "        padded_seqs = torch.zeros(len(seqs), max(lens)).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            end = lens[i]\n",
    "            padded_seqs[i, :end] = torch.LongTensor(seq[:end])\n",
    "        return padded_seqs, lens\n",
    "    \n",
    "    batch_input_seqs = [datum[0] for datum in batch]\n",
    "    batch_output_seqs = [datum[2] for datum in batch]\n",
    "    #batch_input_length = [datum[1] for datum in batch]\n",
    "    #batch_output_length = [datum[3] for datum in batch]\n",
    "\n",
    "    sorted_pairs = sorted(zip(batch_input_seqs, batch_output_seqs), key=lambda x: len(x[0]), reverse = True)\n",
    "    in_seq_sorted, out_seq_sorted = zip(*sorted_pairs)\n",
    "    \n",
    "    padded_input,input_lens = _pad_sequences(in_seq_sorted)\n",
    "    padded_output,output_lens = _pad_sequences(out_seq_sorted)\n",
    "    \n",
    "    input_list = torch.from_numpy(np.array(padded_input))\n",
    "    input_length = torch.LongTensor(input_lens)\n",
    "    output_list = torch.from_numpy(np.array(padded_output))\n",
    "    output_length = torch.LongTensor(output_lens)\n",
    "    \n",
    "    if CUDA:\n",
    "        input_list = input_list.cuda()\n",
    "        output_list = output_list.cuda()\n",
    "        input_length = input_length.cuda()\n",
    "        output_length = out_length.cuda()\n",
    "            \n",
    "    return [input_list, input_length, output_list, output_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_zh_pairs = [[indexesFromSentence(input_zh, train_zh_pairs[i][0]),\n",
    "                indexesFromSentence(output_zh_en, train_zh_pairs[i][1])] for i in range(len(train_zh_pairs))]\n",
    "index_vi_pairs = [[indexesFromSentence(input_vi, train_vi_pairs[i][0]),\n",
    "                  indexesFromSentence(output_vi_en, train_vi_pairs[i][1])] for i in range(len(train_vi_pairs))]\n",
    "'''\n",
    "NMTDataset needs index pairs, need to call indexesFromPairs functions beforehand\n",
    "The dataLoader is sorted according to length of the input_length, and padded to\n",
    "max length of input and output list repectively\n",
    "TODO: output_list is not sorted, hence need to sort (maybe) in the rnn sequence.\n",
    "'''\n",
    "train_zh_dataset = NMTDataset(index_zh_pairs, input_zh, output_zh_en)\n",
    "train_vi_dataset = NMTDataset(index_vi_pairs, input_vi, output_vi_en)\n",
    "\n",
    "train_zh_loader = torch.utils.data.DataLoader(dataset = train_zh_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "#Input_batch in size Batch x maxLen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a batch for testing purpose\n",
    "for i, (input_list, input_length, output_list, output_length) in enumerate(train_zh_loader):\n",
    "    if i== 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_list = input_list.transpose(0,1)\n",
    "input_test_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single batch train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here for the constant definition\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "hidden_size = 256\n",
    "max_length = 10\n",
    "BATCH_SIZE = 3\n",
    "TEST_BATCH_SIZE = 3\n",
    "CLIP = 50\n",
    "TEACHER_RATIO = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_mask(length_list):\n",
    "    max_length = length_list.max().item()\n",
    "    masked_sentences = []\n",
    "    longest_sentence = [1]*max_length\n",
    "    for i in range(BATCH_SIZE):\n",
    "        curr_length = length_list[i].item()\n",
    "        masked_sentence = [1]*max_length\n",
    "        masked_sentence[curr_length:] = [0] * (max_length - curr_length)\n",
    "        masked_sentences.append(masked_sentence)\n",
    "    if CUDA:\n",
    "        masked_sentences = torch.from_numpy(np.array(masked_sentences)).cuda()\n",
    "    else:\n",
    "        masked_sentences = torch.from_numpy(np.array(masked_sentences))\n",
    "    return masked_sentences\n",
    "        \n",
    "def rnn_mask_loss(decoder_outputs, output_list, output_length):\n",
    "    '''\n",
    "    decoder_outputs: 3d matrix containing all decoder output(B x output_lang vocab size)\n",
    "                    while decoder_output is in size(max_len x vocab_size)\n",
    "    output_list: Batch x max_len\n",
    "    output_length: batch\n",
    "    '''\n",
    "    batch_size, max_len = output_list.size()\n",
    "    decoder_outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))#(bxmax_len) x V\n",
    "    output_list = output_list.view(-1, 1)# (b x max_len) x 1 \n",
    "    neg_loss = -torch.gather(decoder_outputs, 1, output_list)#(b x max_len) x 1\n",
    "    neg_loss = neg_loss.view(batch_size, -1)# restore to b x max_len\n",
    "    \n",
    "    mask = rnn_mask(output_length)#b x max_len\n",
    "    mask_loss = neg_loss * mask.float()\n",
    "    \n",
    "    loss = mask_loss.sum() / output_length.float().sum()\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_train(input_list, input_length, output_list,output_length, \n",
    "                batch_encoder, batch_decoder, encoder_optimizer, decoder_optimizer, \n",
    "                attention, criterion):\n",
    "    '''\n",
    "    param: @attention is a Boolean variable indicating whether using attention\n",
    "    '''\n",
    "    batch_encoder.train()\n",
    "    batch_decoder.train()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    max_output_length = output_length.max().item()\n",
    "\n",
    "\n",
    "#     input_length = input_tensor.size(0)\n",
    "#     target_length = target_tensor.size(0)\n",
    "\n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    #batch_encoder = BatchEncoderRNN(input_lang.n_words, hidden_size, BATCH_SIZE, n_layers = 2, dropout = 0.1)\n",
    "    init_hidden = batch_encoder.initHidden()\n",
    "    encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, init_hidden)\n",
    "\n",
    "    #Initialize for decoding process\n",
    "    curr_batch = input_list.size(0)#Take the current batch size\n",
    "    decoder_input = torch.tensor([[SOS_TOKEN]], device=device).repeat(curr_batch,1)\n",
    "    decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "    #encoder_outputs : L x B x H\n",
    "    decoder_outputs = torch.zeros(max_output_length, curr_batch, batch_decoder.output_size)\n",
    "\n",
    "    \n",
    "    # Move new Variables to CUDA\n",
    "    if CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_outputs = decoder_outputs.cuda()\n",
    "    \n",
    "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if attention: #If attention is used\n",
    "        if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(max_output_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_outputs[di] = decoder_output\n",
    "                decoder_input = output_list[:,di].unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "        else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "            for di in range(max_output_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                decoder_outputs[di] = decoder_input\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.detach()# detach from history as input: size batch x 1 \n",
    "                if ((decoder_output == EOS_TOKEN).sum().item()) == decoder_output.size(0):#If all are EOS tokens\n",
    "                    break;\n",
    "            \n",
    "    else:\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(max_output_length):\n",
    "                decoder_output, decoder_hidden = batch_decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                decoder_outputs[di] = decoder_output\n",
    "                decoder_input = output_list[:,di].unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for di in range(max_output_length):\n",
    "                decoder_output, decoder_hidden = batch_decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                decoder_outputs[di] = decoder_input\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.detach()# detach from history as input: size batch x 1 \n",
    "                if ((decoder_output == EOS_TOKEN).sum().item()) == decoder_output.size(0):#If all are EOS tokens\n",
    "                    break;\n",
    "    loss += rnn_mask_loss(decoder_outputs.transpose(0,1).contiguous(), output_list.contiguous(), output_length)\n",
    "            \n",
    "\n",
    "    loss.backward()\n",
    "    ec = torch.nn.utils.clip_grad_norm(batch_encoder.parameters(), CLIP)\n",
    "    dc = torch.nn.utils.clip_grad_norm(batch_decoder.parameters(), CLIP)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, n_layers=1, dropout=0.1):\n",
    "        super(BatchEncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional = True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        '''\n",
    "        input_seqs in size B x L sorted in decreasing order -> will transpose to fit in embedding dimension\n",
    "        '''\n",
    "        embedded = self.embedding(input_seqs.transpose(0,1))#input_seqs B x L -> transpose to L x B\n",
    "        #Input length sorted by loader\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        #Outputs in shape L x B x 2H, hidden as the last state of the GRU\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        #hidden = hidden[:self.n_layers, :, :] + hidden[self.n_layers:,:,:]#Sum bidrectional information\n",
    "        #outputs L x B x H\n",
    "        #hidden size (2*n_layers) x B x H\n",
    "\n",
    "        #outputs: seq_len x Batch x H\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #Due to bidrectional will have self.n_layers * 2\n",
    "        return torch.zeros(self.n_layers *2, self.batch_size, self.hidden_size,device = device)#hidden size 2lays *B*H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256]), torch.Size([77, 3, 256]), torch.Size([4, 3, 256]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_encoder = BatchEncoderRNN(input_zh.n_words, hidden_size, BATCH_SIZE, n_layers = 2, dropout = 0.1)\n",
    "init_hidden = batch_encoder.initHidden()\n",
    "encoder_outputs, encoder_hidden = batch_encoder(input_list, input_length, init_hidden)\n",
    "#inithidden bidirectional, encoder_hidden, summing up both directions\n",
    "init_hidden.size(), encoder_outputs.size(), encoder_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN decoder with no attention used, batch implemented\n",
    "# RNN decoder take one token at a time\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, batch_size, n_layers=1, dropout_p = 0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.embedding = nn.Embedding(output_size, emb_size,padding_idx = PAD_TOKEN)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, n_layers, dropout = dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, self.batch_size, -1)# 1 x B x H\n",
    "        embedded = self.dropout(embedded)\n",
    "        output = F.relu(embedded)\n",
    "        output, hidden = self.gru(output, hidden)#output 1 x B x E, hidden n_layers x B x H\n",
    "        out = self.out(output[0])\n",
    "        out = self.softmax(out)\n",
    "        #out size batch x output_lang_vocab_size\n",
    "        #hidden n_layers x B x H\n",
    "        return out, hidden\n",
    "    \n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(self.n_layers, self.batch_size, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        hidden = hidden.squeeze()\n",
    "        encoder_output = encoder_output.squeeze()\n",
    "#         print(hidden.size())\n",
    "#         print(encoder_output.size())\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, emb_size, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        #print(batch_size)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "        output = F.log_softmax(output)\n",
    "\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for no attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "In function\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "3\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n",
      "torch.Size([1, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "#Inside the batch_train_function for testing purpose\n",
    "encoder1 = BatchEncoderRNN(input_zh.n_words, hidden_size, BATCH_SIZE).to(device)\n",
    "no_attn_decoder = DecoderRNN(attn_model, hidden_size, hidden_size, output_zh_en.n_words, BATCH_SIZE).to(device)\n",
    "\n",
    "max_output_length = output_length.max().item()\n",
    "print(max_output_length)\n",
    "\n",
    "\n",
    "init_hidden = encoder1.initHidden()\n",
    "encoder_outputs, encoder_hidden = encoder1(input_list, input_length, init_hidden)\n",
    "\n",
    "curr_batch = input_list.size(0)#Take the current batch size\n",
    "decoder_input = torch.tensor([[SOS_TOKEN]], device=device).repeat(curr_batch,1)\n",
    "decoder_hidden = encoder_hidden[:batch_decoder.n_layers]#Bidirectional summoned\n",
    "#print('In function')\n",
    "#print(decoder_hidden.size())\n",
    "#encoder_outputs : L x B x H\n",
    "decoder_outputs = torch.zeros(max_output_length, curr_batch, no_batch_decoder.output_size)\n",
    "for di in range(max_output_length):\n",
    "    decoder_output, decoder_hidden, decoder_attention = batch_decoder(\n",
    "        decoder_input, decoder_hidden, encoder_outputs)\n",
    "    decoder_outputs[di] = decoder_output\n",
    "    decoder_input = output_list[:,di].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.84076976776123\n"
     ]
    }
   ],
   "source": [
    "#Test for loss function printing out number should be\n",
    "hidden_size = 256\n",
    "attn_model = 'dot'\n",
    "encoder1 = BatchEncoderRNN(input_zh.n_words, hidden_size, BATCH_SIZE).to(device)\n",
    "batch_decoder = LuongAttnDecoderRNN(attn_model, hidden_size, hidden_size, output_zh_en.n_words).to(device)\n",
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(batch_decoder.parameters(), lr=learning_rate)\n",
    "#criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "\n",
    "loss= batch_train(input_list, input_length, output_list, output_length, \n",
    "            encoder1,batch_decoder, encoder_optimizer, \n",
    "           decoder_optimizer, True, criterion)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.827942848205566\n"
     ]
    }
   ],
   "source": [
    "#Test for loss function printing out number should be\n",
    "#RUN THIS!!!!\n",
    "hidden_size = 256\n",
    "attn_model = 'dot'\n",
    "encoder1 = BatchEncoderRNN(input_zh.n_words, hidden_size, BATCH_SIZE).to(device)\n",
    "batch_decoder1 = LuongAttnDecoderRNN(attn_model, hidden_size, hidden_size, output_zh_en.n_words).to(device)\n",
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(batch_decoder1.parameters(), lr=learning_rate)\n",
    "#criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "loss= batch_train(input_list, input_length, output_list, output_length, \n",
    "            encoder1, batch_decoder1, encoder_optimizer, \n",
    "           decoder_optimizer, True, criterion)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Overfitting test for batch_size = 3\n",
    "init_hidden = batch_encoder.initHidden()\n",
    "learning_rate = 0.01\n",
    "encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(no_batch_decoder.parameters(), lr=learning_rate)\n",
    "#criterion = torch.nn.NLLLoss()\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "train_loss_list = []\n",
    "for i in range(1000):\n",
    "    print('The current round is {}'.format(int(i)))\n",
    "    loss = batch_train(input_list, input_length, output_list, output_length, \n",
    "            encoder1, no_batch_decoder, encoder_optimizer, \n",
    "           decoder_optimizer, criterion)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize = (12,10))\n",
    "ax.plot(train_loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
